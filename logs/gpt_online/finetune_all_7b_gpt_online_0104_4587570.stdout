Alpaca prompt format. modified 'unk_token' = 0 in qlora.py (originally 2 because pad_token_id = 2 for zephyr). model-gpt-online-date Continue finetuning models finetuned on gpt generated data on gpt paraphrased online data. ~3 epochs
Namespace(model_name_or_path='lmsys/vicuna-7b-v1.5', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/public_qa_pairs_paraphrased.csv', dataset_format='alpaca', output_dir='./output/vicuna-7b-gpt-online-0104', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=1110, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/vicuna-7b-gpt-online-0104/runs/Jan13_11-54-57_211affn005', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=275, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=3, past_index=-1, run_name='./output/vicuna-7b-gpt-online-0104', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
Detected that training was already completed!
loading base model lmsys/vicuna-7b-v1.5...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 79953920.0 || all params: 3660320768 || trainable: 2.184341894267557
torch.bfloat16 422051840 0.11530460491051696
torch.uint8 3238002688 0.8846226582948481
torch.float32 266240 7.273679463493403e-05
{'loss': 1.2015, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.9066, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.793, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.7714, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.7335, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.9537, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.7874, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.7538, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.7149, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.7436, 'learning_rate': 0.0002, 'epoch': 0.37}
{'eval_loss': 0.7915006875991821, 'eval_runtime': 160.6198, 'eval_samples_per_second': 6.226, 'eval_steps_per_second': 6.226, 'epoch': 0.37}
{'mmlu_loss': 1.195709877730798, 'mmlu_eval_accuracy_human_aging': 0.6086956521739131, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.5555555555555556, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_miscellaneous': 0.627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.21, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.5555555555555556, 'mmlu_eval_accuracy_professional_accounting': 0.3548387096774194, 'mmlu_eval_accuracy_high_school_us_history': 0.5909090909090909, 'mmlu_eval_accuracy_philosophy': 0.5588235294117647, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.47619047619047616, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_biology': 0.40625, 'mmlu_eval_accuracy_college_biology': 0.5, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.75, 'mmlu_eval_accuracy_high_school_microeconomics': 0.34615384615384615, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_professional_law': 0.35294117647058826, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.42105263157894735, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy': 0.4850306314374128, 'epoch': 0.37}
{'loss': 0.9307, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.7976, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.7351, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.7195, 'learning_rate': 0.0002, 'epoch': 0.52}
{'loss': 0.689, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.9029, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.7691, 'learning_rate': 0.0002, 'epoch': 0.63}
{'loss': 0.7254, 'learning_rate': 0.0002, 'epoch': 0.67}
{'loss': 0.7055, 'learning_rate': 0.0002, 'epoch': 0.71}
{'loss': 0.6946, 'learning_rate': 0.0002, 'epoch': 0.75}
{'eval_loss': 0.7636361122131348, 'eval_runtime': 161.0145, 'eval_samples_per_second': 6.211, 'eval_steps_per_second': 6.211, 'epoch': 0.75}
{'mmlu_loss': 1.255086416158409, 'mmlu_eval_accuracy_human_aging': 0.6086956521739131, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.21, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy_high_school_us_history': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.5294117647058824, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.7666666666666667, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_professional_law': 0.34705882352941175, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.39473684210526316, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4186046511627907, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_us_foreign_policy': 0.5454545454545454, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_world_history': 0.6153846153846154, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy': 0.4801343638066861, 'epoch': 0.75}
{'loss': 0.902, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.747, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.7087, 'learning_rate': 0.0002, 'epoch': 0.86}
{'loss': 0.6966, 'learning_rate': 0.0002, 'epoch': 0.9}
{'loss': 0.6746, 'learning_rate': 0.0002, 'epoch': 0.93}
{'loss': 0.7799, 'learning_rate': 0.0002, 'epoch': 0.97}
{'loss': 0.7411, 'learning_rate': 0.0002, 'epoch': 1.01}
Saving PEFT checkpoint...
{'loss': 0.7611, 'learning_rate': 0.0002, 'epoch': 1.05}
{'loss': 0.6828, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.6185, 'learning_rate': 0.0002, 'epoch': 1.12}
{'eval_loss': 0.7378591895103455, 'eval_runtime': 160.8282, 'eval_samples_per_second': 6.218, 'eval_steps_per_second': 6.218, 'epoch': 1.12}
{'mmlu_loss': 1.2988136080765864, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.21, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.3225806451612903, 'mmlu_eval_accuracy_high_school_us_history': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_biology': 0.46875, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.7666666666666667, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_professional_law': 0.35294117647058826, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.39473684210526316, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.6153846153846154, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.47888275257740526, 'epoch': 1.12}
{'loss': 0.5782, 'learning_rate': 0.0002, 'epoch': 1.16}
{'loss': 0.6495, 'learning_rate': 0.0002, 'epoch': 1.2}
{'loss': 0.7849, 'learning_rate': 0.0002, 'epoch': 1.23}
{'loss': 0.666, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.6063, 'learning_rate': 0.0002, 'epoch': 1.31}
{'loss': 0.5764, 'learning_rate': 0.0002, 'epoch': 1.34}
{'loss': 0.6562, 'learning_rate': 0.0002, 'epoch': 1.38}
{'loss': 0.7504, 'learning_rate': 0.0002, 'epoch': 1.42}
{'loss': 0.6378, 'learning_rate': 0.0002, 'epoch': 1.46}
{'loss': 0.6056, 'learning_rate': 0.0002, 'epoch': 1.49}
{'eval_loss': 0.7293669581413269, 'eval_runtime': 171.4412, 'eval_samples_per_second': 5.833, 'eval_steps_per_second': 5.833, 'epoch': 1.49}
{'mmlu_loss': 1.409895330750226, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_management': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.3181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.22, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.3548387096774194, 'mmlu_eval_accuracy_high_school_us_history': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_security_studies': 0.4444444444444444, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_clinical_knowledge': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.7666666666666667, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_professional_law': 0.35294117647058826, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.34210526315789475, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy': 0.47864425180763615, 'epoch': 1.49}
{'loss': 0.5864, 'learning_rate': 0.0002, 'epoch': 1.53}
{'loss': 0.6441, 'learning_rate': 0.0002, 'epoch': 1.57}
{'loss': 0.7667, 'learning_rate': 0.0002, 'epoch': 1.61}
{'loss': 0.6493, 'learning_rate': 0.0002, 'epoch': 1.64}
{'loss': 0.6119, 'learning_rate': 0.0002, 'epoch': 1.68}
{'loss': 0.6004, 'learning_rate': 0.0002, 'epoch': 1.72}
{'loss': 0.6039, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.7491, 'learning_rate': 0.0002, 'epoch': 1.79}
{'loss': 0.6655, 'learning_rate': 0.0002, 'epoch': 1.83}
{'loss': 0.6227, 'learning_rate': 0.0002, 'epoch': 1.87}
{'eval_loss': 0.7201249003410339, 'eval_runtime': 160.5971, 'eval_samples_per_second': 6.227, 'eval_steps_per_second': 6.227, 'epoch': 1.87}
{'mmlu_loss': 1.4294073157727836, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_management': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_miscellaneous': 0.627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.22, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy_high_school_us_history': 0.5909090909090909, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.7333333333333333, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_professional_law': 0.3352941176470588, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_professional_psychology': 0.5217391304347826, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_us_foreign_policy': 0.5454545454545454, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy': 0.47690685171635616, 'epoch': 1.87}
{'loss': 0.588, 'learning_rate': 0.0002, 'epoch': 1.9}
{'loss': 0.6312, 'learning_rate': 0.0002, 'epoch': 1.94}
{'loss': 0.6379, 'learning_rate': 0.0002, 'epoch': 1.98}
{'loss': 0.6519, 'learning_rate': 0.0002, 'epoch': 2.02}
{'loss': 0.5997, 'learning_rate': 0.0002, 'epoch': 2.05}
Saving PEFT checkpoint...
{'loss': 0.5041, 'learning_rate': 0.0002, 'epoch': 2.09}
{'loss': 0.4418, 'learning_rate': 0.0002, 'epoch': 2.13}
{'loss': 0.4004, 'learning_rate': 0.0002, 'epoch': 2.17}
{'loss': 0.5323, 'learning_rate': 0.0002, 'epoch': 2.2}
{'loss': 0.6262, 'learning_rate': 0.0002, 'epoch': 2.24}
{'eval_loss': 0.7474527359008789, 'eval_runtime': 162.4206, 'eval_samples_per_second': 6.157, 'eval_steps_per_second': 6.157, 'epoch': 2.24}
{'mmlu_loss': 1.4330765220423605, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_management': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_miscellaneous': 0.6627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.2, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_high_school_us_history': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_biology': 0.40625, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.7333333333333333, 'mmlu_eval_accuracy_high_school_microeconomics': 0.34615384615384615, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_professional_law': 0.34705882352941175, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.5217391304347826, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4186046511627907, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_us_foreign_policy': 0.5454545454545454, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_world_history': 0.6153846153846154, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy': 0.474113016173135, 'epoch': 2.24}
{'loss': 0.5001, 'learning_rate': 0.0002, 'epoch': 2.28}
{'loss': 0.4342, 'learning_rate': 0.0002, 'epoch': 2.32}
{'loss': 0.4056, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.5305, 'learning_rate': 0.0002, 'epoch': 2.39}
{'loss': 0.6607, 'learning_rate': 0.0002, 'epoch': 2.43}
{'loss': 0.5209, 'learning_rate': 0.0002, 'epoch': 2.46}
{'loss': 0.4777, 'learning_rate': 0.0002, 'epoch': 2.5}
{'loss': 0.4293, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.5415, 'learning_rate': 0.0002, 'epoch': 2.58}
{'loss': 0.6169, 'learning_rate': 0.0002, 'epoch': 2.61}
{'eval_loss': 0.7518081068992615, 'eval_runtime': 161.4762, 'eval_samples_per_second': 6.193, 'eval_steps_per_second': 6.193, 'epoch': 2.61}
{'mmlu_loss': 1.502360108512278, 'mmlu_eval_accuracy_human_aging': 0.5217391304347826, 'mmlu_eval_accuracy_management': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.2, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_high_school_us_history': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_security_studies': 0.4444444444444444, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.45714285714285713, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.7, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_professional_law': 0.35294117647058826, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.5362318840579711, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4186046511627907, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_us_foreign_policy': 0.5454545454545454, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy': 0.4742061666830184, 'epoch': 2.61}
{'loss': 0.5153, 'learning_rate': 0.0002, 'epoch': 2.65}
{'loss': 0.4718, 'learning_rate': 0.0002, 'epoch': 2.69}
{'loss': 0.4221, 'learning_rate': 0.0002, 'epoch': 2.73}
{'loss': 0.5795, 'learning_rate': 0.0002, 'epoch': 2.76}
{'loss': 0.6485, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.5471, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.4837, 'learning_rate': 0.0002, 'epoch': 2.88}
{'loss': 0.4234, 'learning_rate': 0.0002, 'epoch': 2.91}
{'loss': 0.4905, 'learning_rate': 0.0002, 'epoch': 2.95}
{'loss': 0.5009, 'learning_rate': 0.0002, 'epoch': 2.99}
{'eval_loss': 0.7615624666213989, 'eval_runtime': 160.9387, 'eval_samples_per_second': 6.214, 'eval_steps_per_second': 6.214, 'epoch': 2.99}
{'mmlu_loss': 1.4951824794196755, 'mmlu_eval_accuracy_human_aging': 0.5217391304347826, 'mmlu_eval_accuracy_management': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_miscellaneous': 0.627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.2, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_high_school_us_history': 0.5909090909090909, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.45714285714285713, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.7166666666666667, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_professional_law': 0.36470588235294116, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_professional_psychology': 0.5217391304347826, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.46792066793148684, 'epoch': 2.99}
{'loss': 0.5352, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.4615, 'learning_rate': 0.0002, 'epoch': 3.06}
Saving PEFT checkpoint...
{'loss': 0.3526, 'learning_rate': 0.0002, 'epoch': 3.1}
{'loss': 0.2887, 'learning_rate': 0.0002, 'epoch': 3.14}
{'loss': 0.2275, 'learning_rate': 0.0002, 'epoch': 3.17}
{'loss': 0.4605, 'learning_rate': 0.0002, 'epoch': 3.21}
{'loss': 0.4638, 'learning_rate': 0.0002, 'epoch': 3.25}
{'loss': 0.3477, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.2902, 'learning_rate': 0.0002, 'epoch': 3.32}
{'loss': 0.2256, 'learning_rate': 0.0002, 'epoch': 3.36}
{'eval_loss': 0.9285624027252197, 'eval_runtime': 160.7929, 'eval_samples_per_second': 6.219, 'eval_steps_per_second': 6.219, 'epoch': 3.36}
{'mmlu_loss': 1.4093967518488775, 'mmlu_eval_accuracy_human_aging': 0.5217391304347826, 'mmlu_eval_accuracy_management': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.22, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy_high_school_us_history': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.5294117647058824, 'mmlu_eval_accuracy_security_studies': 0.5185185185185185, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_professional_medicine': 0.45161290322580644, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.4482758620689655, 'mmlu_eval_accuracy_prehistory': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.75, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6363636363636364, 'mmlu_eval_accuracy_professional_law': 0.34705882352941175, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.5217391304347826, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.39473684210526316, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.3953488372093023, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.47101625184515766, 'epoch': 3.36}
{'loss': 0.4542, 'learning_rate': 0.0002, 'epoch': 3.4}
{'loss': 0.4746, 'learning_rate': 0.0002, 'epoch': 3.44}
{'loss': 0.3582, 'learning_rate': 0.0002, 'epoch': 3.47}
{'loss': 0.2836, 'learning_rate': 0.0002, 'epoch': 3.51}
{'loss': 0.2164, 'learning_rate': 0.0002, 'epoch': 3.55}
{'loss': 0.4554, 'learning_rate': 0.0002, 'epoch': 3.59}
{'loss': 0.4578, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.354, 'learning_rate': 0.0002, 'epoch': 3.66}
{'loss': 0.2874, 'learning_rate': 0.0002, 'epoch': 3.7}
{'loss': 0.2276, 'learning_rate': 0.0002, 'epoch': 3.73}
{'eval_loss': 0.9128574132919312, 'eval_runtime': 160.8102, 'eval_samples_per_second': 6.219, 'eval_steps_per_second': 6.219, 'epoch': 3.73}
{'mmlu_loss': 1.4647008873726675, 'mmlu_eval_accuracy_human_aging': 0.5217391304347826, 'mmlu_eval_accuracy_management': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_miscellaneous': 0.627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.21, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy_high_school_us_history': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_security_studies': 0.5185185185185185, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_biology': 0.46875, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_clinical_knowledge': 0.4827586206896552, 'mmlu_eval_accuracy_prehistory': 0.45714285714285713, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.7166666666666667, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.6363636363636364, 'mmlu_eval_accuracy_professional_law': 0.3588235294117647, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.42105263157894735, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.3953488372093023, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.475132297025762, 'epoch': 3.73}
{'loss': 0.4883, 'learning_rate': 0.0002, 'epoch': 3.77}
{'loss': 0.4684, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.346, 'learning_rate': 0.0002, 'epoch': 3.85}
{'loss': 0.28, 'learning_rate': 0.0002, 'epoch': 3.88}
{'loss': 0.2427, 'learning_rate': 0.0002, 'epoch': 3.92}
{'loss': 0.4345, 'learning_rate': 0.0002, 'epoch': 3.96}
{'loss': 0.3054, 'learning_rate': 0.0002, 'epoch': 4.0}
{'loss': 0.4116, 'learning_rate': 0.0002, 'epoch': 4.03}
{'loss': 0.294, 'learning_rate': 0.0002, 'epoch': 4.07}
{'loss': 0.1843, 'learning_rate': 0.0002, 'epoch': 4.11}
{'eval_loss': 1.0278258323669434, 'eval_runtime': 161.1291, 'eval_samples_per_second': 6.206, 'eval_steps_per_second': 6.206, 'epoch': 4.11}
{'mmlu_loss': 1.4896291351270667, 'mmlu_eval_accuracy_human_aging': 0.5217391304347826, 'mmlu_eval_accuracy_management': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_miscellaneous': 0.627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.21, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_high_school_us_history': 0.6818181818181818, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_security_studies': 0.4444444444444444, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.45161290322580644, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_biology': 0.46875, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_clinical_knowledge': 0.4482758620689655, 'mmlu_eval_accuracy_prehistory': 0.45714285714285713, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.7333333333333333, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.6363636363636364, 'mmlu_eval_accuracy_professional_law': 0.36470588235294116, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.5217391304347826, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.37209302325581395, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.4749617777477243, 'epoch': 4.11}
Saving PEFT checkpoint...
{'loss': 0.1311, 'learning_rate': 0.0002, 'epoch': 4.15}
{'train_runtime': 14429.4936, 'train_samples_per_second': 1.231, 'train_steps_per_second': 0.077, 'train_loss': 0.5673469078433406, 'epoch': 4.15}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       4.15
  train_loss               =     0.5673
  train_runtime            = 4:00:29.49
  train_samples_per_second =      1.231
  train_steps_per_second   =      0.077
***** eval metrics *****
  epoch                   =       4.15
  eval_loss               =     1.0288
  eval_runtime            = 0:02:41.00
  eval_samples_per_second =      6.211
  eval_steps_per_second   =      6.211
Namespace(model_name_or_path='ehartford/Samantha-1.11-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/public_qa_pairs_paraphrased.csv', dataset_format='alpaca', output_dir='./output/samantha111-7b-gpt-online-0104', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=1110, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/samantha111-7b-gpt-online-0104/runs/Jan13_16-06-04_211affn005', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=275, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=3, past_index=-1, run_name='./output/samantha111-7b-gpt-online-0104', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
Detected that training was already completed!
loading base model ehartford/Samantha-1.11-7b...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 79953920.0 || all params: 3660320768 || trainable: 2.184341894267557
torch.bfloat16 422051840 0.11530460491051696
torch.uint8 3238002688 0.8846226582948481
torch.float32 266240 7.273679463493403e-05
{'loss': 1.191, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.9439, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.8328, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.8165, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.7708, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.9902, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.8185, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.7814, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.7445, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.7689, 'learning_rate': 0.0002, 'epoch': 0.37}
{'eval_loss': 0.8224566578865051, 'eval_runtime': 160.7557, 'eval_samples_per_second': 6.221, 'eval_steps_per_second': 6.221, 'epoch': 0.37}
{'mmlu_loss': 2.1349884842751776, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_miscellaneous': 0.5930232558139535, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.3058823529411765, 'mmlu_eval_accuracy_high_school_microeconomics': 0.19230769230769232, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.21875, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_professional_psychology': 0.36231884057971014, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_econometrics': 0.0, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_philosophy': 0.47058823529411764, 'mmlu_eval_accuracy_sociology': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.2413793103448276, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_marketing': 0.72, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.3888888888888889, 'mmlu_eval_accuracy_college_biology': 0.1875, 'mmlu_eval_accuracy_nutrition': 0.5151515151515151, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_psychology': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy': 0.39921738265375634, 'epoch': 0.37}
{'loss': 0.9507, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.8231, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.7587, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.7447, 'learning_rate': 0.0002, 'epoch': 0.52}
{'loss': 0.7185, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.9124, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.7869, 'learning_rate': 0.0002, 'epoch': 0.63}
{'loss': 0.7502, 'learning_rate': 0.0002, 'epoch': 0.67}
{'loss': 0.7276, 'learning_rate': 0.0002, 'epoch': 0.71}
{'loss': 0.7174, 'learning_rate': 0.0002, 'epoch': 0.75}
{'eval_loss': 0.7788919806480408, 'eval_runtime': 160.9673, 'eval_samples_per_second': 6.212, 'eval_steps_per_second': 6.212, 'epoch': 0.75}
{'mmlu_loss': 2.06661806941266, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_miscellaneous': 0.5813953488372093, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_microeconomics': 0.15384615384615385, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.21875, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.3023255813953488, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_professional_psychology': 0.34782608695652173, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.34210526315789475, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_philosophy': 0.47058823529411764, 'mmlu_eval_accuracy_sociology': 0.6818181818181818, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.2413793103448276, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.72, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_college_biology': 0.1875, 'mmlu_eval_accuracy_nutrition': 0.5454545454545454, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_psychology': 0.48333333333333334, 'mmlu_eval_accuracy_high_school_world_history': 0.3076923076923077, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy': 0.40912000952428523, 'epoch': 0.75}
{'loss': 0.9181, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.764, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.7275, 'learning_rate': 0.0002, 'epoch': 0.86}
{'loss': 0.7209, 'learning_rate': 0.0002, 'epoch': 0.9}
{'loss': 0.6947, 'learning_rate': 0.0002, 'epoch': 0.93}
{'loss': 0.8042, 'learning_rate': 0.0002, 'epoch': 0.97}
{'loss': 0.7655, 'learning_rate': 0.0002, 'epoch': 1.01}
Saving PEFT checkpoint...
{'loss': 0.7823, 'learning_rate': 0.0002, 'epoch': 1.05}
{'loss': 0.7038, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.6462, 'learning_rate': 0.0002, 'epoch': 1.12}
{'eval_loss': 0.7548709511756897, 'eval_runtime': 161.0311, 'eval_samples_per_second': 6.21, 'eval_steps_per_second': 6.21, 'epoch': 1.12}
{'mmlu_loss': 2.171362516889379, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_miscellaneous': 0.5581395348837209, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.3, 'mmlu_eval_accuracy_high_school_microeconomics': 0.23076923076923078, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.25, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_professional_psychology': 0.36231884057971014, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.47058823529411764, 'mmlu_eval_accuracy_sociology': 0.6363636363636364, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_high_school_geography': 0.5909090909090909, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_college_biology': 0.1875, 'mmlu_eval_accuracy_nutrition': 0.5151515151515151, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.5333333333333333, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy': 0.4183712325267236, 'epoch': 1.12}
{'loss': 0.5971, 'learning_rate': 0.0002, 'epoch': 1.16}
{'loss': 0.6691, 'learning_rate': 0.0002, 'epoch': 1.2}
{'loss': 0.8052, 'learning_rate': 0.0002, 'epoch': 1.23}
{'loss': 0.6855, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.6315, 'learning_rate': 0.0002, 'epoch': 1.31}
{'loss': 0.6029, 'learning_rate': 0.0002, 'epoch': 1.34}
{'loss': 0.6569, 'learning_rate': 0.0002, 'epoch': 1.38}
{'loss': 0.7629, 'learning_rate': 0.0002, 'epoch': 1.42}
{'loss': 0.662, 'learning_rate': 0.0002, 'epoch': 1.46}
{'loss': 0.629, 'learning_rate': 0.0002, 'epoch': 1.49}
{'eval_loss': 0.7416321039199829, 'eval_runtime': 161.0897, 'eval_samples_per_second': 6.208, 'eval_steps_per_second': 6.208, 'epoch': 1.49}
{'mmlu_loss': 2.4762440673826878, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_miscellaneous': 0.5697674418604651, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.28125, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.32558139534883723, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_professional_psychology': 0.37681159420289856, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.5238095238095238, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.3142857142857143, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_college_medicine': 0.3181818181818182, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.5, 'mmlu_eval_accuracy_sociology': 0.6818181818181818, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.5, 'mmlu_eval_accuracy_college_biology': 0.1875, 'mmlu_eval_accuracy_nutrition': 0.5151515151515151, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_psychology': 0.48333333333333334, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy': 0.4115314858857608, 'epoch': 1.49}
{'loss': 0.6106, 'learning_rate': 0.0002, 'epoch': 1.53}
{'loss': 0.659, 'learning_rate': 0.0002, 'epoch': 1.57}
{'loss': 0.7798, 'learning_rate': 0.0002, 'epoch': 1.61}
{'loss': 0.6697, 'learning_rate': 0.0002, 'epoch': 1.64}
{'loss': 0.6353, 'learning_rate': 0.0002, 'epoch': 1.68}
{'loss': 0.6195, 'learning_rate': 0.0002, 'epoch': 1.72}
{'loss': 0.6291, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.7652, 'learning_rate': 0.0002, 'epoch': 1.79}
{'loss': 0.684, 'learning_rate': 0.0002, 'epoch': 1.83}
{'loss': 0.6471, 'learning_rate': 0.0002, 'epoch': 1.87}
{'eval_loss': 0.731673538684845, 'eval_runtime': 160.9593, 'eval_samples_per_second': 6.213, 'eval_steps_per_second': 6.213, 'epoch': 1.87}
{'mmlu_loss': 2.304574961408457, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_miscellaneous': 0.5581395348837209, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.3, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.28125, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.32558139534883723, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_professional_psychology': 0.36231884057971014, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.3142857142857143, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_college_medicine': 0.3181818181818182, 'mmlu_eval_accuracy_moral_disputes': 0.3157894736842105, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.5, 'mmlu_eval_accuracy_sociology': 0.6818181818181818, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_high_school_geography': 0.45454545454545453, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.5, 'mmlu_eval_accuracy_college_biology': 0.1875, 'mmlu_eval_accuracy_nutrition': 0.5151515151515151, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.5166666666666667, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy': 0.4106676279570816, 'epoch': 1.87}
{'loss': 0.6166, 'learning_rate': 0.0002, 'epoch': 1.9}
{'loss': 0.659, 'learning_rate': 0.0002, 'epoch': 1.94}
{'loss': 0.6601, 'learning_rate': 0.0002, 'epoch': 1.98}
{'loss': 0.677, 'learning_rate': 0.0002, 'epoch': 2.02}
{'loss': 0.6302, 'learning_rate': 0.0002, 'epoch': 2.05}
Saving PEFT checkpoint...
{'loss': 0.5431, 'learning_rate': 0.0002, 'epoch': 2.09}
{'loss': 0.4826, 'learning_rate': 0.0002, 'epoch': 2.13}
{'loss': 0.4372, 'learning_rate': 0.0002, 'epoch': 2.17}
{'loss': 0.5716, 'learning_rate': 0.0002, 'epoch': 2.2}
{'loss': 0.66, 'learning_rate': 0.0002, 'epoch': 2.24}
{'eval_loss': 0.7637432813644409, 'eval_runtime': 161.1875, 'eval_samples_per_second': 6.204, 'eval_steps_per_second': 6.204, 'epoch': 2.24}
{'mmlu_loss': 2.495753926712263, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_miscellaneous': 0.5813953488372093, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.3, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.34375, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.32558139534883723, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_professional_psychology': 0.391304347826087, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.34210526315789475, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.47058823529411764, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_high_school_geography': 0.5909090909090909, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.5555555555555556, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_nutrition': 0.5454545454545454, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_psychology': 0.5333333333333333, 'mmlu_eval_accuracy_high_school_world_history': 0.3076923076923077, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy': 0.4192899962568584, 'epoch': 2.24}
{'loss': 0.5337, 'learning_rate': 0.0002, 'epoch': 2.28}
{'loss': 0.4758, 'learning_rate': 0.0002, 'epoch': 2.32}
{'loss': 0.4501, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.5688, 'learning_rate': 0.0002, 'epoch': 2.39}
{'loss': 0.6964, 'learning_rate': 0.0002, 'epoch': 2.43}
{'loss': 0.5603, 'learning_rate': 0.0002, 'epoch': 2.46}
{'loss': 0.5235, 'learning_rate': 0.0002, 'epoch': 2.5}
{'loss': 0.4692, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.5808, 'learning_rate': 0.0002, 'epoch': 2.58}
{'loss': 0.6511, 'learning_rate': 0.0002, 'epoch': 2.61}
{'eval_loss': 0.7645207643508911, 'eval_runtime': 161.9768, 'eval_samples_per_second': 6.174, 'eval_steps_per_second': 6.174, 'epoch': 2.61}
{'mmlu_loss': 2.172433071826347, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_miscellaneous': 0.5930232558139535, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.31176470588235294, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.375, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.32558139534883723, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_professional_psychology': 0.4057971014492754, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.39473684210526316, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.4117647058823529, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.3448275862068966, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_geography': 0.5909090909090909, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_nutrition': 0.5454545454545454, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_psychology': 0.55, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy': 0.4244792800267083, 'epoch': 2.61}
{'loss': 0.5546, 'learning_rate': 0.0002, 'epoch': 2.65}
{'loss': 0.5098, 'learning_rate': 0.0002, 'epoch': 2.69}
{'loss': 0.4597, 'learning_rate': 0.0002, 'epoch': 2.73}
{'loss': 0.6184, 'learning_rate': 0.0002, 'epoch': 2.76}
{'loss': 0.6866, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.585, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.536, 'learning_rate': 0.0002, 'epoch': 2.88}
{'loss': 0.4632, 'learning_rate': 0.0002, 'epoch': 2.91}
{'loss': 0.5367, 'learning_rate': 0.0002, 'epoch': 2.95}
{'loss': 0.542, 'learning_rate': 0.0002, 'epoch': 2.99}
{'eval_loss': 0.7668352127075195, 'eval_runtime': 161.0024, 'eval_samples_per_second': 6.211, 'eval_steps_per_second': 6.211, 'epoch': 2.99}
{'mmlu_loss': 2.192692020101379, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_miscellaneous': 0.5813953488372093, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.31176470588235294, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.3125, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.3023255813953488, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.3548387096774194, 'mmlu_eval_accuracy_professional_psychology': 0.391304347826087, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_prehistory': 0.3142857142857143, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.3157894736842105, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.4411764705882353, 'mmlu_eval_accuracy_sociology': 0.6818181818181818, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.3793103448275862, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_geography': 0.5909090909090909, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_nutrition': 0.5151515151515151, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.4666666666666667, 'mmlu_eval_accuracy_high_school_world_history': 0.3076923076923077, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy': 0.40939583728765827, 'epoch': 2.99}
{'loss': 0.587, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.5116, 'learning_rate': 0.0002, 'epoch': 3.06}
Saving PEFT checkpoint...
{'loss': 0.4135, 'learning_rate': 0.0002, 'epoch': 3.1}
{'loss': 0.3559, 'learning_rate': 0.0002, 'epoch': 3.14}
{'loss': 0.2823, 'learning_rate': 0.0002, 'epoch': 3.17}
{'loss': 0.5146, 'learning_rate': 0.0002, 'epoch': 3.21}
{'loss': 0.5204, 'learning_rate': 0.0002, 'epoch': 3.25}
{'loss': 0.4108, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.3541, 'learning_rate': 0.0002, 'epoch': 3.32}
{'loss': 0.2682, 'learning_rate': 0.0002, 'epoch': 3.36}
{'eval_loss': 0.9488958716392517, 'eval_runtime': 165.9432, 'eval_samples_per_second': 6.026, 'eval_steps_per_second': 6.026, 'epoch': 3.36}
{'mmlu_loss': 2.260803666751732, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_miscellaneous': 0.5930232558139535, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_professional_law': 0.3058823529411765, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.375, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.3488372093023256, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.3548387096774194, 'mmlu_eval_accuracy_professional_psychology': 0.4057971014492754, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_prehistory': 0.3142857142857143, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_college_medicine': 0.2727272727272727, 'mmlu_eval_accuracy_moral_disputes': 0.34210526315789475, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.4117647058823529, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.3333333333333333, 'mmlu_eval_accuracy_professional_accounting': 0.3870967741935484, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_high_school_geography': 0.5909090909090909, 'mmlu_eval_accuracy_security_studies': 0.4074074074074074, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_nutrition': 0.5757575757575758, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy': 0.4187763738639758, 'epoch': 3.36}
{'loss': 0.5068, 'learning_rate': 0.0002, 'epoch': 3.4}
{'loss': 0.5307, 'learning_rate': 0.0002, 'epoch': 3.44}
{'loss': 0.4171, 'learning_rate': 0.0002, 'epoch': 3.47}
{'loss': 0.3439, 'learning_rate': 0.0002, 'epoch': 3.51}
{'loss': 0.2752, 'learning_rate': 0.0002, 'epoch': 3.55}
{'loss': 0.5128, 'learning_rate': 0.0002, 'epoch': 3.59}
{'loss': 0.5189, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.418, 'learning_rate': 0.0002, 'epoch': 3.66}
{'loss': 0.3528, 'learning_rate': 0.0002, 'epoch': 3.7}
{'loss': 0.2837, 'learning_rate': 0.0002, 'epoch': 3.73}
{'eval_loss': 0.9156703352928162, 'eval_runtime': 160.8081, 'eval_samples_per_second': 6.219, 'eval_steps_per_second': 6.219, 'epoch': 3.73}
{'mmlu_loss': 2.409995033723867, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_miscellaneous': 0.6046511627906976, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_professional_law': 0.3235294117647059, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.3125, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.3488372093023256, 'mmlu_eval_accuracy_high_school_statistics': 0.21739130434782608, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.3548387096774194, 'mmlu_eval_accuracy_professional_psychology': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.47058823529411764, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.3448275862068966, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.3333333333333333, 'mmlu_eval_accuracy_professional_accounting': 0.3870967741935484, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_high_school_geography': 0.5454545454545454, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_nutrition': 0.5454545454545454, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_psychology': 0.6, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy': 0.41052007374324073, 'epoch': 3.73}
{'loss': 0.5482, 'learning_rate': 0.0002, 'epoch': 3.77}
{'loss': 0.5232, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.411, 'learning_rate': 0.0002, 'epoch': 3.85}
{'loss': 0.3493, 'learning_rate': 0.0002, 'epoch': 3.88}
{'loss': 0.2937, 'learning_rate': 0.0002, 'epoch': 3.92}
{'loss': 0.4844, 'learning_rate': 0.0002, 'epoch': 3.96}
{'loss': 0.3705, 'learning_rate': 0.0002, 'epoch': 4.0}
{'loss': 0.4831, 'learning_rate': 0.0002, 'epoch': 4.03}
{'loss': 0.3626, 'learning_rate': 0.0002, 'epoch': 4.07}
{'loss': 0.2444, 'learning_rate': 0.0002, 'epoch': 4.11}
{'eval_loss': 0.9933059811592102, 'eval_runtime': 160.7815, 'eval_samples_per_second': 6.22, 'eval_steps_per_second': 6.22, 'epoch': 4.11}
{'mmlu_loss': 2.716180266818527, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_miscellaneous': 0.6046511627906976, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_law': 0.31176470588235294, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.34375, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.32558139534883723, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_professional_medicine': 0.3225806451612903, 'mmlu_eval_accuracy_professional_psychology': 0.4492753623188406, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.34210526315789475, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.4411764705882353, 'mmlu_eval_accuracy_sociology': 0.6363636363636364, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_clinical_knowledge': 0.3793103448275862, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.3333333333333333, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_high_school_geography': 0.5454545454545454, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.5555555555555556, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_nutrition': 0.5151515151515151, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_psychology': 0.5666666666666667, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy': 0.40330272651429255, 'epoch': 4.11}
Saving PEFT checkpoint...
{'loss': 0.1866, 'learning_rate': 0.0002, 'epoch': 4.15}
{'train_runtime': 14390.1018, 'train_samples_per_second': 1.234, 'train_steps_per_second': 0.077, 'train_loss': 0.6040698628167849, 'epoch': 4.15}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       4.15
  train_loss               =     0.6041
  train_runtime            = 3:59:50.10
  train_samples_per_second =      1.234
  train_steps_per_second   =      0.077
***** eval metrics *****
  epoch                   =       4.15
  eval_loss               =     0.9867
  eval_runtime            = 0:02:41.06
  eval_samples_per_second =      6.209
  eval_steps_per_second   =      6.209
Namespace(model_name_or_path='ehartford/samantha-1.2-mistral-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/public_qa_pairs_paraphrased.csv', dataset_format='alpaca', output_dir='./output/samantha12-7b-gpt-online-0104', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=1110, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/samantha12-7b-gpt-online-0104/runs/Jan13_20-16-38_211affn005', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=275, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=3, past_index=-1, run_name='./output/samantha12-7b-gpt-online-0104', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
Detected that training was already completed!
loading base model ehartford/samantha-1.2-mistral-7b...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919867904 || trainable: 2.1400231348204124
torch.bfloat16 429940736 0.10968245525857394
torch.uint8 3489660928 0.8902496240852916
torch.float32 266240 6.792065613443692e-05
{'loss': 1.1824, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.8803, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.7955, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.787, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.7405, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.9714, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.7902, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.7691, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.7272, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.7514, 'learning_rate': 0.0002, 'epoch': 0.37}
{'eval_loss': 0.9133834838867188, 'eval_runtime': 170.0895, 'eval_samples_per_second': 5.879, 'eval_steps_per_second': 5.879, 'epoch': 0.37}
{'mmlu_loss': 12.896048659206915, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.5942028985507246, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.29, 'mmlu_eval_accuracy_professional_law': 0.43529411764705883, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy': 0.6045137123496293, 'epoch': 0.37}
{'loss': 0.9573, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.7984, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.7387, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.73, 'learning_rate': 0.0002, 'epoch': 0.52}
{'loss': 0.6931, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.9086, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.7652, 'learning_rate': 0.0002, 'epoch': 0.63}
{'loss': 0.7401, 'learning_rate': 0.0002, 'epoch': 0.67}
{'loss': 0.7218, 'learning_rate': 0.0002, 'epoch': 0.71}
{'loss': 0.7252, 'learning_rate': 0.0002, 'epoch': 0.75}
{'eval_loss': 0.8007797598838806, 'eval_runtime': 169.7961, 'eval_samples_per_second': 5.889, 'eval_steps_per_second': 5.889, 'epoch': 0.75}
{'mmlu_loss': 13.390402881714506, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.28, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy': 0.6091985803917515, 'epoch': 0.75}
{'loss': 0.9239, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.7381, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.7209, 'learning_rate': 0.0002, 'epoch': 0.86}
{'loss': 0.717, 'learning_rate': 0.0002, 'epoch': 0.9}
{'loss': 0.6842, 'learning_rate': 0.0002, 'epoch': 0.93}
{'loss': 0.8242, 'learning_rate': 0.0002, 'epoch': 0.97}
{'loss': 0.7511, 'learning_rate': 0.0002, 'epoch': 1.01}
Saving PEFT checkpoint...
{'loss': 0.6966, 'learning_rate': 0.0002, 'epoch': 1.05}
{'loss': 0.5961, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.5387, 'learning_rate': 0.0002, 'epoch': 1.12}
{'eval_loss': 0.7887503504753113, 'eval_runtime': 170.1723, 'eval_samples_per_second': 5.876, 'eval_steps_per_second': 5.876, 'epoch': 1.12}
{'mmlu_loss': 14.32336027804424, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.28, 'mmlu_eval_accuracy_professional_law': 0.4294117647058823, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy': 0.6045612678199968, 'epoch': 1.12}
{'loss': 0.4862, 'learning_rate': 0.0002, 'epoch': 1.16}
{'loss': 0.5981, 'learning_rate': 0.0002, 'epoch': 1.2}
{'loss': 0.7121, 'learning_rate': 0.0002, 'epoch': 1.23}
{'loss': 0.5883, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.5431, 'learning_rate': 0.0002, 'epoch': 1.31}
{'loss': 0.4926, 'learning_rate': 0.0002, 'epoch': 1.34}
{'loss': 0.5733, 'learning_rate': 0.0002, 'epoch': 1.38}
{'loss': 0.686, 'learning_rate': 0.0002, 'epoch': 1.42}
{'loss': 0.5697, 'learning_rate': 0.0002, 'epoch': 1.46}
{'loss': 0.5339, 'learning_rate': 0.0002, 'epoch': 1.49}
{'eval_loss': 0.7795907258987427, 'eval_runtime': 170.1719, 'eval_samples_per_second': 5.876, 'eval_steps_per_second': 5.876, 'epoch': 1.49}
{'mmlu_loss': 14.456507170768752, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6153846153846154, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_elementary_mathematics': 0.4878048780487805, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.28, 'mmlu_eval_accuracy_professional_law': 0.4117647058823529, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy': 0.6085747858529973, 'epoch': 1.49}
{'loss': 0.517, 'learning_rate': 0.0002, 'epoch': 1.53}
{'loss': 0.5998, 'learning_rate': 0.0002, 'epoch': 1.57}
{'loss': 0.7036, 'learning_rate': 0.0002, 'epoch': 1.61}
{'loss': 0.5851, 'learning_rate': 0.0002, 'epoch': 1.64}
{'loss': 0.54, 'learning_rate': 0.0002, 'epoch': 1.68}
{'loss': 0.5229, 'learning_rate': 0.0002, 'epoch': 1.72}
{'loss': 0.5655, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.692, 'learning_rate': 0.0002, 'epoch': 1.79}
{'loss': 0.6048, 'learning_rate': 0.0002, 'epoch': 1.83}
{'loss': 0.559, 'learning_rate': 0.0002, 'epoch': 1.87}
{'eval_loss': 0.7897325158119202, 'eval_runtime': 169.6462, 'eval_samples_per_second': 5.895, 'eval_steps_per_second': 5.895, 'epoch': 1.87}
{'mmlu_loss': 14.628906064372995, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_elementary_mathematics': 0.4878048780487805, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_professional_law': 0.40588235294117647, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy': 0.6097395575817379, 'epoch': 1.87}
{'loss': 0.5124, 'learning_rate': 0.0002, 'epoch': 1.9}
{'loss': 0.5829, 'learning_rate': 0.0002, 'epoch': 1.94}
{'loss': 0.5785, 'learning_rate': 0.0002, 'epoch': 1.98}
{'loss': 0.5777, 'learning_rate': 0.0002, 'epoch': 2.02}
{'loss': 0.4342, 'learning_rate': 0.0002, 'epoch': 2.05}
Saving PEFT checkpoint...
{'loss': 0.3588, 'learning_rate': 0.0002, 'epoch': 2.09}
{'loss': 0.309, 'learning_rate': 0.0002, 'epoch': 2.13}
{'loss': 0.2739, 'learning_rate': 0.0002, 'epoch': 2.17}
{'loss': 0.4557, 'learning_rate': 0.0002, 'epoch': 2.2}
{'loss': 0.4653, 'learning_rate': 0.0002, 'epoch': 2.24}
{'eval_loss': 0.9133626818656921, 'eval_runtime': 169.9217, 'eval_samples_per_second': 5.885, 'eval_steps_per_second': 5.885, 'epoch': 2.24}
{'mmlu_loss': 12.78193851548188, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.9166666666666666, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_elementary_mathematics': 0.4878048780487805, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy': 0.6107916134994302, 'epoch': 2.24}
{'loss': 0.3597, 'learning_rate': 0.0002, 'epoch': 2.28}
{'loss': 0.3122, 'learning_rate': 0.0002, 'epoch': 2.32}
{'loss': 0.286, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.449, 'learning_rate': 0.0002, 'epoch': 2.39}
{'loss': 0.4898, 'learning_rate': 0.0002, 'epoch': 2.43}
{'loss': 0.3745, 'learning_rate': 0.0002, 'epoch': 2.46}
{'loss': 0.341, 'learning_rate': 0.0002, 'epoch': 2.5}
{'loss': 0.3055, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.4771, 'learning_rate': 0.0002, 'epoch': 2.58}
{'loss': 0.4645, 'learning_rate': 0.0002, 'epoch': 2.61}
{'eval_loss': 0.9236380457878113, 'eval_runtime': 169.7668, 'eval_samples_per_second': 5.89, 'eval_steps_per_second': 5.89, 'epoch': 2.61}
{'mmlu_loss': 12.557638972511018, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_elementary_mathematics': 0.4634146341463415, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5348837209302325, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.37, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy': 0.6047128158318875, 'epoch': 2.61}
{'loss': 0.3907, 'learning_rate': 0.0002, 'epoch': 2.65}
{'loss': 0.3533, 'learning_rate': 0.0002, 'epoch': 2.69}
{'loss': 0.3205, 'learning_rate': 0.0002, 'epoch': 2.73}
{'loss': 0.5043, 'learning_rate': 0.0002, 'epoch': 2.76}
{'loss': 0.5058, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.3992, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.35, 'learning_rate': 0.0002, 'epoch': 2.88}
{'loss': 0.307, 'learning_rate': 0.0002, 'epoch': 2.91}
{'loss': 0.4034, 'learning_rate': 0.0002, 'epoch': 2.95}
{'loss': 0.3739, 'learning_rate': 0.0002, 'epoch': 2.99}
{'eval_loss': 0.897723376750946, 'eval_runtime': 170.8335, 'eval_samples_per_second': 5.854, 'eval_steps_per_second': 5.854, 'epoch': 2.99}
{'mmlu_loss': 13.990085792416922, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_elementary_mathematics': 0.4634146341463415, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.7272727272727273, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_philosophy': 0.6176470588235294, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.34, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy': 0.6068025771868183, 'epoch': 2.99}
{'loss': 0.3951, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.2624, 'learning_rate': 0.0002, 'epoch': 3.06}
Saving PEFT checkpoint...
{'loss': 0.2199, 'learning_rate': 0.0002, 'epoch': 3.1}
{'loss': 0.1994, 'learning_rate': 0.0002, 'epoch': 3.14}
{'loss': 0.1802, 'learning_rate': 0.0002, 'epoch': 3.17}
{'loss': 0.3718, 'learning_rate': 0.0002, 'epoch': 3.21}
{'loss': 0.2702, 'learning_rate': 0.0002, 'epoch': 3.25}
{'loss': 0.2326, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.2045, 'learning_rate': 0.0002, 'epoch': 3.32}
{'loss': 0.1702, 'learning_rate': 0.0002, 'epoch': 3.36}
{'eval_loss': 1.096610426902771, 'eval_runtime': 170.8357, 'eval_samples_per_second': 5.854, 'eval_steps_per_second': 5.854, 'epoch': 3.36}
{'mmlu_loss': 12.100275950677872, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.5942028985507246, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.37, 'mmlu_eval_accuracy_professional_law': 0.4294117647058823, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy': 0.6050884346584432, 'epoch': 3.36}
{'loss': 0.3618, 'learning_rate': 0.0002, 'epoch': 3.4}
{'loss': 0.2768, 'learning_rate': 0.0002, 'epoch': 3.44}
{'loss': 0.2333, 'learning_rate': 0.0002, 'epoch': 3.47}
{'loss': 0.2014, 'learning_rate': 0.0002, 'epoch': 3.51}
{'loss': 0.1632, 'learning_rate': 0.0002, 'epoch': 3.55}
{'loss': 0.3488, 'learning_rate': 0.0002, 'epoch': 3.59}
{'loss': 0.2869, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.2411, 'learning_rate': 0.0002, 'epoch': 3.66}
{'loss': 0.2046, 'learning_rate': 0.0002, 'epoch': 3.7}
{'loss': 0.1792, 'learning_rate': 0.0002, 'epoch': 3.73}
{'eval_loss': 1.0759323835372925, 'eval_runtime': 170.623, 'eval_samples_per_second': 5.861, 'eval_steps_per_second': 5.861, 'epoch': 3.73}
{'mmlu_loss': 11.697355084447281, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_elementary_mathematics': 0.4634146341463415, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_psychology': 0.5507246376811594, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.3, 'mmlu_eval_accuracy_professional_law': 0.4117647058823529, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy': 0.602955405546606, 'epoch': 3.73}
{'loss': 0.3789, 'learning_rate': 0.0002, 'epoch': 3.77}
{'loss': 0.2914, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.2331, 'learning_rate': 0.0002, 'epoch': 3.85}
{'loss': 0.2161, 'learning_rate': 0.0002, 'epoch': 3.88}
{'loss': 0.1951, 'learning_rate': 0.0002, 'epoch': 3.92}
{'loss': 0.3475, 'learning_rate': 0.0002, 'epoch': 3.96}
{'loss': 0.215, 'learning_rate': 0.0002, 'epoch': 4.0}
{'loss': 0.2524, 'learning_rate': 0.0002, 'epoch': 4.03}
{'loss': 0.1661, 'learning_rate': 0.0002, 'epoch': 4.07}
{'loss': 0.1365, 'learning_rate': 0.0002, 'epoch': 4.11}
{'eval_loss': 1.1438666582107544, 'eval_runtime': 169.9238, 'eval_samples_per_second': 5.885, 'eval_steps_per_second': 5.885, 'epoch': 4.11}
{'mmlu_loss': 14.218611411375598, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_elementary_mathematics': 0.4878048780487805, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_anatomy': 0.7857142857142857, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.32, 'mmlu_eval_accuracy_professional_law': 0.40588235294117647, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy': 0.6103113129492664, 'epoch': 4.11}
Saving PEFT checkpoint...
{'loss': 0.116, 'learning_rate': 0.0002, 'epoch': 4.15}
{'train_runtime': 15115.919, 'train_samples_per_second': 1.175, 'train_steps_per_second': 0.073, 'train_loss': 0.49463717443449, 'epoch': 4.15}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       4.15
  train_loss               =     0.4946
  train_runtime            = 4:11:55.91
  train_samples_per_second =      1.175
  train_steps_per_second   =      0.073
***** eval metrics *****
  epoch                   =       4.15
  eval_loss               =     1.1546
  eval_runtime            = 0:02:55.69
  eval_samples_per_second =      5.692
  eval_steps_per_second   =      5.692
Namespace(model_name_or_path='HuggingFaceH4/zephyr-7b-alpha', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/public_qa_pairs_paraphrased.csv', dataset_format='alpaca', output_dir='./output/zephyr-7b-gpt-online-0104', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=1110, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/zephyr-7b-gpt-online-0104/runs/Jan14_00-39-22_211affn005', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=275, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=3, past_index=-1, run_name='./output/zephyr-7b-gpt-online-0104', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
Detected that training was already completed!
loading base model HuggingFaceH4/zephyr-7b-alpha...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919843328 || trainable: 2.1400365519914986
torch.bfloat16 429916160 0.1096768732895643
torch.uint8 3489660928 0.8902552056284633
torch.float32 266240 6.792108197238643e-05
{'loss': 0.9564, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.8089, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.7691, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.7564, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.7152, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.8501, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.7447, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.7456, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.7034, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.728, 'learning_rate': 0.0002, 'epoch': 0.37}
{'eval_loss': 0.7916495203971863, 'eval_runtime': 168.4587, 'eval_samples_per_second': 5.936, 'eval_steps_per_second': 5.936, 'epoch': 0.37}
{'mmlu_loss': 2.1073123026447, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_clinical_knowledge': 0.5172413793103449, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.47368421052631576, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.5852956640609587, 'epoch': 0.37}
{'loss': 0.8261, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.7631, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.721, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.7102, 'learning_rate': 0.0002, 'epoch': 0.52}
{'loss': 0.6714, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.8056, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.7318, 'learning_rate': 0.0002, 'epoch': 0.63}
{'loss': 0.7223, 'learning_rate': 0.0002, 'epoch': 0.67}
{'loss': 0.7062, 'learning_rate': 0.0002, 'epoch': 0.71}
{'loss': 0.7008, 'learning_rate': 0.0002, 'epoch': 0.75}
{'eval_loss': 0.76289302110672, 'eval_runtime': 168.6868, 'eval_samples_per_second': 5.928, 'eval_steps_per_second': 5.928, 'epoch': 0.75}
{'mmlu_loss': 2.1821650968977706, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.5769230769230769, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_clinical_knowledge': 0.5517241379310345, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5263157894736842, 'mmlu_eval_accuracy_professional_law': 0.37058823529411766, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.5852619650828741, 'epoch': 0.75}
{'loss': 0.8051, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.7124, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.7073, 'learning_rate': 0.0002, 'epoch': 0.86}
{'loss': 0.702, 'learning_rate': 0.0002, 'epoch': 0.9}
{'loss': 0.6675, 'learning_rate': 0.0002, 'epoch': 0.93}
{'loss': 0.7632, 'learning_rate': 0.0002, 'epoch': 0.97}
{'loss': 0.7083, 'learning_rate': 0.0002, 'epoch': 1.01}
Saving PEFT checkpoint...
{'loss': 0.5994, 'learning_rate': 0.0002, 'epoch': 1.05}
{'loss': 0.562, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.5191, 'learning_rate': 0.0002, 'epoch': 1.12}
{'eval_loss': 0.7508568167686462, 'eval_runtime': 168.8717, 'eval_samples_per_second': 5.922, 'eval_steps_per_second': 5.922, 'epoch': 1.12}
{'mmlu_loss': 2.406449667033925, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_professional_law': 0.38235294117647056, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_world_history': 0.6153846153846154, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.6022681734056615, 'epoch': 1.12}
{'loss': 0.4737, 'learning_rate': 0.0002, 'epoch': 1.16}
{'loss': 0.5342, 'learning_rate': 0.0002, 'epoch': 1.2}
{'loss': 0.6077, 'learning_rate': 0.0002, 'epoch': 1.23}
{'loss': 0.5567, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.5173, 'learning_rate': 0.0002, 'epoch': 1.31}
{'loss': 0.4782, 'learning_rate': 0.0002, 'epoch': 1.34}
{'loss': 0.5131, 'learning_rate': 0.0002, 'epoch': 1.38}
{'loss': 0.5961, 'learning_rate': 0.0002, 'epoch': 1.42}
{'loss': 0.5421, 'learning_rate': 0.0002, 'epoch': 1.46}
{'loss': 0.5158, 'learning_rate': 0.0002, 'epoch': 1.49}
{'eval_loss': 0.7507903575897217, 'eval_runtime': 169.4124, 'eval_samples_per_second': 5.903, 'eval_steps_per_second': 5.903, 'epoch': 1.49}
{'mmlu_loss': 2.458135780752445, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7692307692307693, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_professional_law': 0.38235294117647056, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_world_history': 0.6153846153846154, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.6083030826274147, 'epoch': 1.49}
{'loss': 0.5005, 'learning_rate': 0.0002, 'epoch': 1.53}
{'loss': 0.5349, 'learning_rate': 0.0002, 'epoch': 1.57}
{'loss': 0.6088, 'learning_rate': 0.0002, 'epoch': 1.61}
{'loss': 0.5507, 'learning_rate': 0.0002, 'epoch': 1.64}
{'loss': 0.524, 'learning_rate': 0.0002, 'epoch': 1.68}
{'loss': 0.5064, 'learning_rate': 0.0002, 'epoch': 1.72}
{'loss': 0.5144, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.5991, 'learning_rate': 0.0002, 'epoch': 1.79}
{'loss': 0.5724, 'learning_rate': 0.0002, 'epoch': 1.83}
{'loss': 0.5369, 'learning_rate': 0.0002, 'epoch': 1.87}
{'eval_loss': 0.7498475313186646, 'eval_runtime': 168.9233, 'eval_samples_per_second': 5.92, 'eval_steps_per_second': 5.92, 'epoch': 1.87}
{'mmlu_loss': 2.4175286251762036, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6153846153846154, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.7272727272727273, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_miscellaneous': 0.6744186046511628, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.6009525597560357, 'epoch': 1.87}
{'loss': 0.4987, 'learning_rate': 0.0002, 'epoch': 1.9}
{'loss': 0.5297, 'learning_rate': 0.0002, 'epoch': 1.94}
{'loss': 0.5435, 'learning_rate': 0.0002, 'epoch': 1.98}
{'loss': 0.4998, 'learning_rate': 0.0002, 'epoch': 2.02}
{'loss': 0.373, 'learning_rate': 0.0002, 'epoch': 2.05}
Saving PEFT checkpoint...
{'loss': 0.3376, 'learning_rate': 0.0002, 'epoch': 2.09}
{'loss': 0.2916, 'learning_rate': 0.0002, 'epoch': 2.13}
{'loss': 0.2578, 'learning_rate': 0.0002, 'epoch': 2.17}
{'loss': 0.3696, 'learning_rate': 0.0002, 'epoch': 2.2}
{'loss': 0.3956, 'learning_rate': 0.0002, 'epoch': 2.24}
{'eval_loss': 0.8966386914253235, 'eval_runtime': 168.4162, 'eval_samples_per_second': 5.938, 'eval_steps_per_second': 5.938, 'epoch': 2.24}
{'mmlu_loss': 2.5134537320056047, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_prehistory': 0.7142857142857143, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_professional_law': 0.4235294117647059, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_moral_scenarios': 0.28, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.6095717527260747, 'epoch': 2.24}
{'loss': 0.3385, 'learning_rate': 0.0002, 'epoch': 2.28}
{'loss': 0.2941, 'learning_rate': 0.0002, 'epoch': 2.32}
{'loss': 0.2661, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.3533, 'learning_rate': 0.0002, 'epoch': 2.39}
{'loss': 0.4108, 'learning_rate': 0.0002, 'epoch': 2.43}
{'loss': 0.35, 'learning_rate': 0.0002, 'epoch': 2.46}
{'loss': 0.3126, 'learning_rate': 0.0002, 'epoch': 2.5}
{'loss': 0.2928, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.3834, 'learning_rate': 0.0002, 'epoch': 2.58}
{'loss': 0.3939, 'learning_rate': 0.0002, 'epoch': 2.61}
{'eval_loss': 0.8979381918907166, 'eval_runtime': 168.6336, 'eval_samples_per_second': 5.93, 'eval_steps_per_second': 5.93, 'epoch': 2.61}
{'mmlu_loss': 2.4189555014487887, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_professional_psychology': 0.5942028985507246, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_moral_scenarios': 0.3, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.5983476372057563, 'epoch': 2.61}
{'loss': 0.3696, 'learning_rate': 0.0002, 'epoch': 2.65}
{'loss': 0.3297, 'learning_rate': 0.0002, 'epoch': 2.69}
{'loss': 0.2985, 'learning_rate': 0.0002, 'epoch': 2.73}
{'loss': 0.4071, 'learning_rate': 0.0002, 'epoch': 2.76}
{'loss': 0.4201, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.3714, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.3278, 'learning_rate': 0.0002, 'epoch': 2.88}
{'loss': 0.2945, 'learning_rate': 0.0002, 'epoch': 2.91}
{'loss': 0.3347, 'learning_rate': 0.0002, 'epoch': 2.95}
{'loss': 0.3476, 'learning_rate': 0.0002, 'epoch': 2.99}
{'eval_loss': 0.8791502714157104, 'eval_runtime': 169.4151, 'eval_samples_per_second': 5.903, 'eval_steps_per_second': 5.903, 'epoch': 2.99}
{'mmlu_loss': 2.4200512533247043, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_moral_scenarios': 0.32, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.6033879900430577, 'epoch': 2.99}
{'loss': 0.312, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.2237, 'learning_rate': 0.0002, 'epoch': 3.06}
Saving PEFT checkpoint...
{'loss': 0.2056, 'learning_rate': 0.0002, 'epoch': 3.1}
{'loss': 0.1826, 'learning_rate': 0.0002, 'epoch': 3.14}
{'loss': 0.1614, 'learning_rate': 0.0002, 'epoch': 3.17}
{'loss': 0.2834, 'learning_rate': 0.0002, 'epoch': 3.21}
{'loss': 0.23, 'learning_rate': 0.0002, 'epoch': 3.25}
{'loss': 0.2085, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.1872, 'learning_rate': 0.0002, 'epoch': 3.32}
{'loss': 0.1595, 'learning_rate': 0.0002, 'epoch': 3.36}
{'eval_loss': 1.1289962530136108, 'eval_runtime': 168.613, 'eval_samples_per_second': 5.931, 'eval_steps_per_second': 5.931, 'epoch': 3.36}
{'mmlu_loss': 2.6092028295503265, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5652173913043478, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_mathematics': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_professional_law': 0.38235294117647056, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.7222222222222222, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.6074913341138071, 'epoch': 3.36}
{'loss': 0.2795, 'learning_rate': 0.0002, 'epoch': 3.4}
{'loss': 0.2498, 'learning_rate': 0.0002, 'epoch': 3.44}
{'loss': 0.213, 'learning_rate': 0.0002, 'epoch': 3.47}
{'loss': 0.1856, 'learning_rate': 0.0002, 'epoch': 3.51}
{'loss': 0.1622, 'learning_rate': 0.0002, 'epoch': 3.55}
{'loss': 0.2809, 'learning_rate': 0.0002, 'epoch': 3.59}
{'loss': 0.241, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.2133, 'learning_rate': 0.0002, 'epoch': 3.66}
{'loss': 0.1907, 'learning_rate': 0.0002, 'epoch': 3.7}
{'loss': 0.1624, 'learning_rate': 0.0002, 'epoch': 3.73}
{'eval_loss': 1.094144582748413, 'eval_runtime': 168.7594, 'eval_samples_per_second': 5.926, 'eval_steps_per_second': 5.926, 'epoch': 3.73}
{'mmlu_loss': 2.5682373840305868, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.8125, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_prehistory': 0.7142857142857143, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_college_mathematics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.7272727272727273, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_professional_law': 0.40588235294117647, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_world_history': 0.6153846153846154, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.6120014955234699, 'epoch': 3.73}
{'loss': 0.2941, 'learning_rate': 0.0002, 'epoch': 3.77}
{'loss': 0.268, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.2105, 'learning_rate': 0.0002, 'epoch': 3.85}
{'loss': 0.195, 'learning_rate': 0.0002, 'epoch': 3.88}
{'loss': 0.1858, 'learning_rate': 0.0002, 'epoch': 3.92}
{'loss': 0.2807, 'learning_rate': 0.0002, 'epoch': 3.96}
{'loss': 0.203, 'learning_rate': 0.0002, 'epoch': 4.0}
{'loss': 0.2055, 'learning_rate': 0.0002, 'epoch': 4.03}
{'loss': 0.1452, 'learning_rate': 0.0002, 'epoch': 4.07}
{'loss': 0.1236, 'learning_rate': 0.0002, 'epoch': 4.11}
{'eval_loss': 1.158048391342163, 'eval_runtime': 168.621, 'eval_samples_per_second': 5.93, 'eval_steps_per_second': 5.93, 'epoch': 4.11}
{'mmlu_loss': 2.7420110502794315, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.8125, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5507246376811594, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_mathematics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_professional_law': 0.40588235294117647, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.7209302325581395, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_security_studies': 0.5555555555555556, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_world_history': 0.5769230769230769, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy': 0.6111868118183234, 'epoch': 4.11}
Saving PEFT checkpoint...
{'loss': 0.1048, 'learning_rate': 0.0002, 'epoch': 4.15}
{'train_runtime': 14966.8415, 'train_samples_per_second': 1.187, 'train_steps_per_second': 0.074, 'train_loss': 0.4523237244502918, 'epoch': 4.15}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       4.15
  train_loss               =     0.4523
  train_runtime            = 4:09:26.84
  train_samples_per_second =      1.187
  train_steps_per_second   =      0.074
***** eval metrics *****
  epoch                   =       4.15
  eval_loss               =     1.1706
  eval_runtime            = 0:02:48.91
  eval_samples_per_second =       5.92
  eval_steps_per_second   =       5.92
