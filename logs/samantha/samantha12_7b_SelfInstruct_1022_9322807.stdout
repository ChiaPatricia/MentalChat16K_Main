Alpaca prompt format. samantha12-7b-SelfInstruct-1022
Namespace(model_name_or_path='ehartford/samantha-1.2-mistral-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/lab/self_instruct_gpt3.5_instruction.csv', dataset_format='alpaca', output_dir='./output/samantha12-7b-SelfInstruct-1022', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=1850, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/samantha12-7b-SelfInstruct-1022/runs/Oct25_17-01-02_2118ffn008', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=450, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=3, past_index=-1, run_name='./output/samantha12-7b-SelfInstruct-1022', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model ehartford/samantha-1.2-mistral-7b...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919867904 || trainable: 2.1400231348204124
torch.bfloat16 429940736 0.10968245525857394
torch.uint8 3489660928 0.8902496240852916
torch.float32 266240 6.792065613443692e-05
{'loss': 1.1601, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 0.8756, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.7524, 'learning_rate': 0.0002, 'epoch': 0.06}
{'loss': 0.7225, 'learning_rate': 0.0002, 'epoch': 0.08}
{'loss': 0.7024, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 1.0215, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.8108, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.7421, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.7034, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.6621, 'learning_rate': 0.0002, 'epoch': 0.21}
{'loss': 0.9898, 'learning_rate': 0.0002, 'epoch': 0.23}
{'loss': 0.8037, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 0.7054, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 0.6875, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.6488, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 0.9623, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.7664, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.6882, 'learning_rate': 0.0002, 'epoch': 0.37}
{'eval_loss': 0.753066897392273, 'eval_runtime': 407.1827, 'eval_samples_per_second': 2.456, 'eval_steps_per_second': 2.456, 'epoch': 0.38}
{'mmlu_loss': 14.72517160556427, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.43, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_professional_law': 0.4411764705882353, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_miscellaneous': 0.7674418604651163, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy': 0.6060092448905743, 'epoch': 0.38}
{'loss': 0.655, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.6558, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.9542, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.8056, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.7095, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.6744, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.6228, 'learning_rate': 0.0002, 'epoch': 0.51}
{'loss': 0.953, 'learning_rate': 0.0002, 'epoch': 0.54}
{'loss': 0.7643, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.6623, 'learning_rate': 0.0002, 'epoch': 0.58}
{'loss': 0.652, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.6227, 'learning_rate': 0.0002, 'epoch': 0.62}
{'loss': 0.9441, 'learning_rate': 0.0002, 'epoch': 0.64}
{'loss': 0.7565, 'learning_rate': 0.0002, 'epoch': 0.66}
{'loss': 0.6743, 'learning_rate': 0.0002, 'epoch': 0.68}
{'loss': 0.6651, 'learning_rate': 0.0002, 'epoch': 0.7}
{'loss': 0.6224, 'learning_rate': 0.0002, 'epoch': 0.72}
{'loss': 0.9344, 'learning_rate': 0.0002, 'epoch': 0.74}
{'loss': 0.7506, 'learning_rate': 0.0002, 'epoch': 0.76}
{'eval_loss': 0.7301459312438965, 'eval_runtime': 411.4141, 'eval_samples_per_second': 2.431, 'eval_steps_per_second': 2.431, 'epoch': 0.77}
{'mmlu_loss': 16.10933660137194, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.32, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_professional_law': 0.45294117647058824, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_miscellaneous': 0.7674418604651163, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy': 0.6100294016126588, 'epoch': 0.77}
{'loss': 0.6918, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.6518, 'learning_rate': 0.0002, 'epoch': 0.8}
{'loss': 0.6061, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.9366, 'learning_rate': 0.0002, 'epoch': 0.84}
{'loss': 0.7632, 'learning_rate': 0.0002, 'epoch': 0.86}
{'loss': 0.6825, 'learning_rate': 0.0002, 'epoch': 0.89}
{'loss': 0.6452, 'learning_rate': 0.0002, 'epoch': 0.91}
{'loss': 0.6374, 'learning_rate': 0.0002, 'epoch': 0.93}
Saving PEFT checkpoint...
{'loss': 0.8493, 'learning_rate': 0.0002, 'epoch': 0.95}
{'loss': 0.695, 'learning_rate': 0.0002, 'epoch': 0.97}
{'loss': 0.6477, 'learning_rate': 0.0002, 'epoch': 0.99}
{'loss': 0.7226, 'learning_rate': 0.0002, 'epoch': 1.01}
{'loss': 0.7792, 'learning_rate': 0.0002, 'epoch': 1.03}
{'loss': 0.6218, 'learning_rate': 0.0002, 'epoch': 1.05}
{'loss': 0.5861, 'learning_rate': 0.0002, 'epoch': 1.07}
{'loss': 0.5309, 'learning_rate': 0.0002, 'epoch': 1.09}
{'loss': 0.6608, 'learning_rate': 0.0002, 'epoch': 1.11}
{'loss': 0.7782, 'learning_rate': 0.0002, 'epoch': 1.13}
{'loss': 0.6356, 'learning_rate': 0.0002, 'epoch': 1.15}
{'eval_loss': 0.7294881939888, 'eval_runtime': 408.3829, 'eval_samples_per_second': 2.449, 'eval_steps_per_second': 2.449, 'epoch': 1.15}
{'mmlu_loss': 15.78017003391398, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_professional_law': 0.4647058823529412, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy': 0.6073306345770196, 'epoch': 1.15}
{'loss': 0.5743, 'learning_rate': 0.0002, 'epoch': 1.17}
{'loss': 0.5385, 'learning_rate': 0.0002, 'epoch': 1.19}
{'loss': 0.6741, 'learning_rate': 0.0002, 'epoch': 1.21}
{'loss': 0.766, 'learning_rate': 0.0002, 'epoch': 1.24}
{'loss': 0.6481, 'learning_rate': 0.0002, 'epoch': 1.26}
{'loss': 0.5824, 'learning_rate': 0.0002, 'epoch': 1.28}
{'loss': 0.5594, 'learning_rate': 0.0002, 'epoch': 1.3}
{'loss': 0.6663, 'learning_rate': 0.0002, 'epoch': 1.32}
{'loss': 0.7698, 'learning_rate': 0.0002, 'epoch': 1.34}
{'loss': 0.6381, 'learning_rate': 0.0002, 'epoch': 1.36}
{'loss': 0.5793, 'learning_rate': 0.0002, 'epoch': 1.38}
{'loss': 0.5484, 'learning_rate': 0.0002, 'epoch': 1.4}
{'loss': 0.6698, 'learning_rate': 0.0002, 'epoch': 1.42}
{'loss': 0.7563, 'learning_rate': 0.0002, 'epoch': 1.44}
{'loss': 0.6186, 'learning_rate': 0.0002, 'epoch': 1.46}
{'loss': 0.5988, 'learning_rate': 0.0002, 'epoch': 1.48}
{'loss': 0.5348, 'learning_rate': 0.0002, 'epoch': 1.5}
{'loss': 0.6696, 'learning_rate': 0.0002, 'epoch': 1.52}
{'eval_loss': 0.7296079993247986, 'eval_runtime': 407.1578, 'eval_samples_per_second': 2.456, 'eval_steps_per_second': 2.456, 'epoch': 1.54}
{'mmlu_loss': 16.134305808218688, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.7586206896551724, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_professional_law': 0.4764705882352941, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy': 0.6120228783794518, 'epoch': 1.54}
{'loss': 0.772, 'learning_rate': 0.0002, 'epoch': 1.54}
{'loss': 0.6405, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 0.5786, 'learning_rate': 0.0002, 'epoch': 1.59}
{'loss': 0.5448, 'learning_rate': 0.0002, 'epoch': 1.61}
{'loss': 0.6612, 'learning_rate': 0.0002, 'epoch': 1.63}
{'loss': 0.7701, 'learning_rate': 0.0002, 'epoch': 1.65}
{'loss': 0.655, 'learning_rate': 0.0002, 'epoch': 1.67}
{'loss': 0.5965, 'learning_rate': 0.0002, 'epoch': 1.69}
{'loss': 0.5826, 'learning_rate': 0.0002, 'epoch': 1.71}
{'loss': 0.6732, 'learning_rate': 0.0002, 'epoch': 1.73}
{'loss': 0.7648, 'learning_rate': 0.0002, 'epoch': 1.75}
{'loss': 0.6278, 'learning_rate': 0.0002, 'epoch': 1.77}
{'loss': 0.6026, 'learning_rate': 0.0002, 'epoch': 1.79}
{'loss': 0.5664, 'learning_rate': 0.0002, 'epoch': 1.81}
{'loss': 0.6647, 'learning_rate': 0.0002, 'epoch': 1.83}
{'loss': 0.7851, 'learning_rate': 0.0002, 'epoch': 1.85}
Saving PEFT checkpoint...
{'loss': 0.6349, 'learning_rate': 0.0002, 'epoch': 1.87}
{'loss': 0.5765, 'learning_rate': 0.0002, 'epoch': 1.89}
{'loss': 0.556, 'learning_rate': 0.0002, 'epoch': 1.91}
{'eval_loss': 0.7253117561340332, 'eval_runtime': 408.6397, 'eval_samples_per_second': 2.447, 'eval_steps_per_second': 2.447, 'epoch': 1.92}
{'mmlu_loss': 15.479360007990582, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.41, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_professional_law': 0.4588235294117647, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy': 0.6000081639519124, 'epoch': 1.92}
{'loss': 0.6623, 'learning_rate': 0.0002, 'epoch': 1.94}
{'loss': 0.7335, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 0.6085, 'learning_rate': 0.0002, 'epoch': 1.98}
{'loss': 0.5631, 'learning_rate': 0.0002, 'epoch': 2.0}
{'loss': 0.6889, 'learning_rate': 0.0002, 'epoch': 2.02}
{'loss': 0.5909, 'learning_rate': 0.0002, 'epoch': 2.04}
{'loss': 0.5089, 'learning_rate': 0.0002, 'epoch': 2.06}
{'loss': 0.4537, 'learning_rate': 0.0002, 'epoch': 2.08}
{'loss': 0.3943, 'learning_rate': 0.0002, 'epoch': 2.1}
{'loss': 0.6625, 'learning_rate': 0.0002, 'epoch': 2.12}
{'loss': 0.5903, 'learning_rate': 0.0002, 'epoch': 2.14}
{'loss': 0.5137, 'learning_rate': 0.0002, 'epoch': 2.16}
{'loss': 0.4699, 'learning_rate': 0.0002, 'epoch': 2.18}
{'loss': 0.4056, 'learning_rate': 0.0002, 'epoch': 2.2}
{'loss': 0.6625, 'learning_rate': 0.0002, 'epoch': 2.22}
{'loss': 0.5869, 'learning_rate': 0.0002, 'epoch': 2.24}
{'loss': 0.5325, 'learning_rate': 0.0002, 'epoch': 2.26}
{'loss': 0.48, 'learning_rate': 0.0002, 'epoch': 2.29}
{'loss': 0.4022, 'learning_rate': 0.0002, 'epoch': 2.31}
{'eval_loss': 0.7830062508583069, 'eval_runtime': 407.0941, 'eval_samples_per_second': 2.456, 'eval_steps_per_second': 2.456, 'epoch': 2.31}
{'mmlu_loss': 15.829483115073515, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_virology': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_professional_law': 0.4647058823529412, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy': 0.6103617820413251, 'epoch': 2.31}
{'loss': 0.6742, 'learning_rate': 0.0002, 'epoch': 2.33}
{'loss': 0.6271, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.5326, 'learning_rate': 0.0002, 'epoch': 2.37}
{'loss': 0.4882, 'learning_rate': 0.0002, 'epoch': 2.39}
{'loss': 0.4273, 'learning_rate': 0.0002, 'epoch': 2.41}
{'loss': 0.6866, 'learning_rate': 0.0002, 'epoch': 2.43}
{'loss': 0.6269, 'learning_rate': 0.0002, 'epoch': 2.45}
{'loss': 0.5272, 'learning_rate': 0.0002, 'epoch': 2.47}
{'loss': 0.5017, 'learning_rate': 0.0002, 'epoch': 2.49}
{'loss': 0.4363, 'learning_rate': 0.0002, 'epoch': 2.51}
{'loss': 0.6874, 'learning_rate': 0.0002, 'epoch': 2.53}
{'loss': 0.6155, 'learning_rate': 0.0002, 'epoch': 2.55}
{'loss': 0.5465, 'learning_rate': 0.0002, 'epoch': 2.57}
{'loss': 0.4961, 'learning_rate': 0.0002, 'epoch': 2.59}
{'loss': 0.421, 'learning_rate': 0.0002, 'epoch': 2.61}
{'loss': 0.7003, 'learning_rate': 0.0002, 'epoch': 2.64}
{'loss': 0.6207, 'learning_rate': 0.0002, 'epoch': 2.66}
{'loss': 0.5401, 'learning_rate': 0.0002, 'epoch': 2.68}
{'eval_loss': 0.7558367848396301, 'eval_runtime': 410.6651, 'eval_samples_per_second': 2.435, 'eval_steps_per_second': 2.435, 'epoch': 2.69}
{'mmlu_loss': 15.107199868215291, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.34, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.6818181818181818, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_professional_law': 0.4470588235294118, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_miscellaneous': 0.7209302325581395, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy': 0.5961085631232035, 'epoch': 2.69}
{'loss': 0.4879, 'learning_rate': 0.0002, 'epoch': 2.7}
{'loss': 0.441, 'learning_rate': 0.0002, 'epoch': 2.72}
{'loss': 0.6859, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 0.6173, 'learning_rate': 0.0002, 'epoch': 2.76}
{'loss': 0.5316, 'learning_rate': 0.0002, 'epoch': 2.78}
Saving PEFT checkpoint...
{'loss': 0.5012, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.4375, 'learning_rate': 0.0002, 'epoch': 2.82}
{'loss': 0.7105, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.6368, 'learning_rate': 0.0002, 'epoch': 2.86}
{'loss': 0.5436, 'learning_rate': 0.0002, 'epoch': 2.88}
{'loss': 0.4965, 'learning_rate': 0.0002, 'epoch': 2.9}
{'loss': 0.4305, 'learning_rate': 0.0002, 'epoch': 2.92}
{'loss': 0.6613, 'learning_rate': 0.0002, 'epoch': 2.94}
{'loss': 0.5685, 'learning_rate': 0.0002, 'epoch': 2.96}
{'loss': 0.5001, 'learning_rate': 0.0002, 'epoch': 2.99}
{'loss': 0.4771, 'learning_rate': 0.0002, 'epoch': 3.01}
{'loss': 0.5434, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.4313, 'learning_rate': 0.0002, 'epoch': 3.05}
{'loss': 0.3767, 'learning_rate': 0.0002, 'epoch': 3.07}
{'eval_loss': 0.8444089889526367, 'eval_runtime': 407.3871, 'eval_samples_per_second': 2.455, 'eval_steps_per_second': 2.455, 'epoch': 3.08}
{'mmlu_loss': 15.657883204952865, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_college_biology': 0.8125, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6153846153846154, 'mmlu_eval_accuracy_professional_law': 0.43529411764705883, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.75, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy': 0.5941115409096316, 'epoch': 3.08}
{'loss': 0.3348, 'learning_rate': 0.0002, 'epoch': 3.09}
{'loss': 0.3614, 'learning_rate': 0.0002, 'epoch': 3.11}
{'loss': 0.5286, 'learning_rate': 0.0002, 'epoch': 3.13}
{'loss': 0.4422, 'learning_rate': 0.0002, 'epoch': 3.15}
{'loss': 0.408, 'learning_rate': 0.0002, 'epoch': 3.17}
{'loss': 0.3429, 'learning_rate': 0.0002, 'epoch': 3.19}
{'loss': 0.3711, 'learning_rate': 0.0002, 'epoch': 3.21}
{'loss': 0.5451, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.4533, 'learning_rate': 0.0002, 'epoch': 3.25}
{'loss': 0.4001, 'learning_rate': 0.0002, 'epoch': 3.27}
{'loss': 0.3464, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.3719, 'learning_rate': 0.0002, 'epoch': 3.31}
{'loss': 0.5653, 'learning_rate': 0.0002, 'epoch': 3.34}
{'loss': 0.4641, 'learning_rate': 0.0002, 'epoch': 3.36}
{'loss': 0.4081, 'learning_rate': 0.0002, 'epoch': 3.38}
{'loss': 0.3592, 'learning_rate': 0.0002, 'epoch': 3.4}
{'loss': 0.3826, 'learning_rate': 0.0002, 'epoch': 3.42}
{'loss': 0.5501, 'learning_rate': 0.0002, 'epoch': 3.44}
{'loss': 0.4737, 'learning_rate': 0.0002, 'epoch': 3.46}
{'eval_loss': 0.8098863363265991, 'eval_runtime': 407.1342, 'eval_samples_per_second': 2.456, 'eval_steps_per_second': 2.456, 'epoch': 3.46}
{'mmlu_loss': 15.638390507439551, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_moral_scenarios': 0.36, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_security_studies': 0.5555555555555556, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6153846153846154, 'mmlu_eval_accuracy_professional_law': 0.4470588235294118, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy': 0.6002645125047331, 'epoch': 3.46}
{'loss': 0.4164, 'learning_rate': 0.0002, 'epoch': 3.48}
{'loss': 0.3616, 'learning_rate': 0.0002, 'epoch': 3.5}
{'loss': 0.3838, 'learning_rate': 0.0002, 'epoch': 3.52}
{'loss': 0.5628, 'learning_rate': 0.0002, 'epoch': 3.54}
{'loss': 0.4711, 'learning_rate': 0.0002, 'epoch': 3.56}
{'loss': 0.4197, 'learning_rate': 0.0002, 'epoch': 3.58}
{'loss': 0.3584, 'learning_rate': 0.0002, 'epoch': 3.6}
{'loss': 0.3944, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.5562, 'learning_rate': 0.0002, 'epoch': 3.64}
{'loss': 0.4733, 'learning_rate': 0.0002, 'epoch': 3.66}
{'loss': 0.4169, 'learning_rate': 0.0002, 'epoch': 3.69}
{'loss': 0.3711, 'learning_rate': 0.0002, 'epoch': 3.71}
Saving PEFT checkpoint...
{'loss': 0.396, 'learning_rate': 0.0002, 'epoch': 3.73}
{'loss': 0.5616, 'learning_rate': 0.0002, 'epoch': 3.75}
{'loss': 0.4779, 'learning_rate': 0.0002, 'epoch': 3.77}
{'loss': 0.3983, 'learning_rate': 0.0002, 'epoch': 3.79}
{'loss': 0.3597, 'learning_rate': 0.0002, 'epoch': 3.81}
{'train_runtime': 47089.3435, 'train_samples_per_second': 0.629, 'train_steps_per_second': 0.039, 'train_loss': 0.6026181346016961, 'epoch': 3.81}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =        3.81
  train_loss               =      0.6026
  train_runtime            = 13:04:49.34
  train_samples_per_second =       0.629
  train_steps_per_second   =       0.039
***** eval metrics *****
  epoch                   =       3.81
  eval_loss               =     0.8385
  eval_runtime            = 0:06:47.18
  eval_samples_per_second =      2.456
  eval_steps_per_second   =      2.456
