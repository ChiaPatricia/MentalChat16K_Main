A40, guanaco-7b-A40
Namespace(model_name_or_path='huggyllama/llama-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=16, target_max_len=512, dataset='oasst1', dataset_format=None, output_dir='./output/guanaco-7b-A40', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=1875, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/guanaco-7b-A40/runs/Oct12_08-27-42_2118ffn005', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=3, past_index=-1, run_name='./output/guanaco-7b-A40', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 32,
  "transformers_version": "4.31.0"
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model huggyllama/llama-7b...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205
torch.bfloat16 422060032 0.1153065849032323
torch.uint8 3238002688 0.8846206784649213
torch.float32 266240 7.273663184633547e-05
{'loss': 1.3583, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 1.2568, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 1.3264, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 1.4939, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 1.5778, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 1.2714, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 1.2041, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 1.3221, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 1.4325, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 1.5582, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 1.1812, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 1.2143, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 1.2674, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 1.452, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 1.6094, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 1.1993, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 1.1858, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 1.3129, 'learning_rate': 0.0002, 'epoch': 0.33}
{'eval_loss': 1.3209712505340576, 'eval_runtime': 299.4341, 'eval_samples_per_second': 3.34, 'eval_steps_per_second': 3.34, 'epoch': 0.34}
{'mmlu_loss': 1.9123015979160132, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_biology': 0.46875, 'mmlu_eval_accuracy_logical_fallacies': 0.4444444444444444, 'mmlu_eval_accuracy_professional_psychology': 0.391304347826087, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.15384615384615385, 'mmlu_eval_accuracy_high_school_psychology': 0.5166666666666667, 'mmlu_eval_accuracy_electrical_engineering': 0.125, 'mmlu_eval_accuracy_human_aging': 0.4782608695652174, 'mmlu_eval_accuracy_virology': 0.3333333333333333, 'mmlu_eval_accuracy_professional_law': 0.2647058823529412, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_chemistry': 0.18181818181818182, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_professional_accounting': 0.25806451612903225, 'mmlu_eval_accuracy_elementary_mathematics': 0.24390243902439024, 'mmlu_eval_accuracy_prehistory': 0.22857142857142856, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_marketing': 0.56, 'mmlu_eval_accuracy_human_sexuality': 0.16666666666666666, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_professional_medicine': 0.3548387096774194, 'mmlu_eval_accuracy_high_school_world_history': 0.4230769230769231, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.3953488372093023, 'mmlu_eval_accuracy_world_religions': 0.42105263157894735, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_philosophy': 0.4117647058823529, 'mmlu_eval_accuracy_nutrition': 0.3939393939393939, 'mmlu_eval_accuracy_astronomy': 0.3125, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_jurisprudence': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.2692307692307692, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_econometrics': 0.0, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_medical_genetics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_us_foreign_policy': 0.2727272727272727, 'mmlu_eval_accuracy_sociology': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_geography': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.2727272727272727, 'mmlu_eval_accuracy_international_law': 0.3076923076923077, 'mmlu_eval_accuracy_public_relations': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_management': 0.09090909090909091, 'mmlu_eval_accuracy': 0.3243496451720734, 'epoch': 0.34}
{'loss': 1.4194, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 1.6037, 'learning_rate': 0.0002, 'epoch': 0.36}
{'loss': 1.2533, 'learning_rate': 0.0002, 'epoch': 0.38}
{'loss': 1.2378, 'learning_rate': 0.0002, 'epoch': 0.4}
{'loss': 1.3498, 'learning_rate': 0.0002, 'epoch': 0.42}
{'loss': 1.4754, 'learning_rate': 0.0002, 'epoch': 0.44}
{'loss': 1.6487, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 1.1503, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 1.1297, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 1.3307, 'learning_rate': 0.0002, 'epoch': 0.51}
{'loss': 1.4393, 'learning_rate': 0.0002, 'epoch': 0.53}
{'loss': 1.5838, 'learning_rate': 0.0002, 'epoch': 0.54}
{'loss': 1.2143, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 1.1785, 'learning_rate': 0.0002, 'epoch': 0.58}
{'loss': 1.2696, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 1.5247, 'learning_rate': 0.0002, 'epoch': 0.62}
{'loss': 1.5891, 'learning_rate': 0.0002, 'epoch': 0.63}
{'loss': 1.2037, 'learning_rate': 0.0002, 'epoch': 0.65}
{'loss': 1.1959, 'learning_rate': 0.0002, 'epoch': 0.67}
{'eval_loss': 1.3083993196487427, 'eval_runtime': 299.1925, 'eval_samples_per_second': 3.342, 'eval_steps_per_second': 3.342, 'epoch': 0.68}
{'mmlu_loss': 1.9939776819411015, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_logical_fallacies': 0.2222222222222222, 'mmlu_eval_accuracy_professional_psychology': 0.36231884057971014, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.15384615384615385, 'mmlu_eval_accuracy_high_school_psychology': 0.5166666666666667, 'mmlu_eval_accuracy_electrical_engineering': 0.1875, 'mmlu_eval_accuracy_human_aging': 0.391304347826087, 'mmlu_eval_accuracy_virology': 0.3333333333333333, 'mmlu_eval_accuracy_professional_law': 0.2647058823529412, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_chemistry': 0.2727272727272727, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_professional_accounting': 0.22580645161290322, 'mmlu_eval_accuracy_elementary_mathematics': 0.2926829268292683, 'mmlu_eval_accuracy_prehistory': 0.2571428571428571, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.48, 'mmlu_eval_accuracy_human_sexuality': 0.16666666666666666, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.3181818181818182, 'mmlu_eval_accuracy_college_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_high_school_world_history': 0.3076923076923077, 'mmlu_eval_accuracy_security_studies': 0.4444444444444444, 'mmlu_eval_accuracy_miscellaneous': 0.38372093023255816, 'mmlu_eval_accuracy_world_religions': 0.42105263157894735, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_philosophy': 0.35294117647058826, 'mmlu_eval_accuracy_nutrition': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.3125, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_jurisprudence': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_conceptual_physics': 0.2692307692307692, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_medical_genetics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_disputes': 0.3157894736842105, 'mmlu_eval_accuracy_us_foreign_policy': 0.36363636363636365, 'mmlu_eval_accuracy_sociology': 0.5, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_college_medicine': 0.18181818181818182, 'mmlu_eval_accuracy_international_law': 0.3076923076923077, 'mmlu_eval_accuracy_public_relations': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_management': 0.09090909090909091, 'mmlu_eval_accuracy': 0.3226842774585317, 'epoch': 0.68}
{'loss': 1.2677, 'learning_rate': 0.0002, 'epoch': 0.69}
{'loss': 1.4414, 'learning_rate': 0.0002, 'epoch': 0.71}
{'loss': 1.5243, 'learning_rate': 0.0002, 'epoch': 0.73}
{'loss': 1.1465, 'learning_rate': 0.0002, 'epoch': 0.74}
{'loss': 1.1518, 'learning_rate': 0.0002, 'epoch': 0.76}
{'loss': 1.2641, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 1.5341, 'learning_rate': 0.0002, 'epoch': 0.8}
{'loss': 1.6181, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 1.1439, 'learning_rate': 0.0002, 'epoch': 0.83}
{'loss': 1.1383, 'learning_rate': 0.0002, 'epoch': 0.85}
{'loss': 1.2655, 'learning_rate': 0.0002, 'epoch': 0.87}
{'loss': 1.4546, 'learning_rate': 0.0002, 'epoch': 0.89}
{'loss': 1.6344, 'learning_rate': 0.0002, 'epoch': 0.91}
Saving PEFT checkpoint...
{'loss': 1.1332, 'learning_rate': 0.0002, 'epoch': 0.92}
{'loss': 1.1951, 'learning_rate': 0.0002, 'epoch': 0.94}
{'loss': 1.2996, 'learning_rate': 0.0002, 'epoch': 0.96}
{'loss': 1.4442, 'learning_rate': 0.0002, 'epoch': 0.98}
{'loss': 1.5009, 'learning_rate': 0.0002, 'epoch': 1.0}
{'loss': 1.1695, 'learning_rate': 0.0002, 'epoch': 1.02}
{'eval_loss': 1.3099030256271362, 'eval_runtime': 298.945, 'eval_samples_per_second': 3.345, 'eval_steps_per_second': 3.345, 'epoch': 1.02}
{'mmlu_loss': 1.7575895544129365, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.40625, 'mmlu_eval_accuracy_logical_fallacies': 0.3888888888888889, 'mmlu_eval_accuracy_professional_psychology': 0.391304347826087, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.15384615384615385, 'mmlu_eval_accuracy_high_school_psychology': 0.48333333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.125, 'mmlu_eval_accuracy_human_aging': 0.43478260869565216, 'mmlu_eval_accuracy_virology': 0.3333333333333333, 'mmlu_eval_accuracy_professional_law': 0.25882352941176473, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.5, 'mmlu_eval_accuracy_professional_accounting': 0.22580645161290322, 'mmlu_eval_accuracy_elementary_mathematics': 0.2682926829268293, 'mmlu_eval_accuracy_prehistory': 0.22857142857142856, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.52, 'mmlu_eval_accuracy_human_sexuality': 0.16666666666666666, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_medicine': 0.3225806451612903, 'mmlu_eval_accuracy_high_school_world_history': 0.3076923076923077, 'mmlu_eval_accuracy_security_studies': 0.4444444444444444, 'mmlu_eval_accuracy_miscellaneous': 0.3953488372093023, 'mmlu_eval_accuracy_world_religions': 0.42105263157894735, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_philosophy': 0.4411764705882353, 'mmlu_eval_accuracy_nutrition': 0.42424242424242425, 'mmlu_eval_accuracy_astronomy': 0.25, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_jurisprudence': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_conceptual_physics': 0.23076923076923078, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_moral_disputes': 0.3157894736842105, 'mmlu_eval_accuracy_us_foreign_policy': 0.36363636363636365, 'mmlu_eval_accuracy_sociology': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_geography': 0.4090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.22727272727272727, 'mmlu_eval_accuracy_international_law': 0.3076923076923077, 'mmlu_eval_accuracy_public_relations': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.3023255813953488, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_management': 0.18181818181818182, 'mmlu_eval_accuracy': 0.34210770450166367, 'epoch': 1.02}
{'loss': 1.1038, 'learning_rate': 0.0002, 'epoch': 1.03}
{'loss': 1.1853, 'learning_rate': 0.0002, 'epoch': 1.05}
{'loss': 1.3633, 'learning_rate': 0.0002, 'epoch': 1.07}
{'loss': 1.3499, 'learning_rate': 0.0002, 'epoch': 1.09}
{'loss': 1.11, 'learning_rate': 0.0002, 'epoch': 1.11}
{'loss': 1.1507, 'learning_rate': 0.0002, 'epoch': 1.12}
{'loss': 1.2205, 'learning_rate': 0.0002, 'epoch': 1.14}
{'loss': 1.326, 'learning_rate': 0.0002, 'epoch': 1.16}
{'loss': 1.337, 'learning_rate': 0.0002, 'epoch': 1.18}
{'loss': 1.1079, 'learning_rate': 0.0002, 'epoch': 1.2}
{'loss': 1.135, 'learning_rate': 0.0002, 'epoch': 1.22}
{'loss': 1.2313, 'learning_rate': 0.0002, 'epoch': 1.23}
{'loss': 1.3489, 'learning_rate': 0.0002, 'epoch': 1.25}
{'loss': 1.3435, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 1.1378, 'learning_rate': 0.0002, 'epoch': 1.29}
{'loss': 1.1562, 'learning_rate': 0.0002, 'epoch': 1.31}
{'loss': 1.1374, 'learning_rate': 0.0002, 'epoch': 1.32}
{'loss': 1.2827, 'learning_rate': 0.0002, 'epoch': 1.34}
{'eval_loss': 1.3061115741729736, 'eval_runtime': 300.2127, 'eval_samples_per_second': 3.331, 'eval_steps_per_second': 3.331, 'epoch': 1.36}
{'mmlu_loss': 1.7365858693826752, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_biology': 0.46875, 'mmlu_eval_accuracy_logical_fallacies': 0.5, 'mmlu_eval_accuracy_professional_psychology': 0.391304347826087, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_microeconomics': 0.19230769230769232, 'mmlu_eval_accuracy_high_school_psychology': 0.5166666666666667, 'mmlu_eval_accuracy_electrical_engineering': 0.1875, 'mmlu_eval_accuracy_human_aging': 0.43478260869565216, 'mmlu_eval_accuracy_virology': 0.2222222222222222, 'mmlu_eval_accuracy_professional_law': 0.27058823529411763, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_chemistry': 0.18181818181818182, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy_elementary_mathematics': 0.2926829268292683, 'mmlu_eval_accuracy_prehistory': 0.2571428571428571, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_marketing': 0.48, 'mmlu_eval_accuracy_human_sexuality': 0.16666666666666666, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_professional_medicine': 0.3548387096774194, 'mmlu_eval_accuracy_high_school_world_history': 0.38461538461538464, 'mmlu_eval_accuracy_security_studies': 0.37037037037037035, 'mmlu_eval_accuracy_miscellaneous': 0.3953488372093023, 'mmlu_eval_accuracy_world_religions': 0.2631578947368421, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_philosophy': 0.4117647058823529, 'mmlu_eval_accuracy_nutrition': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.3125, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_jurisprudence': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_conceptual_physics': 0.3076923076923077, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_medical_genetics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_disputes': 0.3157894736842105, 'mmlu_eval_accuracy_us_foreign_policy': 0.2727272727272727, 'mmlu_eval_accuracy_sociology': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_geography': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.3181818181818182, 'mmlu_eval_accuracy_international_law': 0.46153846153846156, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.32558139534883723, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_management': 0.09090909090909091, 'mmlu_eval_accuracy': 0.3272616935026928, 'epoch': 1.36}
{'loss': 1.4012, 'learning_rate': 0.0002, 'epoch': 1.36}
{'loss': 1.1568, 'learning_rate': 0.0002, 'epoch': 1.38}
{'loss': 1.1292, 'learning_rate': 0.0002, 'epoch': 1.4}
{'loss': 1.1779, 'learning_rate': 0.0002, 'epoch': 1.41}
{'loss': 1.3296, 'learning_rate': 0.0002, 'epoch': 1.43}
{'loss': 1.3678, 'learning_rate': 0.0002, 'epoch': 1.45}
{'loss': 1.1063, 'learning_rate': 0.0002, 'epoch': 1.47}
{'loss': 1.1107, 'learning_rate': 0.0002, 'epoch': 1.49}
{'loss': 1.166, 'learning_rate': 0.0002, 'epoch': 1.51}
{'loss': 1.3296, 'learning_rate': 0.0002, 'epoch': 1.52}
{'loss': 1.3905, 'learning_rate': 0.0002, 'epoch': 1.54}
{'loss': 1.1734, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 1.1114, 'learning_rate': 0.0002, 'epoch': 1.58}
{'loss': 1.1591, 'learning_rate': 0.0002, 'epoch': 1.6}
{'loss': 1.3089, 'learning_rate': 0.0002, 'epoch': 1.61}
{'loss': 1.3292, 'learning_rate': 0.0002, 'epoch': 1.63}
{'loss': 1.1098, 'learning_rate': 0.0002, 'epoch': 1.65}
{'loss': 1.1086, 'learning_rate': 0.0002, 'epoch': 1.67}
{'loss': 1.201, 'learning_rate': 0.0002, 'epoch': 1.69}
{'eval_loss': 1.2998871803283691, 'eval_runtime': 298.5087, 'eval_samples_per_second': 3.35, 'eval_steps_per_second': 3.35, 'epoch': 1.7}
{'mmlu_loss': 2.0231571954344707, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.46875, 'mmlu_eval_accuracy_logical_fallacies': 0.4444444444444444, 'mmlu_eval_accuracy_professional_psychology': 0.34782608695652173, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_high_school_psychology': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.1875, 'mmlu_eval_accuracy_human_aging': 0.391304347826087, 'mmlu_eval_accuracy_virology': 0.2222222222222222, 'mmlu_eval_accuracy_professional_law': 0.2529411764705882, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_college_mathematics': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_chemistry': 0.22727272727272727, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy_elementary_mathematics': 0.24390243902439024, 'mmlu_eval_accuracy_prehistory': 0.3142857142857143, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.56, 'mmlu_eval_accuracy_human_sexuality': 0.25, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.3181818181818182, 'mmlu_eval_accuracy_college_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_professional_medicine': 0.3548387096774194, 'mmlu_eval_accuracy_high_school_world_history': 0.2692307692307692, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.4883720930232558, 'mmlu_eval_accuracy_world_religions': 0.42105263157894735, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.4411764705882353, 'mmlu_eval_accuracy_nutrition': 0.3939393939393939, 'mmlu_eval_accuracy_astronomy': 0.3125, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_conceptual_physics': 0.2692307692307692, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.3448275862068966, 'mmlu_eval_accuracy_medical_genetics': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.2894736842105263, 'mmlu_eval_accuracy_us_foreign_policy': 0.36363636363636365, 'mmlu_eval_accuracy_sociology': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_college_medicine': 0.22727272727272727, 'mmlu_eval_accuracy_international_law': 0.23076923076923078, 'mmlu_eval_accuracy_public_relations': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_management': 0.2727272727272727, 'mmlu_eval_accuracy': 0.33997968830884046, 'epoch': 1.7}
{'loss': 1.2919, 'learning_rate': 0.0002, 'epoch': 1.7}
{'loss': 1.354, 'learning_rate': 0.0002, 'epoch': 1.72}
{'loss': 1.1279, 'learning_rate': 0.0002, 'epoch': 1.74}
{'loss': 1.1067, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 1.1849, 'learning_rate': 0.0002, 'epoch': 1.78}
{'loss': 1.3227, 'learning_rate': 0.0002, 'epoch': 1.8}
{'loss': 1.3746, 'learning_rate': 0.0002, 'epoch': 1.81}
Saving PEFT checkpoint...
{'loss': 1.1467, 'learning_rate': 0.0002, 'epoch': 1.83}
{'loss': 1.1383, 'learning_rate': 0.0002, 'epoch': 1.85}
{'loss': 1.1608, 'learning_rate': 0.0002, 'epoch': 1.87}
{'loss': 1.2971, 'learning_rate': 0.0002, 'epoch': 1.89}
{'loss': 1.337, 'learning_rate': 0.0002, 'epoch': 1.9}
{'loss': 1.0637, 'learning_rate': 0.0002, 'epoch': 1.92}
{'loss': 1.0969, 'learning_rate': 0.0002, 'epoch': 1.94}
{'loss': 1.2455, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 1.3794, 'learning_rate': 0.0002, 'epoch': 1.98}
{'loss': 1.3726, 'learning_rate': 0.0002, 'epoch': 2.0}
{'loss': 1.0368, 'learning_rate': 0.0002, 'epoch': 2.01}
{'loss': 1.0273, 'learning_rate': 0.0002, 'epoch': 2.03}
{'eval_loss': 1.3122644424438477, 'eval_runtime': 298.3859, 'eval_samples_per_second': 3.351, 'eval_steps_per_second': 3.351, 'epoch': 2.03}
{'mmlu_loss': 1.8386083518189593, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_biology': 0.34375, 'mmlu_eval_accuracy_logical_fallacies': 0.3888888888888889, 'mmlu_eval_accuracy_professional_psychology': 0.43478260869565216, 'mmlu_eval_accuracy_college_physics': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_microeconomics': 0.15384615384615385, 'mmlu_eval_accuracy_high_school_psychology': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.125, 'mmlu_eval_accuracy_human_aging': 0.43478260869565216, 'mmlu_eval_accuracy_virology': 0.2222222222222222, 'mmlu_eval_accuracy_professional_law': 0.25882352941176473, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_chemistry': 0.22727272727272727, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.5, 'mmlu_eval_accuracy_professional_accounting': 0.3548387096774194, 'mmlu_eval_accuracy_elementary_mathematics': 0.2682926829268293, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_marketing': 0.52, 'mmlu_eval_accuracy_human_sexuality': 0.25, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_professional_medicine': 0.3548387096774194, 'mmlu_eval_accuracy_high_school_world_history': 0.4230769230769231, 'mmlu_eval_accuracy_security_studies': 0.37037037037037035, 'mmlu_eval_accuracy_miscellaneous': 0.4418604651162791, 'mmlu_eval_accuracy_world_religions': 0.3684210526315789, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.4411764705882353, 'mmlu_eval_accuracy_nutrition': 0.42424242424242425, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_jurisprudence': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_conceptual_physics': 0.3076923076923077, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_medical_genetics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_disputes': 0.3157894736842105, 'mmlu_eval_accuracy_us_foreign_policy': 0.2727272727272727, 'mmlu_eval_accuracy_sociology': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_geography': 0.4090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.18181818181818182, 'mmlu_eval_accuracy_international_law': 0.23076923076923078, 'mmlu_eval_accuracy_public_relations': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.3023255813953488, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_management': 0.18181818181818182, 'mmlu_eval_accuracy': 0.32729933070767914, 'epoch': 2.03}
{'loss': 1.001, 'learning_rate': 0.0002, 'epoch': 2.05}
{'loss': 1.1419, 'learning_rate': 0.0002, 'epoch': 2.07}
{'loss': 1.0467, 'learning_rate': 0.0002, 'epoch': 2.09}
{'loss': 0.9851, 'learning_rate': 0.0002, 'epoch': 2.1}
{'loss': 1.0586, 'learning_rate': 0.0002, 'epoch': 2.12}
{'loss': 1.1213, 'learning_rate': 0.0002, 'epoch': 2.14}
{'loss': 1.1342, 'learning_rate': 0.0002, 'epoch': 2.16}
{'loss': 1.0682, 'learning_rate': 0.0002, 'epoch': 2.18}
{'loss': 0.9672, 'learning_rate': 0.0002, 'epoch': 2.19}
{'loss': 1.0511, 'learning_rate': 0.0002, 'epoch': 2.21}
{'loss': 1.0618, 'learning_rate': 0.0002, 'epoch': 2.23}
{'loss': 1.1246, 'learning_rate': 0.0002, 'epoch': 2.25}
{'loss': 1.0734, 'learning_rate': 0.0002, 'epoch': 2.27}
{'loss': 0.9529, 'learning_rate': 0.0002, 'epoch': 2.29}
{'loss': 1.0064, 'learning_rate': 0.0002, 'epoch': 2.3}
{'loss': 1.0349, 'learning_rate': 0.0002, 'epoch': 2.32}
{'loss': 1.1049, 'learning_rate': 0.0002, 'epoch': 2.34}
{'loss': 1.0919, 'learning_rate': 0.0002, 'epoch': 2.36}
{'eval_loss': 1.3572050333023071, 'eval_runtime': 300.4311, 'eval_samples_per_second': 3.329, 'eval_steps_per_second': 3.329, 'epoch': 2.37}
{'mmlu_loss': 1.5839753841590756, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.5, 'mmlu_eval_accuracy_logical_fallacies': 0.5, 'mmlu_eval_accuracy_professional_psychology': 0.3333333333333333, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_microeconomics': 0.23076923076923078, 'mmlu_eval_accuracy_high_school_psychology': 0.48333333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.1875, 'mmlu_eval_accuracy_human_aging': 0.4782608695652174, 'mmlu_eval_accuracy_virology': 0.2777777777777778, 'mmlu_eval_accuracy_professional_law': 0.2823529411764706, 'mmlu_eval_accuracy_high_school_statistics': 0.21739130434782608, 'mmlu_eval_accuracy_college_mathematics': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_chemistry': 0.13636363636363635, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_professional_accounting': 0.3225806451612903, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_prehistory': 0.3142857142857143, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_marketing': 0.56, 'mmlu_eval_accuracy_human_sexuality': 0.08333333333333333, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_high_school_world_history': 0.38461538461538464, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.4069767441860465, 'mmlu_eval_accuracy_world_religions': 0.21052631578947367, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_philosophy': 0.4411764705882353, 'mmlu_eval_accuracy_nutrition': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.4375, 'mmlu_eval_accuracy_high_school_mathematics': 0.13793103448275862, 'mmlu_eval_accuracy_jurisprudence': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.2692307692307692, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_medical_genetics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.39473684210526316, 'mmlu_eval_accuracy_us_foreign_policy': 0.36363636363636365, 'mmlu_eval_accuracy_sociology': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_geography': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.22727272727272727, 'mmlu_eval_accuracy_international_law': 0.3076923076923077, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_management': 0.18181818181818182, 'mmlu_eval_accuracy': 0.3249659850713747, 'epoch': 2.37}
{'loss': 0.9972, 'learning_rate': 0.0002, 'epoch': 2.38}
{'loss': 1.0469, 'learning_rate': 0.0002, 'epoch': 2.39}
{'loss': 1.0402, 'learning_rate': 0.0002, 'epoch': 2.41}
{'loss': 1.1272, 'learning_rate': 0.0002, 'epoch': 2.43}
{'loss': 1.0769, 'learning_rate': 0.0002, 'epoch': 2.45}
{'loss': 1.0438, 'learning_rate': 0.0002, 'epoch': 2.47}
{'loss': 1.0934, 'learning_rate': 0.0002, 'epoch': 2.48}
{'loss': 1.0285, 'learning_rate': 0.0002, 'epoch': 2.5}
{'loss': 1.0774, 'learning_rate': 0.0002, 'epoch': 2.52}
{'loss': 1.1088, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.9771, 'learning_rate': 0.0002, 'epoch': 2.56}
{'loss': 1.016, 'learning_rate': 0.0002, 'epoch': 2.58}
{'loss': 1.0346, 'learning_rate': 0.0002, 'epoch': 2.59}
{'loss': 1.129, 'learning_rate': 0.0002, 'epoch': 2.61}
{'loss': 1.1423, 'learning_rate': 0.0002, 'epoch': 2.63}
{'loss': 0.9845, 'learning_rate': 0.0002, 'epoch': 2.65}
{'loss': 1.0413, 'learning_rate': 0.0002, 'epoch': 2.67}
{'loss': 1.1339, 'learning_rate': 0.0002, 'epoch': 2.68}
{'loss': 1.1569, 'learning_rate': 0.0002, 'epoch': 2.7}
{'eval_loss': 1.3459551334381104, 'eval_runtime': 298.777, 'eval_samples_per_second': 3.347, 'eval_steps_per_second': 3.347, 'epoch': 2.71}
{'mmlu_loss': 1.8707040866008386, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_biology': 0.53125, 'mmlu_eval_accuracy_logical_fallacies': 0.5, 'mmlu_eval_accuracy_professional_psychology': 0.391304347826087, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_microeconomics': 0.19230769230769232, 'mmlu_eval_accuracy_high_school_psychology': 0.45, 'mmlu_eval_accuracy_electrical_engineering': 0.25, 'mmlu_eval_accuracy_human_aging': 0.4782608695652174, 'mmlu_eval_accuracy_virology': 0.3333333333333333, 'mmlu_eval_accuracy_professional_law': 0.2529411764705882, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_college_mathematics': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_chemistry': 0.13636363636363635, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_professional_accounting': 0.3225806451612903, 'mmlu_eval_accuracy_elementary_mathematics': 0.2926829268292683, 'mmlu_eval_accuracy_prehistory': 0.34285714285714286, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_marketing': 0.56, 'mmlu_eval_accuracy_human_sexuality': 0.08333333333333333, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_professional_medicine': 0.3225806451612903, 'mmlu_eval_accuracy_high_school_world_history': 0.38461538461538464, 'mmlu_eval_accuracy_security_studies': 0.4074074074074074, 'mmlu_eval_accuracy_miscellaneous': 0.4069767441860465, 'mmlu_eval_accuracy_world_religions': 0.2631578947368421, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_philosophy': 0.47058823529411764, 'mmlu_eval_accuracy_nutrition': 0.3939393939393939, 'mmlu_eval_accuracy_astronomy': 0.4375, 'mmlu_eval_accuracy_high_school_mathematics': 0.13793103448275862, 'mmlu_eval_accuracy_jurisprudence': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_conceptual_physics': 0.3076923076923077, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.3448275862068966, 'mmlu_eval_accuracy_medical_genetics': 0.6363636363636364, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_us_foreign_policy': 0.2727272727272727, 'mmlu_eval_accuracy_sociology': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_geography': 0.5909090909090909, 'mmlu_eval_accuracy_college_medicine': 0.22727272727272727, 'mmlu_eval_accuracy_international_law': 0.38461538461538464, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.32558139534883723, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_management': 0.18181818181818182, 'mmlu_eval_accuracy': 0.3447418849207184, 'epoch': 2.71}
{'loss': 1.0773, 'learning_rate': 0.0002, 'epoch': 2.72}
Saving PEFT checkpoint...
{'loss': 0.975, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 1.0282, 'learning_rate': 0.0002, 'epoch': 2.76}
{'loss': 1.062, 'learning_rate': 0.0002, 'epoch': 2.77}
{'loss': 1.0935, 'learning_rate': 0.0002, 'epoch': 2.79}
{'loss': 1.1202, 'learning_rate': 0.0002, 'epoch': 2.81}
{'loss': 0.9905, 'learning_rate': 0.0002, 'epoch': 2.83}
{'loss': 1.0388, 'learning_rate': 0.0002, 'epoch': 2.85}
{'loss': 1.0884, 'learning_rate': 0.0002, 'epoch': 2.87}
{'loss': 1.1318, 'learning_rate': 0.0002, 'epoch': 2.88}
{'loss': 1.1205, 'learning_rate': 0.0002, 'epoch': 2.9}
{'loss': 1.0555, 'learning_rate': 0.0002, 'epoch': 2.92}
{'loss': 0.9882, 'learning_rate': 0.0002, 'epoch': 2.94}
{'loss': 1.1079, 'learning_rate': 0.0002, 'epoch': 2.96}
{'loss': 1.1742, 'learning_rate': 0.0002, 'epoch': 2.97}
{'loss': 1.1136, 'learning_rate': 0.0002, 'epoch': 2.99}
{'loss': 0.9136, 'learning_rate': 0.0002, 'epoch': 3.01}
{'loss': 0.9269, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.9426, 'learning_rate': 0.0002, 'epoch': 3.05}
{'eval_loss': 1.381097435951233, 'eval_runtime': 300.076, 'eval_samples_per_second': 3.332, 'eval_steps_per_second': 3.332, 'epoch': 3.05}
{'mmlu_loss': 2.134255257484416, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_biology': 0.53125, 'mmlu_eval_accuracy_logical_fallacies': 0.5, 'mmlu_eval_accuracy_professional_psychology': 0.42028985507246375, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_microeconomics': 0.23076923076923078, 'mmlu_eval_accuracy_high_school_psychology': 0.5166666666666667, 'mmlu_eval_accuracy_electrical_engineering': 0.25, 'mmlu_eval_accuracy_human_aging': 0.391304347826087, 'mmlu_eval_accuracy_virology': 0.2777777777777778, 'mmlu_eval_accuracy_professional_law': 0.2647058823529412, 'mmlu_eval_accuracy_high_school_statistics': 0.17391304347826086, 'mmlu_eval_accuracy_college_mathematics': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_chemistry': 0.09090909090909091, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_professional_accounting': 0.3225806451612903, 'mmlu_eval_accuracy_elementary_mathematics': 0.21951219512195122, 'mmlu_eval_accuracy_prehistory': 0.34285714285714286, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_marketing': 0.56, 'mmlu_eval_accuracy_human_sexuality': 0.16666666666666666, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_professional_medicine': 0.3548387096774194, 'mmlu_eval_accuracy_high_school_world_history': 0.19230769230769232, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_miscellaneous': 0.37209302325581395, 'mmlu_eval_accuracy_world_religions': 0.2631578947368421, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.3333333333333333, 'mmlu_eval_accuracy_philosophy': 0.47058823529411764, 'mmlu_eval_accuracy_nutrition': 0.36363636363636365, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_mathematics': 0.13793103448275862, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_business_ethics': 0.2727272727272727, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.20689655172413793, 'mmlu_eval_accuracy_medical_genetics': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.39473684210526316, 'mmlu_eval_accuracy_us_foreign_policy': 0.2727272727272727, 'mmlu_eval_accuracy_sociology': 0.5, 'mmlu_eval_accuracy_high_school_geography': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.22727272727272727, 'mmlu_eval_accuracy_international_law': 0.46153846153846156, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_management': 0.18181818181818182, 'mmlu_eval_accuracy': 0.3301250863249564, 'epoch': 3.05}
{'loss': 0.9136, 'learning_rate': 0.0002, 'epoch': 3.07}
{'loss': 0.755, 'learning_rate': 0.0002, 'epoch': 3.08}
{'loss': 0.783, 'learning_rate': 0.0002, 'epoch': 3.1}
{'loss': 0.9635, 'learning_rate': 0.0002, 'epoch': 3.12}
{'loss': 0.9387, 'learning_rate': 0.0002, 'epoch': 3.14}
{'loss': 0.9234, 'learning_rate': 0.0002, 'epoch': 3.16}
{'loss': 0.7731, 'learning_rate': 0.0002, 'epoch': 3.17}
{'loss': 0.7953, 'learning_rate': 0.0002, 'epoch': 3.19}
{'loss': 0.9082, 'learning_rate': 0.0002, 'epoch': 3.21}
{'loss': 0.8961, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.8922, 'learning_rate': 0.0002, 'epoch': 3.25}
{'loss': 0.7901, 'learning_rate': 0.0002, 'epoch': 3.26}
{'loss': 0.7719, 'learning_rate': 0.0002, 'epoch': 3.28}
{'loss': 0.9493, 'learning_rate': 0.0002, 'epoch': 3.3}
{'loss': 0.916, 'learning_rate': 0.0002, 'epoch': 3.32}
{'loss': 0.9321, 'learning_rate': 0.0002, 'epoch': 3.34}
{'loss': 0.8169, 'learning_rate': 0.0002, 'epoch': 3.36}
{'loss': 0.7479, 'learning_rate': 0.0002, 'epoch': 3.37}
{'loss': 0.924, 'learning_rate': 0.0002, 'epoch': 3.39}
{'eval_loss': 1.3927901983261108, 'eval_runtime': 297.222, 'eval_samples_per_second': 3.364, 'eval_steps_per_second': 3.364, 'epoch': 3.39}
{'mmlu_loss': 1.7765332255232653, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_logical_fallacies': 0.5, 'mmlu_eval_accuracy_professional_psychology': 0.36231884057971014, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_microeconomics': 0.23076923076923078, 'mmlu_eval_accuracy_high_school_psychology': 0.43333333333333335, 'mmlu_eval_accuracy_electrical_engineering': 0.1875, 'mmlu_eval_accuracy_human_aging': 0.43478260869565216, 'mmlu_eval_accuracy_virology': 0.2777777777777778, 'mmlu_eval_accuracy_professional_law': 0.2411764705882353, 'mmlu_eval_accuracy_high_school_statistics': 0.21739130434782608, 'mmlu_eval_accuracy_college_mathematics': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_chemistry': 0.09090909090909091, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_professional_accounting': 0.3225806451612903, 'mmlu_eval_accuracy_elementary_mathematics': 0.2926829268292683, 'mmlu_eval_accuracy_prehistory': 0.3142857142857143, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_marketing': 0.56, 'mmlu_eval_accuracy_human_sexuality': 0.16666666666666666, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_computer_science': 0.3333333333333333, 'mmlu_eval_accuracy_professional_medicine': 0.3225806451612903, 'mmlu_eval_accuracy_high_school_world_history': 0.19230769230769232, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.4186046511627907, 'mmlu_eval_accuracy_world_religions': 0.21052631578947367, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_philosophy': 0.4117647058823529, 'mmlu_eval_accuracy_nutrition': 0.42424242424242425, 'mmlu_eval_accuracy_astronomy': 0.4375, 'mmlu_eval_accuracy_high_school_mathematics': 0.13793103448275862, 'mmlu_eval_accuracy_jurisprudence': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.4117647058823529, 'mmlu_eval_accuracy_business_ethics': 0.2727272727272727, 'mmlu_eval_accuracy_conceptual_physics': 0.3076923076923077, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_clinical_knowledge': 0.3793103448275862, 'mmlu_eval_accuracy_medical_genetics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_us_foreign_policy': 0.36363636363636365, 'mmlu_eval_accuracy_sociology': 0.5, 'mmlu_eval_accuracy_high_school_geography': 0.5909090909090909, 'mmlu_eval_accuracy_college_medicine': 0.22727272727272727, 'mmlu_eval_accuracy_international_law': 0.46153846153846156, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_management': 0.18181818181818182, 'mmlu_eval_accuracy': 0.3274888509723248, 'epoch': 3.39}
{'train_runtime': 35901.9351, 'train_samples_per_second': 0.836, 'train_steps_per_second': 0.052, 'train_loss': 1.1718460372924804, 'epoch': 3.4}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =        3.4
  train_loss               =     1.1718
  train_runtime            = 9:58:21.93
  train_samples_per_second =      0.836
  train_steps_per_second   =      0.052
***** eval metrics *****
  epoch                   =        3.4
  eval_loss               =     1.3906
  eval_runtime            = 0:04:58.24
  eval_samples_per_second =      3.353
  eval_steps_per_second   =      3.353
