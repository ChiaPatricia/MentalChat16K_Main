Namespace(model_name_or_path='huggyllama/llama-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=16, target_max_len=512, dataset='oasst1', dataset_format=None, output_dir='./output/guanaco-7b', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=1875, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/guanaco-7b/runs/Oct12_04-07-12_211affn019', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=3, past_index=-1, run_name='./output/guanaco-7b', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 32,
  "transformers_version": "4.31.0"
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model huggyllama/llama-7b...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205
torch.float32 422326272 0.11537932153507864
torch.uint8 3238002688 0.8846206784649213
{'loss': 1.3559, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 1.2567, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 1.3264, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 1.4932, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 1.5781, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 1.2712, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 1.2042, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 1.3215, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 1.4331, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 1.5571, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 1.1816, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 1.2145, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 1.2674, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 1.4519, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 1.6089, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 1.1987, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 1.1858, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 1.3125, 'learning_rate': 0.0002, 'epoch': 0.33}
{'eval_loss': 1.3205409049987793, 'eval_runtime': 1181.5405, 'eval_samples_per_second': 0.846, 'eval_steps_per_second': 0.846, 'epoch': 0.34}
