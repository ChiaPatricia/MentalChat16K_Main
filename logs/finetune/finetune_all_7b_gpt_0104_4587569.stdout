Alpaca prompt format. modified 'unk_token' = 0 in qlora.py (originally 2 because pad_token_id = 2 for zephyr). model-gpt-date Finetune base models on gpt generated data. ~6 epochs
Namespace(model_name_or_path='mistralai/Mixtral-8x7B-v0.1', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/lab/self_instruct_gpt3.5_instruction.csv', dataset_format='alpaca', output_dir='./output/Mixtral-8x7B-v0.1-gpt-0104', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=480, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mixtral-8x7B-v0.1-gpt-0104/runs/Jan10_00-38-43_2115ga003', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=120, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=48, dataloader_num_workers=3, past_index=-1, run_name='./output/Mixtral-8x7B-v0.1-gpt-0104', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mixtral-8x7B-v0.1...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 484450304.0 || all params: 24451510272 || trainable: 1.9812694537513107
torch.bfloat16 1231052800 0.05034669786470031
torch.uint8 23220191232 0.9496424136463255
torch.float32 266240 1.0888488974232306e-05
{'loss': 0.8742, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.8022, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.6639, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.7766, 'learning_rate': 0.0002, 'epoch': 0.66}
{'eval_loss': 0.7285072207450867, 'eval_runtime': 304.6389, 'eval_samples_per_second': 3.283, 'eval_steps_per_second': 0.41, 'epoch': 0.79}
{'mmlu_loss': 2.047788199658195, 'mmlu_eval_accuracy_elementary_mathematics': 0.6585365853658537, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_prehistory': 0.8, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7741935483870968, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8076923076923077, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_machine_learning': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.6774193548387096, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_psychology': 0.6956521739130435, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_professional_law': 0.5470588235294118, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.49, 'mmlu_eval_accuracy_miscellaneous': 0.7906976744186046, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy': 0.7011803548172886, 'epoch': 0.79}
{'loss': 0.7432, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.636, 'learning_rate': 0.0002, 'epoch': 0.99}
{'loss': 0.7164, 'learning_rate': 0.0002, 'epoch': 1.15}
{'loss': 0.6769, 'learning_rate': 0.0002, 'epoch': 1.32}
{'loss': 0.6049, 'learning_rate': 0.0002, 'epoch': 1.48}
{'eval_loss': 0.6969893574714661, 'eval_runtime': 304.3487, 'eval_samples_per_second': 3.286, 'eval_steps_per_second': 0.411, 'epoch': 1.58}
{'mmlu_loss': 2.0337954678883157, 'mmlu_eval_accuracy_elementary_mathematics': 0.6341463414634146, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_prehistory': 0.8, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8076923076923077, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_machine_learning': 0.6363636363636364, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.6451612903225806, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_psychology': 0.7101449275362319, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_law': 0.5647058823529412, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_moral_scenarios': 0.49, 'mmlu_eval_accuracy_miscellaneous': 0.8023255813953488, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy': 0.7004519676240859, 'epoch': 1.58}
{'loss': 0.7065, 'learning_rate': 0.0002, 'epoch': 1.65}
{'loss': 0.6706, 'learning_rate': 0.0002, 'epoch': 1.81}
{'loss': 0.5949, 'learning_rate': 0.0002, 'epoch': 1.98}
Saving PEFT checkpoint...
{'loss': 0.6596, 'learning_rate': 0.0002, 'epoch': 2.14}
{'loss': 0.6066, 'learning_rate': 0.0002, 'epoch': 2.3}
{'eval_loss': 0.6946532130241394, 'eval_runtime': 302.9294, 'eval_samples_per_second': 3.301, 'eval_steps_per_second': 0.413, 'epoch': 2.37}
{'mmlu_loss': 2.0822541719923415, 'mmlu_eval_accuracy_elementary_mathematics': 0.6341463414634146, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.8064516129032258, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8076923076923077, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_european_history': 0.8888888888888888, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_moral_disputes': 0.7105263157894737, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.6451612903225806, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_psychology': 0.7101449275362319, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_law': 0.5588235294117647, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_moral_scenarios': 0.44, 'mmlu_eval_accuracy_miscellaneous': 0.8023255813953488, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy': 0.7080042677142393, 'epoch': 2.37}
{'loss': 0.5618, 'learning_rate': 0.0002, 'epoch': 2.47}
{'loss': 0.6563, 'learning_rate': 0.0002, 'epoch': 2.63}
{'loss': 0.6131, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.5668, 'learning_rate': 0.0002, 'epoch': 2.96}
{'loss': 0.6049, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7057806849479675, 'eval_runtime': 302.7637, 'eval_samples_per_second': 3.303, 'eval_steps_per_second': 0.413, 'epoch': 3.16}
{'mmlu_loss': 2.0732533702005944, 'mmlu_eval_accuracy_elementary_mathematics': 0.6097560975609756, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7741935483870968, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_electrical_engineering': 0.875, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.5714285714285714, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.7105263157894737, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.6363636363636364, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.6774193548387096, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_psychology': 0.7536231884057971, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_law': 0.5647058823529412, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.44, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy': 0.7045104185845575, 'epoch': 3.16}
{'loss': 0.5302, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.5311, 'learning_rate': 0.0002, 'epoch': 3.46}
{'loss': 0.5986, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.5382, 'learning_rate': 0.0002, 'epoch': 3.79}
{'loss': 0.548, 'learning_rate': 0.0002, 'epoch': 3.95}
{'eval_loss': 0.7022013068199158, 'eval_runtime': 302.8273, 'eval_samples_per_second': 3.302, 'eval_steps_per_second': 0.413, 'epoch': 3.95}
{'mmlu_loss': 2.124857733026147, 'mmlu_eval_accuracy_elementary_mathematics': 0.6341463414634146, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_prehistory': 0.8571428571428571, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7741935483870968, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8076923076923077, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.7105263157894737, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.6, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_security_studies': 0.8518518518518519, 'mmlu_eval_accuracy_professional_accounting': 0.7096774193548387, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_psychology': 0.7246376811594203, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_law': 0.5352941176470588, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.47, 'mmlu_eval_accuracy_miscellaneous': 0.8023255813953488, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7674418604651163, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy': 0.703310646714917, 'epoch': 3.95}
Saving PEFT checkpoint...
{'loss': 0.5568, 'learning_rate': 0.0002, 'epoch': 4.12}
{'loss': 0.4569, 'learning_rate': 0.0002, 'epoch': 4.28}
{'loss': 0.4977, 'learning_rate': 0.0002, 'epoch': 4.44}
{'loss': 0.5371, 'learning_rate': 0.0002, 'epoch': 4.61}
{'eval_loss': 0.7630207538604736, 'eval_runtime': 302.6244, 'eval_samples_per_second': 3.304, 'eval_steps_per_second': 0.413, 'epoch': 4.74}
{'mmlu_loss': 2.1242739831407866, 'mmlu_eval_accuracy_elementary_mathematics': 0.5853658536585366, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7741935483870968, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_electrical_engineering': 0.8125, 'mmlu_eval_accuracy_college_biology': 0.8125, 'mmlu_eval_accuracy_conceptual_physics': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.5714285714285714, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_moral_disputes': 0.7105263157894737, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.6363636363636364, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_psychology': 0.7246376811594203, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_professional_law': 0.5411764705882353, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_moral_scenarios': 0.49, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy': 0.7094449914112163, 'epoch': 4.74}
{'loss': 0.4789, 'learning_rate': 0.0002, 'epoch': 4.77}
{'loss': 0.5182, 'learning_rate': 0.0002, 'epoch': 4.94}
{'loss': 0.4777, 'learning_rate': 0.0002, 'epoch': 5.1}
{'loss': 0.3848, 'learning_rate': 0.0002, 'epoch': 5.27}
{'loss': 0.4613, 'learning_rate': 0.0002, 'epoch': 5.43}
{'eval_loss': 0.842500627040863, 'eval_runtime': 303.2053, 'eval_samples_per_second': 3.298, 'eval_steps_per_second': 0.412, 'epoch': 5.53}
{'mmlu_loss': 2.184044106552998, 'mmlu_eval_accuracy_elementary_mathematics': 0.5609756097560976, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.7714285714285715, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_electrical_engineering': 0.8125, 'mmlu_eval_accuracy_college_biology': 0.8125, 'mmlu_eval_accuracy_conceptual_physics': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.7368421052631579, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.6451612903225806, 'mmlu_eval_accuracy_high_school_chemistry': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_psychology': 0.6956521739130435, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.8235294117647058, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_law': 0.5352941176470588, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.8488372093023255, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy': 0.6976539760074967, 'epoch': 5.53}
{'loss': 0.4548, 'learning_rate': 0.0002, 'epoch': 5.6}
{'loss': 0.4065, 'learning_rate': 0.0002, 'epoch': 5.76}
{'loss': 0.4855, 'learning_rate': 0.0002, 'epoch': 5.93}
Saving PEFT checkpoint...
{'loss': 0.3963, 'learning_rate': 0.0002, 'epoch': 6.09}
{'loss': 0.3116, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.846142053604126, 'eval_runtime': 304.6081, 'eval_samples_per_second': 3.283, 'eval_steps_per_second': 0.41, 'epoch': 6.32}
{'mmlu_loss': 2.153357394039631, 'mmlu_eval_accuracy_elementary_mathematics': 0.6585365853658537, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.7714285714285715, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_electrical_engineering': 0.875, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_moral_disputes': 0.7105263157894737, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_college_mathematics': 0.7272727272727273, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_chemistry': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_professional_psychology': 0.7101449275362319, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_law': 0.5411764705882353, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.48, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6976744186046512, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy': 0.7034666881805411, 'epoch': 6.32}
{'loss': 0.417, 'learning_rate': 0.0002, 'epoch': 6.42}
{'loss': 0.3779, 'learning_rate': 0.0002, 'epoch': 6.58}
{'loss': 0.3343, 'learning_rate': 0.0002, 'epoch': 6.75}
{'loss': 0.4367, 'learning_rate': 0.0002, 'epoch': 6.91}
{'loss': 0.3253, 'learning_rate': 0.0002, 'epoch': 7.08}
{'eval_loss': 0.9492945671081543, 'eval_runtime': 305.6814, 'eval_samples_per_second': 3.271, 'eval_steps_per_second': 0.409, 'epoch': 7.11}
{'mmlu_loss': 2.1820535821219287, 'mmlu_eval_accuracy_elementary_mathematics': 0.6341463414634146, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.7714285714285715, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_european_history': 0.9444444444444444, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_college_mathematics': 0.6363636363636364, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_law': 0.5352941176470588, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_world_history': 0.8846153846153846, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.46, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7209302325581395, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy': 0.6938321311308393, 'epoch': 7.11}
{'loss': 0.2637, 'learning_rate': 0.0002, 'epoch': 7.24}
{'loss': 0.3463, 'learning_rate': 0.0002, 'epoch': 7.41}
{'loss': 0.3084, 'learning_rate': 0.0002, 'epoch': 7.57}
{'loss': 0.2938, 'learning_rate': 0.0002, 'epoch': 7.74}
{'loss': 0.3751, 'learning_rate': 0.0002, 'epoch': 7.9}
{'eval_loss': 0.8706464171409607, 'eval_runtime': 307.2062, 'eval_samples_per_second': 3.255, 'eval_steps_per_second': 0.407, 'epoch': 7.9}
{'mmlu_loss': 2.275080408900976, 'mmlu_eval_accuracy_elementary_mathematics': 0.6097560975609756, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.8, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_electrical_engineering': 0.875, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.7307692307692307, 'mmlu_eval_accuracy_high_school_european_history': 0.8888888888888888, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_moral_disputes': 0.7368421052631579, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_college_mathematics': 0.6363636363636364, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_sociology': 0.9545454545454546, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_philosophy': 0.8529411764705882, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_law': 0.5411764705882353, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_world_history': 0.8846153846153846, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_moral_scenarios': 0.47, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy': 0.7037830131195528, 'epoch': 7.9}
Saving PEFT checkpoint...
{'train_runtime': 52993.5006, 'train_samples_per_second': 1.159, 'train_steps_per_second': 0.009, 'train_loss': 0.5288325443863868, 'epoch': 7.9}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =         7.9
  train_loss               =      0.5288
  train_runtime            = 14:43:13.50
  train_samples_per_second =       1.159
  train_steps_per_second   =       0.009
***** eval metrics *****
  epoch                   =        7.9
  eval_loss               =     0.8706
  eval_runtime            = 0:05:04.21
  eval_samples_per_second =      3.287
  eval_steps_per_second   =      0.411
Namespace(model_name_or_path='mistralai/Mistral-7B-Instruct-v0.2', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/lab/self_instruct_gpt3.5_instruction.csv', dataset_format='alpaca', output_dir='./output/Mistral-7B-Instruct-v0.2-gpt-0104', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=480, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mistral-7B-Instruct-v0.2-gpt-0104/runs/Jan10_15-39-38_2115ga003', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=120, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=48, dataloader_num_workers=3, past_index=-1, run_name='./output/Mistral-7B-Instruct-v0.2-gpt-0104', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mistral-7B-Instruct-v0.2...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919851520 || trainable: 2.140032079582443
torch.bfloat16 429924352 0.10967873395367791
torch.uint8 3489660928 0.8902533451062963
torch.float32 266240 6.792094002580996e-05
{'loss': 0.9791, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.8553, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.7062, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.815, 'learning_rate': 0.0002, 'epoch': 0.66}
{'eval_loss': 0.7583989500999451, 'eval_runtime': 129.914, 'eval_samples_per_second': 7.697, 'eval_steps_per_second': 0.962, 'epoch': 0.79}
{'mmlu_loss': 2.5151809534678855, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_business_ethics': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_moral_disputes': 0.5263157894736842, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6153846153846154, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_human_sexuality': 0.3333333333333333, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.4294117647058823, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.38, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.6059131823025311, 'epoch': 0.79}
{'loss': 0.7762, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.6656, 'learning_rate': 0.0002, 'epoch': 0.99}
{'loss': 0.7449, 'learning_rate': 0.0002, 'epoch': 1.15}
{'loss': 0.7058, 'learning_rate': 0.0002, 'epoch': 1.32}
{'loss': 0.6306, 'learning_rate': 0.0002, 'epoch': 1.48}
{'eval_loss': 0.723554790019989, 'eval_runtime': 131.3729, 'eval_samples_per_second': 7.612, 'eval_steps_per_second': 0.951, 'epoch': 1.58}
{'mmlu_loss': 2.4953154421721897, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_mathematics': 0.4827586206896552, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_human_sexuality': 0.3333333333333333, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.43529411764705883, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.35, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.6062701877141808, 'epoch': 1.58}
{'loss': 0.7337, 'learning_rate': 0.0002, 'epoch': 1.65}
{'loss': 0.6971, 'learning_rate': 0.0002, 'epoch': 1.81}
{'loss': 0.6171, 'learning_rate': 0.0002, 'epoch': 1.98}
Saving PEFT checkpoint...
{'loss': 0.6839, 'learning_rate': 0.0002, 'epoch': 2.14}
{'loss': 0.629, 'learning_rate': 0.0002, 'epoch': 2.3}
{'eval_loss': 0.7118095755577087, 'eval_runtime': 137.9526, 'eval_samples_per_second': 7.249, 'eval_steps_per_second': 0.906, 'epoch': 2.37}
{'mmlu_loss': 2.593882914632559, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.45294117647058824, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.35, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7209302325581395, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.6140432330356649, 'epoch': 2.37}
{'loss': 0.5827, 'learning_rate': 0.0002, 'epoch': 2.47}
{'loss': 0.6774, 'learning_rate': 0.0002, 'epoch': 2.63}
{'loss': 0.6346, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.5856, 'learning_rate': 0.0002, 'epoch': 2.96}
{'loss': 0.6278, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7169782519340515, 'eval_runtime': 129.897, 'eval_samples_per_second': 7.698, 'eval_steps_per_second': 0.962, 'epoch': 3.16}
{'mmlu_loss': 2.739343598484993, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_moral_disputes': 0.5263157894736842, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.43529411764705883, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.38, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.6117388596464862, 'epoch': 3.16}
{'loss': 0.5539, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.5506, 'learning_rate': 0.0002, 'epoch': 3.46}
{'loss': 0.6182, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.5548, 'learning_rate': 0.0002, 'epoch': 3.79}
{'loss': 0.5584, 'learning_rate': 0.0002, 'epoch': 3.95}
{'eval_loss': 0.7245473861694336, 'eval_runtime': 129.907, 'eval_samples_per_second': 7.698, 'eval_steps_per_second': 0.962, 'epoch': 3.95}
{'mmlu_loss': 2.757056253341337, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_professional_accounting': 0.6451612903225806, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.47058823529411764, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.34, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.6032479761751541, 'epoch': 3.95}
Saving PEFT checkpoint...
{'loss': 0.5711, 'learning_rate': 0.0002, 'epoch': 4.12}
{'loss': 0.4736, 'learning_rate': 0.0002, 'epoch': 4.28}
{'loss': 0.511, 'learning_rate': 0.0002, 'epoch': 4.44}
{'loss': 0.5483, 'learning_rate': 0.0002, 'epoch': 4.61}
{'eval_loss': 0.7704208493232727, 'eval_runtime': 129.8965, 'eval_samples_per_second': 7.698, 'eval_steps_per_second': 0.962, 'epoch': 4.74}
{'mmlu_loss': 2.9147914849842587, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_mathematics': 0.4827586206896552, 'mmlu_eval_accuracy_business_ethics': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.4764705882352941, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.6092757457099547, 'epoch': 4.74}
{'loss': 0.4902, 'learning_rate': 0.0002, 'epoch': 4.77}
{'loss': 0.5289, 'learning_rate': 0.0002, 'epoch': 4.94}
{'loss': 0.4917, 'learning_rate': 0.0002, 'epoch': 5.1}
{'loss': 0.3978, 'learning_rate': 0.0002, 'epoch': 5.27}
{'loss': 0.4721, 'learning_rate': 0.0002, 'epoch': 5.43}
{'eval_loss': 0.8898787498474121, 'eval_runtime': 129.8671, 'eval_samples_per_second': 7.7, 'eval_steps_per_second': 0.963, 'epoch': 5.53}
{'mmlu_loss': 3.0511689369256296, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_business_ethics': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.4764705882352941, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.31, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7142857142857143, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.6099821946251851, 'epoch': 5.53}
{'loss': 0.464, 'learning_rate': 0.0002, 'epoch': 5.6}
{'loss': 0.4128, 'learning_rate': 0.0002, 'epoch': 5.76}
{'loss': 0.4962, 'learning_rate': 0.0002, 'epoch': 5.93}
Saving PEFT checkpoint...
{'loss': 0.4121, 'learning_rate': 0.0002, 'epoch': 6.09}
{'loss': 0.3232, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.8780407309532166, 'eval_runtime': 129.9377, 'eval_samples_per_second': 7.696, 'eval_steps_per_second': 0.962, 'epoch': 6.32}
{'mmlu_loss': 2.9998535557339587, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.4764705882352941, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.29, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7142857142857143, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy': 0.6054804680795696, 'epoch': 6.32}
{'loss': 0.4332, 'learning_rate': 0.0002, 'epoch': 6.42}
{'loss': 0.3885, 'learning_rate': 0.0002, 'epoch': 6.58}
{'loss': 0.3391, 'learning_rate': 0.0002, 'epoch': 6.75}
{'loss': 0.4495, 'learning_rate': 0.0002, 'epoch': 6.91}
{'loss': 0.3415, 'learning_rate': 0.0002, 'epoch': 7.08}
{'eval_loss': 0.9437398910522461, 'eval_runtime': 129.9992, 'eval_samples_per_second': 7.692, 'eval_steps_per_second': 0.962, 'epoch': 7.11}
{'mmlu_loss': 3.439017627388239, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_econometrics': 0.8333333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.4411764705882353, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.28, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.6744186046511628, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7142857142857143, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy': 0.610349901778092, 'epoch': 7.11}
{'loss': 0.274, 'learning_rate': 0.0002, 'epoch': 7.24}
{'loss': 0.3673, 'learning_rate': 0.0002, 'epoch': 7.41}
{'loss': 0.3175, 'learning_rate': 0.0002, 'epoch': 7.57}
{'loss': 0.2919, 'learning_rate': 0.0002, 'epoch': 7.74}
{'loss': 0.3869, 'learning_rate': 0.0002, 'epoch': 7.9}
{'eval_loss': 0.8847121000289917, 'eval_runtime': 130.0043, 'eval_samples_per_second': 7.692, 'eval_steps_per_second': 0.962, 'epoch': 7.9}
{'mmlu_loss': 3.035938817386826, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_econometrics': 0.8333333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_professional_law': 0.4647058823529412, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_moral_scenarios': 0.26, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_professional_psychology': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy': 0.6049684941270446, 'epoch': 7.9}
Saving PEFT checkpoint...
{'train_runtime': 25957.3369, 'train_samples_per_second': 2.367, 'train_steps_per_second': 0.018, 'train_loss': 0.5492965444922447, 'epoch': 7.9}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =        7.9
  train_loss               =     0.5493
  train_runtime            = 7:12:37.33
  train_samples_per_second =      2.367
  train_steps_per_second   =      0.018
***** eval metrics *****
  epoch                   =        7.9
  eval_loss               =     0.8847
  eval_runtime            = 0:02:09.95
  eval_samples_per_second =      7.695
  eval_steps_per_second   =      0.962
Namespace(model_name_or_path='mistralai/Mixtral-8x7B-Instruct-v0.1', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/lab/self_instruct_gpt3.5_instruction.csv', dataset_format='alpaca', output_dir='./output/Mixtral-8x7B-Instruct-v0.1-gpt-0104', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=480, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mixtral-8x7B-Instruct-v0.1-gpt-0104/runs/Jan10_22-59-45_2115ga003', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=120, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=48, dataloader_num_workers=3, past_index=-1, run_name='./output/Mixtral-8x7B-Instruct-v0.1-gpt-0104', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mixtral-8x7B-Instruct-v0.1...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 484450304.0 || all params: 24451510272 || trainable: 1.9812694537513107
torch.bfloat16 1231052800 0.05034669786470031
torch.uint8 23220191232 0.9496424136463255
torch.float32 266240 1.0888488974232306e-05
{'loss': 0.9537, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.8249, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.6788, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.7874, 'learning_rate': 0.0002, 'epoch': 0.66}
{'eval_loss': 0.7308481335639954, 'eval_runtime': 302.9285, 'eval_samples_per_second': 3.301, 'eval_steps_per_second': 0.413, 'epoch': 0.79}
{'mmlu_loss': 1.6490160919105012, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_moral_scenarios': 0.53, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.6153846153846154, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_elementary_mathematics': 0.6097560975609756, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5470588235294118, 'mmlu_eval_accuracy_prehistory': 0.8, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.8064516129032258, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy': 0.7012079534013294, 'epoch': 0.79}
{'loss': 0.7515, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.6392, 'learning_rate': 0.0002, 'epoch': 0.99}
{'loss': 0.7219, 'learning_rate': 0.0002, 'epoch': 1.15}
{'loss': 0.6773, 'learning_rate': 0.0002, 'epoch': 1.32}
{'loss': 0.604, 'learning_rate': 0.0002, 'epoch': 1.48}
{'eval_loss': 0.6998686790466309, 'eval_runtime': 302.7274, 'eval_samples_per_second': 3.303, 'eval_steps_per_second': 0.413, 'epoch': 1.58}
{'mmlu_loss': 1.6756523214280605, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_moral_scenarios': 0.51, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_elementary_mathematics': 0.6097560975609756, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.8023255813953488, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5588235294117647, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.8387096774193549, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy': 0.7028061904917177, 'epoch': 1.58}
{'loss': 0.7084, 'learning_rate': 0.0002, 'epoch': 1.65}
{'loss': 0.6712, 'learning_rate': 0.0002, 'epoch': 1.81}
{'loss': 0.5941, 'learning_rate': 0.0002, 'epoch': 1.98}
Saving PEFT checkpoint...
{'loss': 0.6575, 'learning_rate': 0.0002, 'epoch': 2.14}
{'loss': 0.5989, 'learning_rate': 0.0002, 'epoch': 2.3}
{'eval_loss': 0.6986465454101562, 'eval_runtime': 306.3532, 'eval_samples_per_second': 3.264, 'eval_steps_per_second': 0.408, 'epoch': 2.37}
{'mmlu_loss': 1.7217479664832354, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_moral_scenarios': 0.53, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_elementary_mathematics': 0.5853658536585366, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.8023255813953488, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5647058823529412, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.8064516129032258, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy': 0.7008335261833835, 'epoch': 2.37}
{'loss': 0.5546, 'learning_rate': 0.0002, 'epoch': 2.47}
{'loss': 0.6511, 'learning_rate': 0.0002, 'epoch': 2.63}
{'loss': 0.607, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.5621, 'learning_rate': 0.0002, 'epoch': 2.96}
{'loss': 0.5972, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7187943458557129, 'eval_runtime': 305.2868, 'eval_samples_per_second': 3.276, 'eval_steps_per_second': 0.409, 'epoch': 3.16}
{'mmlu_loss': 1.7287585179631908, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_moral_scenarios': 0.51, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_elementary_mathematics': 0.6097560975609756, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5588235294117647, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.8064516129032258, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6956521739130435, 'mmlu_eval_accuracy': 0.7070590959438272, 'epoch': 3.16}
{'loss': 0.5171, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.5205, 'learning_rate': 0.0002, 'epoch': 3.46}
{'loss': 0.5891, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.5277, 'learning_rate': 0.0002, 'epoch': 3.79}
{'loss': 0.536, 'learning_rate': 0.0002, 'epoch': 3.95}
{'eval_loss': 0.7152032852172852, 'eval_runtime': 306.027, 'eval_samples_per_second': 3.268, 'eval_steps_per_second': 0.408, 'epoch': 3.95}
{'mmlu_loss': 1.7442391052221258, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_moral_scenarios': 0.52, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_elementary_mathematics': 0.6097560975609756, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5588235294117647, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.8387096774193549, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy': 0.7050397576533429, 'epoch': 3.95}
Saving PEFT checkpoint...
{'loss': 0.5379, 'learning_rate': 0.0002, 'epoch': 4.12}
{'loss': 0.4418, 'learning_rate': 0.0002, 'epoch': 4.28}
{'loss': 0.4893, 'learning_rate': 0.0002, 'epoch': 4.44}
{'loss': 0.5243, 'learning_rate': 0.0002, 'epoch': 4.61}
{'eval_loss': 0.7787638902664185, 'eval_runtime': 302.9893, 'eval_samples_per_second': 3.3, 'eval_steps_per_second': 0.413, 'epoch': 4.74}
{'mmlu_loss': 1.8265183633193374, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_moral_scenarios': 0.49, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8846153846153846, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_nutrition': 0.7878787878787878, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_elementary_mathematics': 0.5609756097560976, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.8023255813953488, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5235294117647059, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_professional_medicine': 0.8064516129032258, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy': 0.7036873656304842, 'epoch': 4.74}
{'loss': 0.4652, 'learning_rate': 0.0002, 'epoch': 4.77}
{'loss': 0.5063, 'learning_rate': 0.0002, 'epoch': 4.94}
{'loss': 0.4551, 'learning_rate': 0.0002, 'epoch': 5.1}
{'loss': 0.3653, 'learning_rate': 0.0002, 'epoch': 5.27}
{'loss': 0.4455, 'learning_rate': 0.0002, 'epoch': 5.43}
{'eval_loss': 0.9259434342384338, 'eval_runtime': 303.0382, 'eval_samples_per_second': 3.3, 'eval_steps_per_second': 0.412, 'epoch': 5.53}
{'mmlu_loss': 1.9011872708797455, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_world_history': 0.8846153846153846, 'mmlu_eval_accuracy_moral_scenarios': 0.55, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_anatomy': 0.7142857142857143, 'mmlu_eval_accuracy_high_school_chemistry': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.6153846153846154, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8846153846153846, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_nutrition': 0.7878787878787878, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_elementary_mathematics': 0.5609756097560976, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.8372093023255814, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5588235294117647, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_college_mathematics': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy_professional_medicine': 0.8064516129032258, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6956521739130435, 'mmlu_eval_accuracy': 0.704737755539423, 'epoch': 5.53}
{'loss': 0.4406, 'learning_rate': 0.0002, 'epoch': 5.6}
{'loss': 0.3888, 'learning_rate': 0.0002, 'epoch': 5.76}
{'loss': 0.4713, 'learning_rate': 0.0002, 'epoch': 5.93}
Saving PEFT checkpoint...
{'loss': 0.3736, 'learning_rate': 0.0002, 'epoch': 6.09}
{'loss': 0.2954, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.9818103313446045, 'eval_runtime': 303.1088, 'eval_samples_per_second': 3.299, 'eval_steps_per_second': 0.412, 'epoch': 6.32}
{'mmlu_loss': 1.9630205699553092, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_world_history': 0.8846153846153846, 'mmlu_eval_accuracy_moral_scenarios': 0.63, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_high_school_chemistry': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9615384615384616, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_elementary_mathematics': 0.5853658536585366, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.8372093023255814, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5470588235294118, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_college_mathematics': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy_professional_medicine': 0.7741935483870968, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy': 0.7098105006646189, 'epoch': 6.32}
{'loss': 0.3949, 'learning_rate': 0.0002, 'epoch': 6.42}
{'loss': 0.3578, 'learning_rate': 0.0002, 'epoch': 6.58}
{'loss': 0.3189, 'learning_rate': 0.0002, 'epoch': 6.75}
{'loss': 0.4097, 'learning_rate': 0.0002, 'epoch': 6.91}
{'loss': 0.3043, 'learning_rate': 0.0002, 'epoch': 7.08}
{'eval_loss': 1.1156346797943115, 'eval_runtime': 302.8096, 'eval_samples_per_second': 3.302, 'eval_steps_per_second': 0.413, 'epoch': 7.11}
{'mmlu_loss': 2.0486315032467246, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_world_history': 0.8846153846153846, 'mmlu_eval_accuracy_moral_scenarios': 0.56, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_chemistry': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_elementary_mathematics': 0.5609756097560976, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5470588235294118, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_professional_medicine': 0.8387096774193549, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy': 0.7030957347284784, 'epoch': 7.11}
{'loss': 0.2389, 'learning_rate': 0.0002, 'epoch': 7.24}
{'loss': 0.3207, 'learning_rate': 0.0002, 'epoch': 7.41}
{'loss': 0.2812, 'learning_rate': 0.0002, 'epoch': 7.57}
{'loss': 0.2623, 'learning_rate': 0.0002, 'epoch': 7.74}
{'loss': 0.3395, 'learning_rate': 0.0002, 'epoch': 7.9}
{'eval_loss': 0.9886812567710876, 'eval_runtime': 303.0813, 'eval_samples_per_second': 3.299, 'eval_steps_per_second': 0.412, 'epoch': 7.9}
{'mmlu_loss': 1.9765584509198864, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_moral_scenarios': 0.55, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_high_school_chemistry': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_government_and_politics': 1.0, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8846153846153846, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_machine_learning': 0.6363636363636364, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_elementary_mathematics': 0.5365853658536586, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_professional_law': 0.5352941176470588, 'mmlu_eval_accuracy_prehistory': 0.8571428571428571, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_college_mathematics': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy_professional_medicine': 0.8387096774193549, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_computer_security': 0.9090909090909091, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy': 0.7129242088234561, 'epoch': 7.9}
Saving PEFT checkpoint...
{'train_runtime': 53040.5285, 'train_samples_per_second': 1.158, 'train_steps_per_second': 0.009, 'train_loss': 0.519989064335823, 'epoch': 7.9}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =         7.9
  train_loss               =        0.52
  train_runtime            = 14:44:00.52
  train_samples_per_second =       1.158
  train_steps_per_second   =       0.009
***** eval metrics *****
  epoch                   =        7.9
  eval_loss               =     0.9887
  eval_runtime            = 0:05:03.17
  eval_samples_per_second =      3.298
  eval_steps_per_second   =      0.412
Namespace(model_name_or_path='mistralai/Mistral-7B-v0.1', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/lab/self_instruct_gpt3.5_instruction.csv', dataset_format='alpaca', output_dir='./output/Mistral-7B-v0.1-gpt-0104', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=480, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mistral-7B-v0.1-gpt-0104/runs/Jan11_14-01-20_2115ga003', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=120, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=48, dataloader_num_workers=3, past_index=-1, run_name='./output/Mistral-7B-v0.1-gpt-0104', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mistral-7B-v0.1...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919851520 || trainable: 2.140032079582443
torch.bfloat16 429924352 0.10967873395367791
torch.uint8 3489660928 0.8902533451062963
torch.float32 266240 6.792094002580996e-05
{'loss': 0.9256, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.8475, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.7037, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.8152, 'learning_rate': 0.0002, 'epoch': 0.66}
{'eval_loss': 0.7682070136070251, 'eval_runtime': 130.0176, 'eval_samples_per_second': 7.691, 'eval_steps_per_second': 0.961, 'epoch': 0.79}
{'mmlu_loss': 2.1148580697675547, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.4235294117647059, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy': 0.595741990187957, 'epoch': 0.79}
{'loss': 0.7815, 'learning_rate': 0.0002, 'epoch': 0.82}
{'loss': 0.6711, 'learning_rate': 0.0002, 'epoch': 0.99}
{'loss': 0.7487, 'learning_rate': 0.0002, 'epoch': 1.15}
{'loss': 0.7127, 'learning_rate': 0.0002, 'epoch': 1.32}
{'loss': 0.6365, 'learning_rate': 0.0002, 'epoch': 1.48}
{'eval_loss': 0.7305068969726562, 'eval_runtime': 129.9697, 'eval_samples_per_second': 7.694, 'eval_steps_per_second': 0.962, 'epoch': 1.58}
{'mmlu_loss': 2.177089008192221, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.34, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy': 0.6023259372599596, 'epoch': 1.58}
{'loss': 0.7395, 'learning_rate': 0.0002, 'epoch': 1.65}
{'loss': 0.705, 'learning_rate': 0.0002, 'epoch': 1.81}
{'loss': 0.6259, 'learning_rate': 0.0002, 'epoch': 1.98}
Saving PEFT checkpoint...
{'loss': 0.6939, 'learning_rate': 0.0002, 'epoch': 2.14}
{'loss': 0.6448, 'learning_rate': 0.0002, 'epoch': 2.3}
{'eval_loss': 0.7164767980575562, 'eval_runtime': 129.9134, 'eval_samples_per_second': 7.697, 'eval_steps_per_second': 0.962, 'epoch': 2.37}
{'mmlu_loss': 2.1382699720561504, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.45294117647058824, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy': 0.5970474962892799, 'epoch': 2.37}
{'loss': 0.5962, 'learning_rate': 0.0002, 'epoch': 2.47}
{'loss': 0.6879, 'learning_rate': 0.0002, 'epoch': 2.63}
{'loss': 0.6464, 'learning_rate': 0.0002, 'epoch': 2.8}
{'loss': 0.5954, 'learning_rate': 0.0002, 'epoch': 2.96}
{'loss': 0.6405, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7138532996177673, 'eval_runtime': 129.92, 'eval_samples_per_second': 7.697, 'eval_steps_per_second': 0.962, 'epoch': 3.16}
{'mmlu_loss': 2.2193179856985807, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5348837209302325, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.4470588235294118, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy': 0.5940129153153709, 'epoch': 3.16}
{'loss': 0.5747, 'learning_rate': 0.0002, 'epoch': 3.29}
{'loss': 0.5688, 'learning_rate': 0.0002, 'epoch': 3.46}
{'loss': 0.6349, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.5726, 'learning_rate': 0.0002, 'epoch': 3.79}
{'loss': 0.5716, 'learning_rate': 0.0002, 'epoch': 3.95}
{'eval_loss': 0.7104366421699524, 'eval_runtime': 141.1516, 'eval_samples_per_second': 7.085, 'eval_steps_per_second': 0.886, 'epoch': 3.95}
{'mmlu_loss': 2.201529851804177, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.4588235294117647, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.35, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy': 0.5929736375420627, 'epoch': 3.95}
Saving PEFT checkpoint...
{'loss': 0.5946, 'learning_rate': 0.0002, 'epoch': 4.12}
{'loss': 0.5017, 'learning_rate': 0.0002, 'epoch': 4.28}
{'loss': 0.5344, 'learning_rate': 0.0002, 'epoch': 4.44}
{'loss': 0.5767, 'learning_rate': 0.0002, 'epoch': 4.61}
{'eval_loss': 0.7627300024032593, 'eval_runtime': 131.6925, 'eval_samples_per_second': 7.593, 'eval_steps_per_second': 0.949, 'epoch': 4.74}
{'mmlu_loss': 2.2252803140630326, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.4470588235294118, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.34, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_psychology': 0.8, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy': 0.5980650364083544, 'epoch': 4.74}
{'loss': 0.5166, 'learning_rate': 0.0002, 'epoch': 4.77}
{'loss': 0.5492, 'learning_rate': 0.0002, 'epoch': 4.94}
{'loss': 0.5281, 'learning_rate': 0.0002, 'epoch': 5.1}
{'loss': 0.4293, 'learning_rate': 0.0002, 'epoch': 5.27}
{'loss': 0.5029, 'learning_rate': 0.0002, 'epoch': 5.43}
{'eval_loss': 0.8063044548034668, 'eval_runtime': 133.311, 'eval_samples_per_second': 7.501, 'eval_steps_per_second': 0.938, 'epoch': 5.53}
{'mmlu_loss': 2.302581754202644, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.4470588235294118, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_psychology': 0.8166666666666667, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_high_school_geography': 0.7727272727272727, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy': 0.5876663837031768, 'epoch': 5.53}
{'loss': 0.498, 'learning_rate': 0.0002, 'epoch': 5.6}
{'loss': 0.4431, 'learning_rate': 0.0002, 'epoch': 5.76}
{'loss': 0.5203, 'learning_rate': 0.0002, 'epoch': 5.93}
Saving PEFT checkpoint...
{'loss': 0.4538, 'learning_rate': 0.0002, 'epoch': 6.09}
{'loss': 0.351, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.8064922094345093, 'eval_runtime': 130.0081, 'eval_samples_per_second': 7.692, 'eval_steps_per_second': 0.961, 'epoch': 6.32}
{'mmlu_loss': 2.342777314285437, 'mmlu_eval_accuracy_moral_disputes': 0.7105263157894737, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.4411764705882353, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.32, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_psychology': 0.8166666666666667, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_high_school_geography': 0.7272727272727273, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy': 0.5910811872145654, 'epoch': 6.32}
{'loss': 0.4679, 'learning_rate': 0.0002, 'epoch': 6.42}
{'loss': 0.4307, 'learning_rate': 0.0002, 'epoch': 6.58}
{'loss': 0.3721, 'learning_rate': 0.0002, 'epoch': 6.75}
{'loss': 0.4841, 'learning_rate': 0.0002, 'epoch': 6.91}
{'loss': 0.3882, 'learning_rate': 0.0002, 'epoch': 7.08}
{'eval_loss': 0.8375257849693298, 'eval_runtime': 129.9984, 'eval_samples_per_second': 7.692, 'eval_steps_per_second': 0.962, 'epoch': 7.11}
{'mmlu_loss': 2.38185339483122, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.43529411764705883, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.34, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_psychology': 0.8, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_miscellaneous': 0.7674418604651163, 'mmlu_eval_accuracy_high_school_geography': 0.7272727272727273, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy': 0.5961204632038102, 'epoch': 7.11}
{'loss': 0.3027, 'learning_rate': 0.0002, 'epoch': 7.24}
{'loss': 0.4109, 'learning_rate': 0.0002, 'epoch': 7.41}
{'loss': 0.3546, 'learning_rate': 0.0002, 'epoch': 7.57}
{'loss': 0.3167, 'learning_rate': 0.0002, 'epoch': 7.74}
{'loss': 0.427, 'learning_rate': 0.0002, 'epoch': 7.9}
{'eval_loss': 0.8697112798690796, 'eval_runtime': 129.862, 'eval_samples_per_second': 7.7, 'eval_steps_per_second': 0.963, 'epoch': 7.9}
{'mmlu_loss': 2.4119111020118, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_philosophy': 0.6176470588235294, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7692307692307693, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_professional_law': 0.4294117647058823, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.32, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_psychology': 0.8166666666666667, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_professional_psychology': 0.5942028985507246, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_miscellaneous': 0.7674418604651163, 'mmlu_eval_accuracy_high_school_geography': 0.7272727272727273, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy': 0.5918510406385902, 'epoch': 7.9}
Saving PEFT checkpoint...
{'train_runtime': 25953.0244, 'train_samples_per_second': 2.367, 'train_steps_per_second': 0.018, 'train_loss': 0.5686536843578021, 'epoch': 7.9}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =        7.9
  train_loss               =     0.5687
  train_runtime            = 7:12:33.02
  train_samples_per_second =      2.367
  train_steps_per_second   =      0.018
***** eval metrics *****
  epoch                   =        7.9
  eval_loss               =     0.8697
  eval_runtime            = 0:02:12.11
  eval_samples_per_second =      7.569
  eval_steps_per_second   =      0.946
