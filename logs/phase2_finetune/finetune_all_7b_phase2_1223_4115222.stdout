Alpaca prompt format. modified 'unk_token' = 0 in qlora.py (originally 2 because pad_token_id = 2 for zephyr). model-phase2-date Finetune base models on gpt generated data and gpt paraphrased data.
Namespace(model_name_or_path='mistralai/Mixtral-8x7B-v0.1', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/Mixtral-8x7B-v0.1-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mixtral-8x7B-v0.1-phase2-1223/runs/Dec23_09-38-40_2117ga001', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/Mixtral-8x7B-v0.1-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mixtral-8x7B-v0.1...
Namespace(model_name_or_path='mistralai/Mistral-7B-Instruct-v0.2', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/Mistral-7B-Instruct-v0.2-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mistral-7B-Instruct-v0.2-phase2-1223/runs/Dec23_09-38-53_2117ga001', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/Mistral-7B-Instruct-v0.2-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mistral-7B-Instruct-v0.2...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919851520 || trainable: 2.140032079582443
torch.bfloat16 429924352 0.10967873395367791
torch.uint8 3489660928 0.8902533451062963
torch.float32 266240 6.792094002580996e-05
Namespace(model_name_or_path='mistralai/Mixtral-8x7B-Instruct-v0.1', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/Mixtral-8x7B-Instruct-v0.1-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mixtral-8x7B-Instruct-v0.1-phase2-1223/runs/Dec23_09-44-38_2117ga001', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/Mixtral-8x7B-Instruct-v0.1-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mixtral-8x7B-Instruct-v0.1...
Namespace(model_name_or_path='mistralai/Mistral-7B-v0.1', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/Mistral-7B-v0.1-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mistral-7B-v0.1-phase2-1223/runs/Dec23_09-44-49_2117ga001', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/Mistral-7B-v0.1-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mistral-7B-v0.1...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919851520 || trainable: 2.140032079582443
torch.bfloat16 429924352 0.10967873395367791
torch.uint8 3489660928 0.8902533451062963
torch.float32 266240 6.792094002580996e-05
{'loss': 0.9427, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.8087, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.9091, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.7377, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.7742, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.8777, 'learning_rate': 0.0002, 'epoch': 0.59}
{'eval_loss': 0.7911564707756042, 'eval_runtime': 128.9347, 'eval_samples_per_second': 7.756, 'eval_steps_per_second': 0.969, 'epoch': 0.63}
{'mmlu_loss': 1.9404549251000087, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_moral_scenarios': 0.42, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.4470588235294118, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy': 0.6078858140532424, 'epoch': 0.63}
{'loss': 0.7549, 'learning_rate': 0.0002, 'epoch': 0.68}
{'loss': 0.8433, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.7063, 'learning_rate': 0.0002, 'epoch': 0.88}
{'loss': 0.7304, 'learning_rate': 0.0002, 'epoch': 0.98}
{'loss': 0.8231, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.6609, 'learning_rate': 0.0002, 'epoch': 1.17}
{'eval_loss': 0.8023615479469299, 'eval_runtime': 128.9274, 'eval_samples_per_second': 7.756, 'eval_steps_per_second': 0.97, 'epoch': 1.25}
{'mmlu_loss': 2.120287360623479, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_moral_scenarios': 0.38, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.4294117647058823, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy': 0.593926284361089, 'epoch': 1.25}
{'loss': 0.7228, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.7081, 'learning_rate': 0.0002, 'epoch': 1.37}
{'loss': 0.6511, 'learning_rate': 0.0002, 'epoch': 1.47}
Saving PEFT checkpoint...
{'loss': 0.7668, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 0.6616, 'learning_rate': 0.0002, 'epoch': 1.66}
{'loss': 0.7197, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.693, 'learning_rate': 0.0002, 'epoch': 1.86}
{'eval_loss': 0.7313445806503296, 'eval_runtime': 128.8993, 'eval_samples_per_second': 7.758, 'eval_steps_per_second': 0.97, 'epoch': 1.88}
{'mmlu_loss': 2.3244932095209756, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_moral_scenarios': 0.36, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.4235294117647059, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy': 0.5966417632627047, 'epoch': 1.88}
{'loss': 0.6467, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 0.7489, 'learning_rate': 0.0002, 'epoch': 2.05}
{'loss': 0.6138, 'learning_rate': 0.0002, 'epoch': 2.15}
{'loss': 0.5439, 'learning_rate': 0.0002, 'epoch': 2.25}
{'loss': 0.7342, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.5932, 'learning_rate': 0.0002, 'epoch': 2.44}
{'eval_loss': 0.7558112740516663, 'eval_runtime': 128.8709, 'eval_samples_per_second': 7.76, 'eval_steps_per_second': 0.97, 'epoch': 2.5}
{'mmlu_loss': 2.2231717612594366, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_moral_scenarios': 0.35, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.4235294117647059, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_philosophy': 0.6176470588235294, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.7241379310344828, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy': 0.5927984825425886, 'epoch': 2.5}
{'loss': 0.6669, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.6204, 'learning_rate': 0.0002, 'epoch': 2.64}
{'loss': 0.5501, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 0.7444, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.5973, 'learning_rate': 0.0002, 'epoch': 2.93}
Saving PEFT checkpoint...
{'loss': 0.6357, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.6113, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7254273295402527, 'eval_runtime': 128.8885, 'eval_samples_per_second': 7.759, 'eval_steps_per_second': 0.97, 'epoch': 3.13}
{'mmlu_loss': 2.22523532807827, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_moral_scenarios': 0.36, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy': 0.5967683379132819, 'epoch': 3.13}
{'loss': 0.4424, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.6633, 'learning_rate': 0.0002, 'epoch': 3.33}
{'loss': 0.5589, 'learning_rate': 0.0002, 'epoch': 3.42}
{'loss': 0.5247, 'learning_rate': 0.0002, 'epoch': 3.52}
{'loss': 0.6326, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.4766, 'learning_rate': 0.0002, 'epoch': 3.72}
{'eval_loss': 0.774691641330719, 'eval_runtime': 128.8217, 'eval_samples_per_second': 7.763, 'eval_steps_per_second': 0.97, 'epoch': 3.76}
{'mmlu_loss': 2.3114456490923962, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_moral_scenarios': 0.33, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_anatomy': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_professional_medicine': 0.6129032258064516, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.43529411764705883, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy': 0.5923598952512513, 'epoch': 3.76}
{'loss': 0.6756, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.567, 'learning_rate': 0.0002, 'epoch': 3.91}
{'loss': 0.4936, 'learning_rate': 0.0002, 'epoch': 4.01}
{'loss': 0.649, 'learning_rate': 0.0002, 'epoch': 4.11}
{'loss': 0.4207, 'learning_rate': 0.0002, 'epoch': 4.21}
{'loss': 0.5505, 'learning_rate': 0.0002, 'epoch': 4.3}
{'eval_loss': 0.7566028833389282, 'eval_runtime': 128.834, 'eval_samples_per_second': 7.762, 'eval_steps_per_second': 0.97, 'epoch': 4.38}
{'mmlu_loss': 2.566730205590526, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_moral_scenarios': 0.36, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_professional_psychology': 0.5942028985507246, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_psychology': 0.8166666666666667, 'mmlu_eval_accuracy': 0.5820480584720481, 'epoch': 4.38}
{'loss': 0.5423, 'learning_rate': 0.0002, 'epoch': 4.4}
Saving PEFT checkpoint...
{'loss': 0.3746, 'learning_rate': 0.0002, 'epoch': 4.5}
{'loss': 0.6803, 'learning_rate': 0.0002, 'epoch': 4.6}
{'loss': 0.4602, 'learning_rate': 0.0002, 'epoch': 4.69}
{'loss': 0.5585, 'learning_rate': 0.0002, 'epoch': 4.79}
{'loss': 0.5603, 'learning_rate': 0.0002, 'epoch': 4.89}
{'loss': 0.3704, 'learning_rate': 0.0002, 'epoch': 4.99}
{'eval_loss': 0.7841284275054932, 'eval_runtime': 128.9536, 'eval_samples_per_second': 7.755, 'eval_steps_per_second': 0.969, 'epoch': 5.01}
{'mmlu_loss': 2.580739182730516, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_moral_scenarios': 0.36, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.36363636363636365, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_abstract_algebra': 0.45454545454545453, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy': 0.5922624808533011, 'epoch': 5.01}
{'loss': 0.647, 'learning_rate': 0.0002, 'epoch': 5.09}
{'loss': 0.4414, 'learning_rate': 0.0002, 'epoch': 5.18}
{'loss': 0.4222, 'learning_rate': 0.0002, 'epoch': 5.28}
{'loss': 0.5619, 'learning_rate': 0.0002, 'epoch': 5.38}
{'loss': 0.2843, 'learning_rate': 0.0002, 'epoch': 5.48}
{'loss': 0.631, 'learning_rate': 0.0002, 'epoch': 5.57}
{'eval_loss': 0.7763831615447998, 'eval_runtime': 128.8881, 'eval_samples_per_second': 7.759, 'eval_steps_per_second': 0.97, 'epoch': 5.63}
{'mmlu_loss': 2.389965008944273, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_moral_scenarios': 0.37, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.7222222222222222, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy': 0.5983898472085859, 'epoch': 5.63}
{'loss': 0.4627, 'learning_rate': 0.0002, 'epoch': 5.67}
{'loss': 0.4136, 'learning_rate': 0.0002, 'epoch': 5.77}
{'loss': 0.567, 'learning_rate': 0.0002, 'epoch': 5.87}
Saving PEFT checkpoint...
{'loss': 0.2955, 'learning_rate': 0.0002, 'epoch': 5.97}
{'loss': 0.568, 'learning_rate': 0.0002, 'epoch': 6.06}
{'loss': 0.4374, 'learning_rate': 0.0002, 'epoch': 6.16}
{'loss': 0.2475, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.913083016872406, 'eval_runtime': 128.9781, 'eval_samples_per_second': 7.753, 'eval_steps_per_second': 0.969, 'epoch': 6.26}
{'mmlu_loss': 2.4926180969923735, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_moral_scenarios': 0.35, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_abstract_algebra': 0.5454545454545454, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy': 0.6029208700452842, 'epoch': 6.26}
{'loss': 0.5862, 'learning_rate': 0.0002, 'epoch': 6.36}
{'train_runtime': 28723.5902, 'train_samples_per_second': 2.897, 'train_steps_per_second': 0.023, 'train_loss': 0.6087144638941838, 'epoch': 6.36}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       6.36
  train_loss               =     0.6087
  train_runtime            = 7:58:43.59
  train_samples_per_second =      2.897
  train_steps_per_second   =      0.023
***** eval metrics *****
  epoch                   =       6.36
  eval_loss               =     0.7582
  eval_runtime            = 0:02:09.05
  eval_samples_per_second =      7.748
  eval_steps_per_second   =      0.969
Namespace(model_name_or_path='ehartford/samantha-1.2-mistral-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/samantha-v1.2-mistral-7b-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/samantha-v1.2-mistral-7b-phase2-1223/runs/Dec23_17-55-38_2117ga001', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/samantha-v1.2-mistral-7b-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model ehartford/samantha-1.2-mistral-7b...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919867904 || trainable: 2.1400231348204124
torch.bfloat16 429940736 0.10968245525857394
torch.uint8 3489660928 0.8902496240852916
torch.float32 266240 6.792065613443692e-05
{'loss': 0.993, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.8313, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.931, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.7468, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.7953, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.884, 'learning_rate': 0.0002, 'epoch': 0.59}
{'eval_loss': 0.7926766872406006, 'eval_runtime': 129.0401, 'eval_samples_per_second': 7.75, 'eval_steps_per_second': 0.969, 'epoch': 0.63}
{'mmlu_loss': 15.886333271861076, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.4588235294117647, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_abstract_algebra': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy': 0.606246989567068, 'epoch': 0.63}
{'loss': 0.7623, 'learning_rate': 0.0002, 'epoch': 0.68}
{'loss': 0.8532, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.7121, 'learning_rate': 0.0002, 'epoch': 0.88}
{'loss': 0.7427, 'learning_rate': 0.0002, 'epoch': 0.98}
{'loss': 0.832, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.6648, 'learning_rate': 0.0002, 'epoch': 1.17}
{'eval_loss': 0.8110252618789673, 'eval_runtime': 129.0209, 'eval_samples_per_second': 7.751, 'eval_steps_per_second': 0.969, 'epoch': 1.25}
{'mmlu_loss': 15.80661645034949, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.4588235294117647, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.7241379310344828, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_moral_scenarios': 0.38, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy': 0.6036106936985504, 'epoch': 1.25}
{'loss': 0.7315, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.7098, 'learning_rate': 0.0002, 'epoch': 1.37}
{'loss': 0.6603, 'learning_rate': 0.0002, 'epoch': 1.47}
Saving PEFT checkpoint...
{'loss': 0.771, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 0.6652, 'learning_rate': 0.0002, 'epoch': 1.66}
{'loss': 0.728, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.6953, 'learning_rate': 0.0002, 'epoch': 1.86}
{'eval_loss': 0.740060031414032, 'eval_runtime': 129.0485, 'eval_samples_per_second': 7.749, 'eval_steps_per_second': 0.969, 'epoch': 1.88}
{'mmlu_loss': 16.984498182932537, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.47058823529411764, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy': 0.6010005020617907, 'epoch': 1.88}
{'loss': 0.6534, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 0.7527, 'learning_rate': 0.0002, 'epoch': 2.05}
{'loss': 0.6161, 'learning_rate': 0.0002, 'epoch': 2.15}
{'loss': 0.5529, 'learning_rate': 0.0002, 'epoch': 2.25}
{'loss': 0.7326, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.5952, 'learning_rate': 0.0002, 'epoch': 2.44}
{'eval_loss': 0.7680550813674927, 'eval_runtime': 129.1328, 'eval_samples_per_second': 7.744, 'eval_steps_per_second': 0.968, 'epoch': 2.5}
{'mmlu_loss': 15.75528977314631, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.45294117647058824, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.7241379310344828, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_moral_scenarios': 0.42, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy': 0.5970034090381778, 'epoch': 2.5}
{'loss': 0.6666, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.622, 'learning_rate': 0.0002, 'epoch': 2.64}
{'loss': 0.5548, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 0.7453, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.6021, 'learning_rate': 0.0002, 'epoch': 2.93}
Saving PEFT checkpoint...
{'loss': 0.6395, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.6126, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7315234541893005, 'eval_runtime': 129.0592, 'eval_samples_per_second': 7.748, 'eval_steps_per_second': 0.969, 'epoch': 3.13}
{'mmlu_loss': 15.57295036315918, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.4588235294117647, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5116279069767442, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_clinical_knowledge': 0.7241379310344828, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy': 0.6008311789804942, 'epoch': 3.13}
{'loss': 0.4492, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.667, 'learning_rate': 0.0002, 'epoch': 3.33}
{'loss': 0.5597, 'learning_rate': 0.0002, 'epoch': 3.42}
{'loss': 0.5294, 'learning_rate': 0.0002, 'epoch': 3.52}
{'loss': 0.6346, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.4848, 'learning_rate': 0.0002, 'epoch': 3.72}
{'eval_loss': 0.7679077386856079, 'eval_runtime': 129.0251, 'eval_samples_per_second': 7.75, 'eval_steps_per_second': 0.969, 'epoch': 3.76}
{'mmlu_loss': 16.276955137650173, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.43529411764705883, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5348837209302325, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy': 0.6008046756616937, 'epoch': 3.76}
{'loss': 0.6812, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.5687, 'learning_rate': 0.0002, 'epoch': 3.91}
{'loss': 0.5018, 'learning_rate': 0.0002, 'epoch': 4.01}
{'loss': 0.6496, 'learning_rate': 0.0002, 'epoch': 4.11}
{'loss': 0.4239, 'learning_rate': 0.0002, 'epoch': 4.21}
{'loss': 0.5611, 'learning_rate': 0.0002, 'epoch': 4.3}
{'eval_loss': 0.7571831345558167, 'eval_runtime': 129.0932, 'eval_samples_per_second': 7.746, 'eval_steps_per_second': 0.968, 'epoch': 4.38}
{'mmlu_loss': 16.127933184305828, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.4411764705882353, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5348837209302325, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_clinical_knowledge': 0.7241379310344828, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy': 0.596771907150353, 'epoch': 4.38}
{'loss': 0.5469, 'learning_rate': 0.0002, 'epoch': 4.4}
Saving PEFT checkpoint...
{'loss': 0.3811, 'learning_rate': 0.0002, 'epoch': 4.5}
{'loss': 0.6732, 'learning_rate': 0.0002, 'epoch': 4.6}
{'loss': 0.4649, 'learning_rate': 0.0002, 'epoch': 4.69}
{'loss': 0.5633, 'learning_rate': 0.0002, 'epoch': 4.79}
{'loss': 0.5614, 'learning_rate': 0.0002, 'epoch': 4.89}
{'loss': 0.3679, 'learning_rate': 0.0002, 'epoch': 4.99}
{'eval_loss': 0.7799355983734131, 'eval_runtime': 129.0565, 'eval_samples_per_second': 7.749, 'eval_steps_per_second': 0.969, 'epoch': 5.01}
{'mmlu_loss': 15.974108710885048, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.4588235294117647, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_clinical_knowledge': 0.7241379310344828, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_moral_scenarios': 0.45, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy': 0.6008306616101278, 'epoch': 5.01}
{'loss': 0.6472, 'learning_rate': 0.0002, 'epoch': 5.09}
{'loss': 0.4417, 'learning_rate': 0.0002, 'epoch': 5.18}
{'loss': 0.4163, 'learning_rate': 0.0002, 'epoch': 5.28}
{'loss': 0.5588, 'learning_rate': 0.0002, 'epoch': 5.38}
{'loss': 0.2944, 'learning_rate': 0.0002, 'epoch': 5.48}
{'loss': 0.6254, 'learning_rate': 0.0002, 'epoch': 5.57}
{'eval_loss': 0.7832518815994263, 'eval_runtime': 128.9541, 'eval_samples_per_second': 7.755, 'eval_steps_per_second': 0.969, 'epoch': 5.63}
{'mmlu_loss': 16.399085879325867, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.5428571428571428, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_professional_law': 0.4294117647058823, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_clinical_knowledge': 0.7241379310344828, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy': 0.5915375475708649, 'epoch': 5.63}
{'loss': 0.4626, 'learning_rate': 0.0002, 'epoch': 5.67}
{'loss': 0.4147, 'learning_rate': 0.0002, 'epoch': 5.77}
{'loss': 0.5706, 'learning_rate': 0.0002, 'epoch': 5.87}
Saving PEFT checkpoint...
{'loss': 0.314, 'learning_rate': 0.0002, 'epoch': 5.97}
{'loss': 0.5683, 'learning_rate': 0.0002, 'epoch': 6.06}
{'loss': 0.436, 'learning_rate': 0.0002, 'epoch': 6.16}
{'loss': 0.2439, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.9111002087593079, 'eval_runtime': 129.0769, 'eval_samples_per_second': 7.747, 'eval_steps_per_second': 0.968, 'epoch': 6.26}
{'mmlu_loss': 15.538026317954063, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_professional_law': 0.4470588235294118, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5581395348837209, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.44, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_professional_medicine': 0.7096774193548387, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy': 0.6025673882357218, 'epoch': 6.26}
{'loss': 0.5842, 'learning_rate': 0.0002, 'epoch': 6.36}
{'train_runtime': 28736.7903, 'train_samples_per_second': 2.895, 'train_steps_per_second': 0.023, 'train_loss': 0.6138961186775794, 'epoch': 6.36}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       6.36
  train_loss               =     0.6139
  train_runtime            = 7:58:56.79
  train_samples_per_second =      2.895
  train_steps_per_second   =      0.023
***** eval metrics *****
  epoch                   =       6.36
  eval_loss               =     0.7654
  eval_runtime            = 0:02:09.01
  eval_samples_per_second =      7.751
  eval_steps_per_second   =      0.969
Namespace(model_name_or_path='lmsys/vicuna-7b-v1.5', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/vicuna-7b-v1.5-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/vicuna-7b-v1.5-phase2-1223/runs/Dec24_02-02-29_2117ga001', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/vicuna-7b-v1.5-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model lmsys/vicuna-7b-v1.5...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 79953920.0 || all params: 3660320768 || trainable: 2.184341894267557
torch.bfloat16 422051840 0.11530460491051696
torch.uint8 3238002688 0.8846226582948481
torch.float32 266240 7.273679463493403e-05
{'loss': 1.0216, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.8813, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.9595, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.7803, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.8313, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.8903, 'learning_rate': 0.0002, 'epoch': 0.59}
{'eval_loss': 0.802422285079956, 'eval_runtime': 123.974, 'eval_samples_per_second': 8.066, 'eval_steps_per_second': 1.008, 'epoch': 0.63}
{'mmlu_loss': 2.1947465601066747, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_world_history': 0.5, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.36470588235294116, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.5, 'mmlu_eval_accuracy_high_school_psychology': 0.7833333333333333, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.5555555555555556, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_moral_disputes': 0.39473684210526316, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.5294117647058824, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5217391304347826, 'mmlu_eval_accuracy_security_studies': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_microeconomics': 0.34615384615384615, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_miscellaneous': 0.627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.21, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_professional_accounting': 0.3225806451612903, 'mmlu_eval_accuracy': 0.48234563493063276, 'epoch': 0.63}
{'loss': 0.7892, 'learning_rate': 0.0002, 'epoch': 0.68}
{'loss': 0.8682, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.7365, 'learning_rate': 0.0002, 'epoch': 0.88}
{'loss': 0.7775, 'learning_rate': 0.0002, 'epoch': 0.98}
{'loss': 0.8382, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.7094, 'learning_rate': 0.0002, 'epoch': 1.17}
{'eval_loss': 0.7941049933433533, 'eval_runtime': 123.9009, 'eval_samples_per_second': 8.071, 'eval_steps_per_second': 1.009, 'epoch': 1.25}
{'mmlu_loss': 2.219414516662558, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_world_history': 0.5384615384615384, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.3588235294117647, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_high_school_psychology': 0.7666666666666667, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.5555555555555556, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.42105263157894735, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.5588235294117647, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_human_aging': 0.6086956521739131, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_us_history': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4186046511627907, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.22, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_professional_accounting': 0.3548387096774194, 'mmlu_eval_accuracy': 0.4766561678760385, 'epoch': 1.25}
{'loss': 0.7921, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.7307, 'learning_rate': 0.0002, 'epoch': 1.37}
{'loss': 0.7193, 'learning_rate': 0.0002, 'epoch': 1.47}
Saving PEFT checkpoint...
{'loss': 0.7983, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 0.6965, 'learning_rate': 0.0002, 'epoch': 1.66}
{'loss': 0.7701, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.7115, 'learning_rate': 0.0002, 'epoch': 1.86}
{'eval_loss': 0.7375524044036865, 'eval_runtime': 123.9872, 'eval_samples_per_second': 8.065, 'eval_steps_per_second': 1.008, 'epoch': 1.88}
{'mmlu_loss': 2.038565974061688, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_world_history': 0.5384615384615384, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_us_foreign_policy': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.37058823529411766, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.5, 'mmlu_eval_accuracy_high_school_psychology': 0.7833333333333333, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.4473684210526316, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_human_aging': 0.6086956521739131, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5454545454545454, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4186046511627907, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_miscellaneous': 0.6511627906976745, 'mmlu_eval_accuracy_moral_scenarios': 0.2, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_professional_accounting': 0.3548387096774194, 'mmlu_eval_accuracy': 0.47888389764216754, 'epoch': 1.88}
{'loss': 0.6991, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 0.7785, 'learning_rate': 0.0002, 'epoch': 2.05}
{'loss': 0.6556, 'learning_rate': 0.0002, 'epoch': 2.15}
{'loss': 0.6668, 'learning_rate': 0.0002, 'epoch': 2.25}
{'loss': 0.7468, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.6606, 'learning_rate': 0.0002, 'epoch': 2.44}
{'eval_loss': 0.7550709247589111, 'eval_runtime': 123.9662, 'eval_samples_per_second': 8.067, 'eval_steps_per_second': 1.008, 'epoch': 2.5}
{'mmlu_loss': 2.1166717167943716, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_world_history': 0.5384615384615384, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_us_foreign_policy': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.36470588235294116, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_high_school_psychology': 0.7833333333333333, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.47368421052631576, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.6176470588235294, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.7272727272727273, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5909090909090909, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.21, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_professional_accounting': 0.3548387096774194, 'mmlu_eval_accuracy': 0.47918504837852227, 'epoch': 2.5}
{'loss': 0.7305, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.6521, 'learning_rate': 0.0002, 'epoch': 2.64}
{'loss': 0.6569, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 0.757, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.658, 'learning_rate': 0.0002, 'epoch': 2.93}
Saving PEFT checkpoint...
{'loss': 0.7004, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.6468, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7114297151565552, 'eval_runtime': 123.978, 'eval_samples_per_second': 8.066, 'eval_steps_per_second': 1.008, 'epoch': 3.13}
{'mmlu_loss': 2.157911455258727, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_world_history': 0.5384615384615384, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.36470588235294116, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_high_school_psychology': 0.8, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.4473684210526316, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.6176470588235294, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5362318840579711, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.7272727272727273, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5454545454545454, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4186046511627907, 'mmlu_eval_accuracy_high_school_biology': 0.40625, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.22, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_electrical_engineering': 0.3125, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy': 0.47313319960053196, 'epoch': 3.13}
{'loss': 0.5949, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.7136, 'learning_rate': 0.0002, 'epoch': 3.33}
{'loss': 0.6296, 'learning_rate': 0.0002, 'epoch': 3.42}
{'loss': 0.6479, 'learning_rate': 0.0002, 'epoch': 3.52}
{'loss': 0.6621, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.6036, 'learning_rate': 0.0002, 'epoch': 3.72}
{'eval_loss': 0.7391510605812073, 'eval_runtime': 123.9754, 'eval_samples_per_second': 8.066, 'eval_steps_per_second': 1.008, 'epoch': 3.76}
{'mmlu_loss': 2.203178023919463, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_world_history': 0.5384615384615384, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_us_foreign_policy': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.36470588235294116, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_high_school_psychology': 0.8, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.47368421052631576, 'mmlu_eval_accuracy_high_school_mathematics': 0.20689655172413793, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.5142857142857142, 'mmlu_eval_accuracy_professional_psychology': 0.5217391304347826, 'mmlu_eval_accuracy_security_studies': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5454545454545454, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_biology': 0.40625, 'mmlu_eval_accuracy_miscellaneous': 0.6511627906976745, 'mmlu_eval_accuracy_moral_scenarios': 0.21, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_electrical_engineering': 0.3125, 'mmlu_eval_accuracy_professional_accounting': 0.3548387096774194, 'mmlu_eval_accuracy': 0.4698778977934645, 'epoch': 3.76}
{'loss': 0.7178, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.6267, 'learning_rate': 0.0002, 'epoch': 3.91}
{'loss': 0.6108, 'learning_rate': 0.0002, 'epoch': 4.01}
{'loss': 0.6832, 'learning_rate': 0.0002, 'epoch': 4.11}
{'loss': 0.5655, 'learning_rate': 0.0002, 'epoch': 4.21}
{'loss': 0.6189, 'learning_rate': 0.0002, 'epoch': 4.3}
{'eval_loss': 0.7186266183853149, 'eval_runtime': 123.9819, 'eval_samples_per_second': 8.066, 'eval_steps_per_second': 1.008, 'epoch': 4.38}
{'mmlu_loss': 2.3173411109795174, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_world_history': 0.5384615384615384, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.3588235294117647, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_high_school_psychology': 0.7833333333333333, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.4473684210526316, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.631578947368421, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.6176470588235294, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.4857142857142857, 'mmlu_eval_accuracy_professional_psychology': 0.5217391304347826, 'mmlu_eval_accuracy_security_studies': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5454545454545454, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4186046511627907, 'mmlu_eval_accuracy_high_school_biology': 0.40625, 'mmlu_eval_accuracy_miscellaneous': 0.627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.22, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy': 0.4726278088696345, 'epoch': 4.38}
{'loss': 0.5937, 'learning_rate': 0.0002, 'epoch': 4.4}
Saving PEFT checkpoint...
{'loss': 0.5324, 'learning_rate': 0.0002, 'epoch': 4.5}
{'loss': 0.6963, 'learning_rate': 0.0002, 'epoch': 4.6}
{'loss': 0.5868, 'learning_rate': 0.0002, 'epoch': 4.69}
{'loss': 0.6328, 'learning_rate': 0.0002, 'epoch': 4.79}
{'loss': 0.6037, 'learning_rate': 0.0002, 'epoch': 4.89}
{'loss': 0.5254, 'learning_rate': 0.0002, 'epoch': 4.99}
{'eval_loss': 0.7194883227348328, 'eval_runtime': 123.9218, 'eval_samples_per_second': 8.07, 'eval_steps_per_second': 1.009, 'epoch': 5.01}
{'mmlu_loss': 2.2455265975246825, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_high_school_world_history': 0.5769230769230769, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.38095238095238093, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_professional_medicine': 0.4838709677419355, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.375, 'mmlu_eval_accuracy_high_school_psychology': 0.7833333333333333, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.39473684210526316, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.5882352941176471, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.5142857142857142, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_security_studies': 0.5185185185185185, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_college_medicine': 0.4090909090909091, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_biology': 0.40625, 'mmlu_eval_accuracy_miscellaneous': 0.6511627906976745, 'mmlu_eval_accuracy_moral_scenarios': 0.23, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy': 0.4743196471127803, 'epoch': 5.01}
{'loss': 0.6872, 'learning_rate': 0.0002, 'epoch': 5.09}
{'loss': 0.5625, 'learning_rate': 0.0002, 'epoch': 5.18}
{'loss': 0.519, 'learning_rate': 0.0002, 'epoch': 5.28}
{'loss': 0.6025, 'learning_rate': 0.0002, 'epoch': 5.38}
{'loss': 0.4607, 'learning_rate': 0.0002, 'epoch': 5.48}
{'loss': 0.6555, 'learning_rate': 0.0002, 'epoch': 5.57}
{'eval_loss': 0.7144777774810791, 'eval_runtime': 123.9318, 'eval_samples_per_second': 8.069, 'eval_steps_per_second': 1.009, 'epoch': 5.63}
{'mmlu_loss': 2.3170021437108517, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_world_history': 0.6153846153846154, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.3588235294117647, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_professional_medicine': 0.4838709677419355, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_high_school_psychology': 0.7666666666666667, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.5555555555555556, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.4473684210526316, 'mmlu_eval_accuracy_high_school_mathematics': 0.20689655172413793, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.631578947368421, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.5588235294117647, 'mmlu_eval_accuracy_logical_fallacies': 0.5555555555555556, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.5142857142857142, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_security_studies': 0.5185185185185185, 'mmlu_eval_accuracy_high_school_microeconomics': 0.34615384615384615, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_college_medicine': 0.4090909090909091, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5454545454545454, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4186046511627907, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_miscellaneous': 0.6395348837209303, 'mmlu_eval_accuracy_moral_scenarios': 0.23, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy': 0.4716371803350756, 'epoch': 5.63}
{'loss': 0.575, 'learning_rate': 0.0002, 'epoch': 5.67}
{'loss': 0.5279, 'learning_rate': 0.0002, 'epoch': 5.77}
{'loss': 0.6077, 'learning_rate': 0.0002, 'epoch': 5.87}
Saving PEFT checkpoint...
{'loss': 0.4649, 'learning_rate': 0.0002, 'epoch': 5.97}
{'loss': 0.631, 'learning_rate': 0.0002, 'epoch': 6.06}
{'loss': 0.5393, 'learning_rate': 0.0002, 'epoch': 6.16}
{'loss': 0.3912, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.7908422946929932, 'eval_runtime': 123.8716, 'eval_samples_per_second': 8.073, 'eval_steps_per_second': 1.009, 'epoch': 6.26}
{'mmlu_loss': 2.2277616759141288, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.42857142857142855, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_professional_law': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_college_biology': 0.3125, 'mmlu_eval_accuracy_high_school_psychology': 0.7666666666666667, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.5555555555555556, 'mmlu_eval_accuracy_machine_learning': 0.09090909090909091, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_moral_disputes': 0.42105263157894735, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_econometrics': 0.16666666666666666, 'mmlu_eval_accuracy_world_religions': 0.6842105263157895, 'mmlu_eval_accuracy_formal_logic': 0.14285714285714285, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_philosophy': 0.5588235294117647, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_jurisprudence': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.5142857142857142, 'mmlu_eval_accuracy_professional_psychology': 0.5072463768115942, 'mmlu_eval_accuracy_security_studies': 0.48148148148148145, 'mmlu_eval_accuracy_high_school_microeconomics': 0.3076923076923077, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.6818181818181818, 'mmlu_eval_accuracy_management': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_us_history': 0.5454545454545454, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.4418604651162791, 'mmlu_eval_accuracy_high_school_biology': 0.4375, 'mmlu_eval_accuracy_miscellaneous': 0.627906976744186, 'mmlu_eval_accuracy_moral_scenarios': 0.22, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy': 0.4727097849243925, 'epoch': 6.26}
{'loss': 0.6391, 'learning_rate': 0.0002, 'epoch': 6.36}
{'train_runtime': 28624.8043, 'train_samples_per_second': 2.907, 'train_steps_per_second': 0.023, 'train_loss': 0.6772346173799955, 'epoch': 6.36}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       6.36
  train_loss               =     0.6772
  train_runtime            = 7:57:04.80
  train_samples_per_second =      2.907
  train_steps_per_second   =      0.023
***** eval metrics *****
  epoch                   =       6.36
  eval_loss               =      0.732
  eval_runtime            = 0:02:03.89
  eval_samples_per_second =      8.072
  eval_steps_per_second   =      1.009
Namespace(model_name_or_path='HuggingFaceH4/zephyr-7b-alpha', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/zephyr-7b-alpha-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/zephyr-7b-alpha-phase2-1223/runs/Dec24_10-07-34_2117ga001', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/zephyr-7b-alpha-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model HuggingFaceH4/zephyr-7b-alpha...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919843328 || trainable: 2.1400365519914986
torch.bfloat16 429916160 0.1096768732895643
torch.uint8 3489660928 0.8902552056284633
torch.float32 266240 6.792108197238643e-05
{'loss': 0.9179, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.7655, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.866, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.7321, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.7506, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.8404, 'learning_rate': 0.0002, 'epoch': 0.59}
{'eval_loss': 0.7582620978355408, 'eval_runtime': 126.4328, 'eval_samples_per_second': 7.909, 'eval_steps_per_second': 0.989, 'epoch': 0.63}
{'mmlu_loss': 2.653655944392085, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.37, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_clinical_knowledge': 0.5862068965517241, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.27586206896551724, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_security_studies': 0.5555555555555556, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_miscellaneous': 0.6744186046511628, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_professional_law': 0.3941176470588235, 'mmlu_eval_accuracy_global_facts': 0.6, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy': 0.5963584293898636, 'epoch': 0.63}
{'loss': 0.7195, 'learning_rate': 0.0002, 'epoch': 0.68}
{'loss': 0.7939, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.6974, 'learning_rate': 0.0002, 'epoch': 0.88}
{'loss': 0.7098, 'learning_rate': 0.0002, 'epoch': 0.98}
{'loss': 0.7861, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.6425, 'learning_rate': 0.0002, 'epoch': 1.17}
{'eval_loss': 0.77900230884552, 'eval_runtime': 126.471, 'eval_samples_per_second': 7.907, 'eval_steps_per_second': 0.988, 'epoch': 1.25}
{'mmlu_loss': 2.714219447846214, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.38, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_professional_law': 0.3764705882352941, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_world_history': 0.6538461538461539, 'mmlu_eval_accuracy': 0.5918024890822121, 'epoch': 1.25}
{'loss': 0.6859, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.6978, 'learning_rate': 0.0002, 'epoch': 1.37}
{'loss': 0.6324, 'learning_rate': 0.0002, 'epoch': 1.47}
Saving PEFT checkpoint...
{'loss': 0.7387, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 0.6447, 'learning_rate': 0.0002, 'epoch': 1.66}
{'loss': 0.6849, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.6867, 'learning_rate': 0.0002, 'epoch': 1.86}
{'eval_loss': 0.7201505899429321, 'eval_runtime': 126.566, 'eval_samples_per_second': 7.901, 'eval_steps_per_second': 0.988, 'epoch': 1.88}
{'mmlu_loss': 2.777148544167479, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_professional_law': 0.3764705882352941, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy': 0.5958421196656004, 'epoch': 1.88}
{'loss': 0.6315, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 0.7168, 'learning_rate': 0.0002, 'epoch': 2.05}
{'loss': 0.6093, 'learning_rate': 0.0002, 'epoch': 2.15}
{'loss': 0.519, 'learning_rate': 0.0002, 'epoch': 2.25}
{'loss': 0.7144, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.5749, 'learning_rate': 0.0002, 'epoch': 2.44}
{'eval_loss': 0.7504211068153381, 'eval_runtime': 126.4624, 'eval_samples_per_second': 7.907, 'eval_steps_per_second': 0.988, 'epoch': 2.5}
{'mmlu_loss': 2.962878957390785, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.37, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_professional_psychology': 0.5652173913043478, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6976744186046512, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy': 0.595161700954148, 'epoch': 2.5}
{'loss': 0.6311, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.6153, 'learning_rate': 0.0002, 'epoch': 2.64}
{'loss': 0.5238, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 0.7194, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.5792, 'learning_rate': 0.0002, 'epoch': 2.93}
Saving PEFT checkpoint...
{'loss': 0.6044, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.6119, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7123981714248657, 'eval_runtime': 126.5498, 'eval_samples_per_second': 7.902, 'eval_steps_per_second': 0.988, 'epoch': 3.13}
{'mmlu_loss': 2.9484955463558435, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.42, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_professional_psychology': 0.5652173913043478, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_professional_accounting': 0.3870967741935484, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_professional_law': 0.4235294117647059, 'mmlu_eval_accuracy_global_facts': 0.6, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy': 0.5931085546767279, 'epoch': 3.13}
{'loss': 0.417, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.6486, 'learning_rate': 0.0002, 'epoch': 3.33}
{'loss': 0.5401, 'learning_rate': 0.0002, 'epoch': 3.42}
{'loss': 0.4844, 'learning_rate': 0.0002, 'epoch': 3.52}
{'loss': 0.6241, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.4542, 'learning_rate': 0.0002, 'epoch': 3.72}
{'eval_loss': 0.8039422631263733, 'eval_runtime': 126.5206, 'eval_samples_per_second': 7.904, 'eval_steps_per_second': 0.988, 'epoch': 3.76}
{'mmlu_loss': 2.9586757651219764, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.36, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_professional_psychology': 0.5507246376811594, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.7142857142857143, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_professional_law': 0.4, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy': 0.5963076068036162, 'epoch': 3.76}
{'loss': 0.6481, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.5512, 'learning_rate': 0.0002, 'epoch': 3.91}
{'loss': 0.4738, 'learning_rate': 0.0002, 'epoch': 4.01}
{'loss': 0.6328, 'learning_rate': 0.0002, 'epoch': 4.11}
{'loss': 0.4, 'learning_rate': 0.0002, 'epoch': 4.21}
{'loss': 0.5258, 'learning_rate': 0.0002, 'epoch': 4.3}
{'eval_loss': 0.7489664554595947, 'eval_runtime': 126.549, 'eval_samples_per_second': 7.902, 'eval_steps_per_second': 0.988, 'epoch': 4.38}
{'mmlu_loss': 3.0742333941161633, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_professional_psychology': 0.5507246376811594, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_professional_law': 0.4411764705882353, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy': 0.5930295242514783, 'epoch': 4.38}
{'loss': 0.5364, 'learning_rate': 0.0002, 'epoch': 4.4}
Saving PEFT checkpoint...
{'loss': 0.3363, 'learning_rate': 0.0002, 'epoch': 4.5}
{'loss': 0.6556, 'learning_rate': 0.0002, 'epoch': 4.6}
{'loss': 0.439, 'learning_rate': 0.0002, 'epoch': 4.69}
{'loss': 0.5241, 'learning_rate': 0.0002, 'epoch': 4.79}
{'loss': 0.5573, 'learning_rate': 0.0002, 'epoch': 4.89}
{'loss': 0.3433, 'learning_rate': 0.0002, 'epoch': 4.99}
{'eval_loss': 0.8048546314239502, 'eval_runtime': 126.5297, 'eval_samples_per_second': 7.903, 'eval_steps_per_second': 0.988, 'epoch': 5.01}
{'mmlu_loss': 3.241852850963672, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.38, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_professional_psychology': 0.5507246376811594, 'mmlu_eval_accuracy_clinical_knowledge': 0.6896551724137931, 'mmlu_eval_accuracy_high_school_psychology': 0.9166666666666666, 'mmlu_eval_accuracy_medical_genetics': 1.0, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_professional_law': 0.4117647058823529, 'mmlu_eval_accuracy_global_facts': 0.6, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy': 0.5959075206175329, 'epoch': 5.01}
{'loss': 0.6248, 'learning_rate': 0.0002, 'epoch': 5.09}
{'loss': 0.428, 'learning_rate': 0.0002, 'epoch': 5.18}
{'loss': 0.3718, 'learning_rate': 0.0002, 'epoch': 5.28}
{'loss': 0.5536, 'learning_rate': 0.0002, 'epoch': 5.38}
{'loss': 0.2729, 'learning_rate': 0.0002, 'epoch': 5.48}
{'loss': 0.5938, 'learning_rate': 0.0002, 'epoch': 5.57}
{'eval_loss': 0.7503455877304077, 'eval_runtime': 126.5037, 'eval_samples_per_second': 7.905, 'eval_steps_per_second': 0.988, 'epoch': 5.63}
{'mmlu_loss': 2.943867474794388, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.38, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_professional_psychology': 0.5362318840579711, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_high_school_psychology': 0.9166666666666666, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_professional_medicine': 0.6774193548387096, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_college_computer_science': 0.18181818181818182, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_professional_law': 0.4235294117647059, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy': 0.5902229210650827, 'epoch': 5.63}
{'loss': 0.439, 'learning_rate': 0.0002, 'epoch': 5.67}
{'loss': 0.3712, 'learning_rate': 0.0002, 'epoch': 5.77}
{'loss': 0.5677, 'learning_rate': 0.0002, 'epoch': 5.87}
Saving PEFT checkpoint...
{'loss': 0.2767, 'learning_rate': 0.0002, 'epoch': 5.97}
{'loss': 0.54, 'learning_rate': 0.0002, 'epoch': 6.06}
{'loss': 0.4316, 'learning_rate': 0.0002, 'epoch': 6.16}
{'loss': 0.2221, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.9437435269355774, 'eval_runtime': 126.5108, 'eval_samples_per_second': 7.904, 'eval_steps_per_second': 0.988, 'epoch': 6.26}
{'mmlu_loss': 3.2389283925294876, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_world_religions': 0.7894736842105263, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.41, 'mmlu_eval_accuracy_college_biology': 0.75, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_professional_psychology': 0.5652173913043478, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_high_school_psychology': 0.9166666666666666, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_professional_medicine': 0.6451612903225806, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_international_law': 0.9230769230769231, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_moral_disputes': 0.5263157894736842, 'mmlu_eval_accuracy_nutrition': 0.5757575757575758, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_professional_law': 0.4411764705882353, 'mmlu_eval_accuracy_global_facts': 0.6, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy': 0.5954313841505451, 'epoch': 6.26}
{'loss': 0.5692, 'learning_rate': 0.0002, 'epoch': 6.36}
{'train_runtime': 28434.7255, 'train_samples_per_second': 2.926, 'train_steps_per_second': 0.023, 'train_loss': 0.5861230373382569, 'epoch': 6.36}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       6.36
  train_loss               =     0.5861
  train_runtime            = 7:53:54.72
  train_samples_per_second =      2.926
  train_steps_per_second   =      0.023
***** eval metrics *****
  epoch                   =       6.36
  eval_loss               =     0.7504
  eval_runtime            = 0:02:06.52
  eval_samples_per_second =      7.904
  eval_steps_per_second   =      0.988
