Alpaca prompt format. modified 'unk_token' = 0 in qlora.py (originally 2 because pad_token_id = 2 for zephyr). model-phase2-date Finetune base models on gpt generated data and gpt paraphrased data.
Namespace(model_name_or_path='mistralai/Mistral-7B-Instruct-v0.2', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/Mistral-7B-Instruct-v0.2-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mistral-7B-Instruct-v0.2-phase2-1223/runs/Dec24_04-37-15_2119ga002', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/Mistral-7B-Instruct-v0.2-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mistral-7B-Instruct-v0.2...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 83886080.0 || all params: 3919851520 || trainable: 2.140032079582443
torch.bfloat16 429924352 0.10967873395367791
torch.uint8 3489660928 0.8902533451062963
torch.float32 266240 6.792094002580996e-05
{'loss': 0.992, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.8169, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.9163, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.7374, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.7797, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.8802, 'learning_rate': 0.0002, 'epoch': 0.59}
{'eval_loss': 0.7826634049415588, 'eval_runtime': 128.8966, 'eval_samples_per_second': 7.758, 'eval_steps_per_second': 0.97, 'epoch': 0.63}
{'mmlu_loss': 2.5995431089152894, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_moral_scenarios': 0.35, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6153846153846154, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_world_history': 0.6923076923076923, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_sociology': 0.7727272727272727, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_clinical_knowledge': 0.5862068965517241, 'mmlu_eval_accuracy_conceptual_physics': 0.6153846153846154, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_abstract_algebra': 0.09090909090909091, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_high_school_mathematics': 0.4827586206896552, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.4470588235294118, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy': 0.6117032856981871, 'epoch': 0.63}
{'loss': 0.7492, 'learning_rate': 0.0002, 'epoch': 0.68}
{'loss': 0.8429, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.7016, 'learning_rate': 0.0002, 'epoch': 0.88}
{'loss': 0.7321, 'learning_rate': 0.0002, 'epoch': 0.98}
{'loss': 0.8155, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.6504, 'learning_rate': 0.0002, 'epoch': 1.17}
{'eval_loss': 0.8023205399513245, 'eval_runtime': 128.7344, 'eval_samples_per_second': 7.768, 'eval_steps_per_second': 0.971, 'epoch': 1.25}
{'mmlu_loss': 2.370823687252899, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_moral_scenarios': 0.34, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_microeconomics': 0.5769230769230769, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_jurisprudence': 0.7272727272727273, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9047619047619048, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_clinical_knowledge': 0.6551724137931034, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.45294117647058824, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy': 0.6177441491353037, 'epoch': 1.25}
{'loss': 0.7122, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.7036, 'learning_rate': 0.0002, 'epoch': 1.37}
{'loss': 0.6451, 'learning_rate': 0.0002, 'epoch': 1.47}
Saving PEFT checkpoint...
{'loss': 0.7607, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 0.6535, 'learning_rate': 0.0002, 'epoch': 1.66}
{'loss': 0.7081, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.6861, 'learning_rate': 0.0002, 'epoch': 1.86}
{'eval_loss': 0.7311581969261169, 'eval_runtime': 128.7052, 'eval_samples_per_second': 7.77, 'eval_steps_per_second': 0.971, 'epoch': 1.88}
{'mmlu_loss': 2.6469144926716885, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_moral_scenarios': 0.34, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_sociology': 0.8181818181818182, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_psychology': 0.9, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_high_school_mathematics': 0.4827586206896552, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.4823529411764706, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy': 0.6152809073583534, 'epoch': 1.88}
{'loss': 0.6397, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 0.7315, 'learning_rate': 0.0002, 'epoch': 2.05}
{'loss': 0.6016, 'learning_rate': 0.0002, 'epoch': 2.15}
{'loss': 0.5164, 'learning_rate': 0.0002, 'epoch': 2.25}
{'loss': 0.7188, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.5749, 'learning_rate': 0.0002, 'epoch': 2.44}
{'eval_loss': 0.7590822577476501, 'eval_runtime': 128.7688, 'eval_samples_per_second': 7.766, 'eval_steps_per_second': 0.971, 'epoch': 2.5}
{'mmlu_loss': 2.592708071383337, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_moral_scenarios': 0.36, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_world_history': 0.7307692307692307, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_physics': 0.058823529411764705, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_high_school_mathematics': 0.5517241379310345, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.45294117647058824, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy': 0.6205195404892796, 'epoch': 2.5}
{'loss': 0.6403, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.6102, 'learning_rate': 0.0002, 'epoch': 2.64}
{'loss': 0.5244, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 0.7317, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.5838, 'learning_rate': 0.0002, 'epoch': 2.93}
Saving PEFT checkpoint...
{'loss': 0.6117, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.5961, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7287679314613342, 'eval_runtime': 128.7878, 'eval_samples_per_second': 7.765, 'eval_steps_per_second': 0.971, 'epoch': 3.13}
{'mmlu_loss': 3.0017116895566383, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_moral_scenarios': 0.37, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.4634146341463415, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6153846153846154, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_miscellaneous': 0.6744186046511628, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_high_school_mathematics': 0.6206896551724138, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.4647058823529412, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy': 0.6215147958627536, 'epoch': 3.13}
{'loss': 0.4087, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.6401, 'learning_rate': 0.0002, 'epoch': 3.33}
{'loss': 0.5304, 'learning_rate': 0.0002, 'epoch': 3.42}
{'loss': 0.4917, 'learning_rate': 0.0002, 'epoch': 3.52}
{'loss': 0.6151, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.4406, 'learning_rate': 0.0002, 'epoch': 3.72}
{'eval_loss': 0.7943761944770813, 'eval_runtime': 128.7613, 'eval_samples_per_second': 7.766, 'eval_steps_per_second': 0.971, 'epoch': 3.76}
{'mmlu_loss': 3.058618122090896, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_moral_scenarios': 0.3, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_biology': 0.6875, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_miscellaneous': 0.6744186046511628, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6764705882352942, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_high_school_mathematics': 0.5172413793103449, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.4588235294117647, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5555555555555556, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy': 0.6127015885276249, 'epoch': 3.76}
{'loss': 0.6505, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.545, 'learning_rate': 0.0002, 'epoch': 3.91}
{'loss': 0.4541, 'learning_rate': 0.0002, 'epoch': 4.01}
{'loss': 0.6252, 'learning_rate': 0.0002, 'epoch': 4.11}
{'loss': 0.3781, 'learning_rate': 0.0002, 'epoch': 4.21}
{'loss': 0.5018, 'learning_rate': 0.0002, 'epoch': 4.3}
{'eval_loss': 0.7563692331314087, 'eval_runtime': 128.7422, 'eval_samples_per_second': 7.767, 'eval_steps_per_second': 0.971, 'epoch': 4.38}
{'mmlu_loss': 3.2277735055734715, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.35, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.4634146341463415, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_us_history': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_high_school_mathematics': 0.5517241379310345, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.49411764705882355, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy': 0.6146446975731116, 'epoch': 4.38}
{'loss': 0.5085, 'learning_rate': 0.0002, 'epoch': 4.4}
Saving PEFT checkpoint...
{'loss': 0.3181, 'learning_rate': 0.0002, 'epoch': 4.5}
{'loss': 0.6422, 'learning_rate': 0.0002, 'epoch': 4.6}
{'loss': 0.4178, 'learning_rate': 0.0002, 'epoch': 4.69}
{'loss': 0.5142, 'learning_rate': 0.0002, 'epoch': 4.79}
{'loss': 0.5315, 'learning_rate': 0.0002, 'epoch': 4.89}
{'loss': 0.3202, 'learning_rate': 0.0002, 'epoch': 4.99}
{'eval_loss': 0.7682018280029297, 'eval_runtime': 128.7766, 'eval_samples_per_second': 7.765, 'eval_steps_per_second': 0.971, 'epoch': 5.01}
{'mmlu_loss': 3.0085669880112014, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.5813953488372093, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.8, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_sociology': 0.9545454545454546, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_us_history': 0.6818181818181818, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_high_school_mathematics': 0.5172413793103449, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.47058823529411764, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5555555555555556, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy': 0.6120710411614169, 'epoch': 5.01}
{'loss': 0.6026, 'learning_rate': 0.0002, 'epoch': 5.09}
{'loss': 0.398, 'learning_rate': 0.0002, 'epoch': 5.18}
{'loss': 0.3698, 'learning_rate': 0.0002, 'epoch': 5.28}
{'loss': 0.5229, 'learning_rate': 0.0002, 'epoch': 5.38}
{'loss': 0.2439, 'learning_rate': 0.0002, 'epoch': 5.48}
{'loss': 0.5855, 'learning_rate': 0.0002, 'epoch': 5.57}
{'eval_loss': 0.7654271125793457, 'eval_runtime': 128.8558, 'eval_samples_per_second': 7.761, 'eval_steps_per_second': 0.97, 'epoch': 5.63}
{'mmlu_loss': 3.226047891502579, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_moral_scenarios': 0.39, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.4634146341463415, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_us_history': 0.6818181818181818, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.6976744186046512, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7142857142857143, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy_high_school_mathematics': 0.5172413793103449, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_law': 0.4823529411764706, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.5925925925925926, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy': 0.611357842411465, 'epoch': 5.63}
{'loss': 0.4116, 'learning_rate': 0.0002, 'epoch': 5.67}
{'loss': 0.3723, 'learning_rate': 0.0002, 'epoch': 5.77}
{'loss': 0.5337, 'learning_rate': 0.0002, 'epoch': 5.87}
Saving PEFT checkpoint...
{'loss': 0.2445, 'learning_rate': 0.0002, 'epoch': 5.97}
{'loss': 0.5266, 'learning_rate': 0.0002, 'epoch': 6.06}
{'loss': 0.393, 'learning_rate': 0.0002, 'epoch': 6.16}
{'loss': 0.203, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.9336754083633423, 'eval_runtime': 128.814, 'eval_samples_per_second': 7.763, 'eval_steps_per_second': 0.97, 'epoch': 6.26}
{'mmlu_loss': 3.500124460707108, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_anatomy': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_sociology': 0.9545454545454546, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_high_school_us_history': 0.6818181818181818, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_miscellaneous': 0.686046511627907, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7619047619047619, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_philosophy': 0.6470588235294118, 'mmlu_eval_accuracy_clinical_knowledge': 0.6206896551724138, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_college_mathematics': 0.5454545454545454, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy_high_school_mathematics': 0.4827586206896552, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_logical_fallacies': 0.6666666666666666, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_law': 0.4823529411764706, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_geography': 0.9545454545454546, 'mmlu_eval_accuracy': 0.6090226544658088, 'epoch': 6.26}
{'loss': 0.5472, 'learning_rate': 0.0002, 'epoch': 6.36}
{'train_runtime': 28675.3796, 'train_samples_per_second': 2.901, 'train_steps_per_second': 0.023, 'train_loss': 0.5854328430615938, 'epoch': 6.36}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       6.36
  train_loss               =     0.5854
  train_runtime            = 7:57:55.37
  train_samples_per_second =      2.901
  train_steps_per_second   =      0.023
***** eval metrics *****
  epoch                   =       6.36
  eval_loss               =     0.7862
  eval_runtime            = 0:02:08.78
  eval_samples_per_second =      7.765
  eval_steps_per_second   =      0.971
Namespace(model_name_or_path='mistralai/Mixtral-8x7B-Instruct-v0.1', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/Mixtral-8x7B-Instruct-v0.1-phase2-1223', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/Mixtral-8x7B-Instruct-v0.1-phase2-1223/runs/Dec24_12-43-01_2119ga002', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/Mixtral-8x7B-Instruct-v0.1-phase2-1223', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model mistralai/Mixtral-8x7B-Instruct-v0.1...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 484450304.0 || all params: 24451510272 || trainable: 1.9812694537513107
torch.bfloat16 1231052800 0.05034669786470031
torch.uint8 23220191232 0.9496424136463255
torch.float32 266240 1.0888488974232306e-05
{'loss': 0.9687, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.7773, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.8742, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.7142, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.7519, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.8454, 'learning_rate': 0.0002, 'epoch': 0.59}
{'eval_loss': 0.7605315446853638, 'eval_runtime': 301.419, 'eval_samples_per_second': 3.318, 'eval_steps_per_second': 0.415, 'epoch': 0.63}
{'mmlu_loss': 1.8207399612292647, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_moral_scenarios': 0.53, 'mmlu_eval_accuracy_jurisprudence': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy_professional_medicine': 0.8064516129032258, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_nutrition': 0.7878787878787878, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_elementary_mathematics': 0.6097560975609756, 'mmlu_eval_accuracy_prehistory': 0.8, 'mmlu_eval_accuracy_miscellaneous': 0.7906976744186046, 'mmlu_eval_accuracy_management': 0.9090909090909091, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_professional_law': 0.5823529411764706, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy': 0.7028260709708521, 'epoch': 0.63}
{'loss': 0.7219, 'learning_rate': 0.0002, 'epoch': 0.68}
{'loss': 0.8042, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.6747, 'learning_rate': 0.0002, 'epoch': 0.88}
{'loss': 0.6968, 'learning_rate': 0.0002, 'epoch': 0.98}
{'loss': 0.7899, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.6261, 'learning_rate': 0.0002, 'epoch': 1.17}
{'eval_loss': 0.7834702134132385, 'eval_runtime': 301.8345, 'eval_samples_per_second': 3.313, 'eval_steps_per_second': 0.414, 'epoch': 1.25}
{'mmlu_loss': 1.7261269567534328, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_geography': 1.0, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_moral_scenarios': 0.58, 'mmlu_eval_accuracy_jurisprudence': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.7741935483870968, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_nutrition': 0.7878787878787878, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_elementary_mathematics': 0.6341463414634146, 'mmlu_eval_accuracy_prehistory': 0.8571428571428571, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.6086956521739131, 'mmlu_eval_accuracy_professional_law': 0.5823529411764706, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_professional_accounting': 0.6129032258064516, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy': 0.7074427635214047, 'epoch': 1.25}
{'loss': 0.6816, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.6789, 'learning_rate': 0.0002, 'epoch': 1.37}
{'loss': 0.6187, 'learning_rate': 0.0002, 'epoch': 1.47}
Saving PEFT checkpoint...
{'loss': 0.7361, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 0.6303, 'learning_rate': 0.0002, 'epoch': 1.66}
{'loss': 0.6826, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.664, 'learning_rate': 0.0002, 'epoch': 1.86}
{'eval_loss': 0.7089322805404663, 'eval_runtime': 301.4192, 'eval_samples_per_second': 3.318, 'eval_steps_per_second': 0.415, 'epoch': 1.88}
{'mmlu_loss': 1.6825773678719997, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_biology': 0.71875, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_moral_scenarios': 0.56, 'mmlu_eval_accuracy_jurisprudence': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_elementary_mathematics': 0.6097560975609756, 'mmlu_eval_accuracy_prehistory': 0.8, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.6086956521739131, 'mmlu_eval_accuracy_professional_law': 0.5823529411764706, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy': 0.6973175472441488, 'epoch': 1.88}
{'loss': 0.6219, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 0.7139, 'learning_rate': 0.0002, 'epoch': 2.05}
{'loss': 0.5814, 'learning_rate': 0.0002, 'epoch': 2.15}
{'loss': 0.4914, 'learning_rate': 0.0002, 'epoch': 2.25}
{'loss': 0.6955, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.5536, 'learning_rate': 0.0002, 'epoch': 2.44}
{'eval_loss': 0.7650157809257507, 'eval_runtime': 301.9123, 'eval_samples_per_second': 3.312, 'eval_steps_per_second': 0.414, 'epoch': 2.5}
{'mmlu_loss': 1.804464885033667, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.54, 'mmlu_eval_accuracy_jurisprudence': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.8888888888888888, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.7741935483870968, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_nutrition': 0.7878787878787878, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_elementary_mathematics': 0.6341463414634146, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.6086956521739131, 'mmlu_eval_accuracy_professional_law': 0.5529411764705883, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.6153846153846154, 'mmlu_eval_accuracy': 0.7098199971858002, 'epoch': 2.5}
{'loss': 0.6235, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.5925, 'learning_rate': 0.0002, 'epoch': 2.64}
{'loss': 0.5054, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 0.7143, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.5604, 'learning_rate': 0.0002, 'epoch': 2.93}
Saving PEFT checkpoint...
{'loss': 0.5872, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.5776, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7126870155334473, 'eval_runtime': 301.6749, 'eval_samples_per_second': 3.315, 'eval_steps_per_second': 0.414, 'epoch': 3.13}
{'mmlu_loss': 1.8143842158218224, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.875, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_moral_scenarios': 0.53, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9523809523809523, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.8387096774193549, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_nutrition': 0.7878787878787878, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_elementary_mathematics': 0.5609756097560976, 'mmlu_eval_accuracy_prehistory': 0.8285714285714286, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.6086956521739131, 'mmlu_eval_accuracy_professional_law': 0.5647058823529412, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_conceptual_physics': 0.6153846153846154, 'mmlu_eval_accuracy': 0.7086173727367915, 'epoch': 3.13}
{'loss': 0.3819, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.6163, 'learning_rate': 0.0002, 'epoch': 3.33}
{'loss': 0.5105, 'learning_rate': 0.0002, 'epoch': 3.42}
{'loss': 0.464, 'learning_rate': 0.0002, 'epoch': 3.52}
{'loss': 0.602, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.4196, 'learning_rate': 0.0002, 'epoch': 3.72}
{'eval_loss': 0.7955430150032043, 'eval_runtime': 301.7121, 'eval_samples_per_second': 3.314, 'eval_steps_per_second': 0.414, 'epoch': 3.76}
{'mmlu_loss': 1.858954935024182, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_professional_psychology': 0.7101449275362319, 'mmlu_eval_accuracy_moral_scenarios': 0.53, 'mmlu_eval_accuracy_jurisprudence': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9047619047619048, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6046511627906976, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.9230769230769231, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_elementary_mathematics': 0.5365853658536586, 'mmlu_eval_accuracy_prehistory': 0.8857142857142857, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_professional_law': 0.5411764705882353, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.4117647058823529, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_chemistry': 0.5, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_high_school_world_history': 0.8846153846153846, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.6538461538461539, 'mmlu_eval_accuracy': 0.7039058112493296, 'epoch': 3.76}
{'loss': 0.633, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.5305, 'learning_rate': 0.0002, 'epoch': 3.91}
{'loss': 0.4233, 'learning_rate': 0.0002, 'epoch': 4.01}
{'loss': 0.611, 'learning_rate': 0.0002, 'epoch': 4.11}
{'loss': 0.3566, 'learning_rate': 0.0002, 'epoch': 4.21}
{'loss': 0.4803, 'learning_rate': 0.0002, 'epoch': 4.3}
{'eval_loss': 0.7393915057182312, 'eval_runtime': 301.6658, 'eval_samples_per_second': 3.315, 'eval_steps_per_second': 0.414, 'epoch': 4.38}
{'mmlu_loss': 1.939779616271456, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.8181818181818182, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_professional_psychology': 0.7101449275362319, 'mmlu_eval_accuracy_moral_scenarios': 0.52, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9047619047619048, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8846153846153846, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_world_religions': 0.8947368421052632, 'mmlu_eval_accuracy_elementary_mathematics': 0.5609756097560976, 'mmlu_eval_accuracy_prehistory': 0.8571428571428571, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.6086956521739131, 'mmlu_eval_accuracy_professional_law': 0.5411764705882353, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy': 0.6969517449965128, 'epoch': 4.38}
{'loss': 0.4932, 'learning_rate': 0.0002, 'epoch': 4.4}
Saving PEFT checkpoint...
{'loss': 0.2883, 'learning_rate': 0.0002, 'epoch': 4.5}
{'loss': 0.6314, 'learning_rate': 0.0002, 'epoch': 4.6}
{'loss': 0.3957, 'learning_rate': 0.0002, 'epoch': 4.69}
{'loss': 0.4981, 'learning_rate': 0.0002, 'epoch': 4.79}
{'loss': 0.5161, 'learning_rate': 0.0002, 'epoch': 4.89}
{'loss': 0.2858, 'learning_rate': 0.0002, 'epoch': 4.99}
{'eval_loss': 0.7493340969085693, 'eval_runtime': 301.6953, 'eval_samples_per_second': 3.315, 'eval_steps_per_second': 0.414, 'epoch': 5.01}
{'mmlu_loss': 1.8269057512904208, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_high_school_european_history': 0.8888888888888888, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.8333333333333334, 'mmlu_eval_accuracy_professional_psychology': 0.6811594202898551, 'mmlu_eval_accuracy_moral_scenarios': 0.56, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9047619047619048, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_marketing': 0.92, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_elementary_mathematics': 0.5609756097560976, 'mmlu_eval_accuracy_prehistory': 0.8, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_computer_security': 0.8181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.6086956521739131, 'mmlu_eval_accuracy_professional_law': 0.5764705882352941, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.6153846153846154, 'mmlu_eval_accuracy': 0.7077072264357502, 'epoch': 5.01}
{'loss': 0.5859, 'learning_rate': 0.0002, 'epoch': 5.09}
{'loss': 0.381, 'learning_rate': 0.0002, 'epoch': 5.18}
{'loss': 0.3463, 'learning_rate': 0.0002, 'epoch': 5.28}
{'loss': 0.5087, 'learning_rate': 0.0002, 'epoch': 5.38}
{'loss': 0.219, 'learning_rate': 0.0002, 'epoch': 5.48}
{'loss': 0.5661, 'learning_rate': 0.0002, 'epoch': 5.57}
{'eval_loss': 0.7548927068710327, 'eval_runtime': 301.7467, 'eval_samples_per_second': 3.314, 'eval_steps_per_second': 0.414, 'epoch': 5.63}
{'mmlu_loss': 1.8885748855148752, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8275862068965517, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.56, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9047619047619048, 'mmlu_eval_accuracy_public_relations': 0.4166666666666667, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8461538461538461, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_nutrition': 0.7575757575757576, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_elementary_mathematics': 0.5853658536585366, 'mmlu_eval_accuracy_prehistory': 0.8571428571428571, 'mmlu_eval_accuracy_miscellaneous': 0.8255813953488372, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.6086956521739131, 'mmlu_eval_accuracy_professional_law': 0.5470588235294118, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_philosophy': 0.8235294117647058, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.6578947368421053, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy': 0.7007459007421725, 'epoch': 5.63}
{'loss': 0.3905, 'learning_rate': 0.0002, 'epoch': 5.67}
{'loss': 0.3548, 'learning_rate': 0.0002, 'epoch': 5.77}
{'loss': 0.5263, 'learning_rate': 0.0002, 'epoch': 5.87}
Saving PEFT checkpoint...
{'loss': 0.2259, 'learning_rate': 0.0002, 'epoch': 5.97}
{'loss': 0.5104, 'learning_rate': 0.0002, 'epoch': 6.06}
{'loss': 0.3854, 'learning_rate': 0.0002, 'epoch': 6.16}
{'loss': 0.1912, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.9957911372184753, 'eval_runtime': 301.6517, 'eval_samples_per_second': 3.315, 'eval_steps_per_second': 0.414, 'epoch': 6.26}
{'mmlu_loss': 1.9711820967495441, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_high_school_psychology': 0.8833333333333333, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_clinical_knowledge': 0.8620689655172413, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_formal_logic': 0.35714285714285715, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_astronomy': 0.8125, 'mmlu_eval_accuracy_high_school_biology': 0.75, 'mmlu_eval_accuracy_anatomy': 0.6428571428571429, 'mmlu_eval_accuracy_logical_fallacies': 0.7777777777777778, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_moral_scenarios': 0.56, 'mmlu_eval_accuracy_jurisprudence': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.9047619047619048, 'mmlu_eval_accuracy_public_relations': 0.5, 'mmlu_eval_accuracy_high_school_computer_science': 0.7777777777777778, 'mmlu_eval_accuracy_econometrics': 0.75, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy_professional_medicine': 0.7419354838709677, 'mmlu_eval_accuracy_international_law': 1.0, 'mmlu_eval_accuracy_high_school_microeconomics': 0.8846153846153846, 'mmlu_eval_accuracy_marketing': 0.96, 'mmlu_eval_accuracy_nutrition': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_world_religions': 0.9473684210526315, 'mmlu_eval_accuracy_elementary_mathematics': 0.5609756097560976, 'mmlu_eval_accuracy_prehistory': 0.8, 'mmlu_eval_accuracy_miscellaneous': 0.813953488372093, 'mmlu_eval_accuracy_management': 1.0, 'mmlu_eval_accuracy_security_studies': 0.8148148148148148, 'mmlu_eval_accuracy_computer_security': 0.7272727272727273, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.6086956521739131, 'mmlu_eval_accuracy_professional_law': 0.5588235294117647, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_college_medicine': 0.8181818181818182, 'mmlu_eval_accuracy_college_biology': 0.875, 'mmlu_eval_accuracy_high_school_physics': 0.4117647058823529, 'mmlu_eval_accuracy_philosophy': 0.8235294117647058, 'mmlu_eval_accuracy_high_school_chemistry': 0.5454545454545454, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.7272727272727273, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_conceptual_physics': 0.5769230769230769, 'mmlu_eval_accuracy': 0.7025680388761562, 'epoch': 6.26}
{'loss': 0.532, 'learning_rate': 0.0002, 'epoch': 6.36}
{'train_runtime': 60319.0754, 'train_samples_per_second': 1.379, 'train_steps_per_second': 0.011, 'train_loss': 0.5633443344556368, 'epoch': 6.36}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =        6.36
  train_loss               =      0.5633
  train_runtime            = 16:45:19.07
  train_samples_per_second =       1.379
  train_steps_per_second   =       0.011
***** eval metrics *****
  epoch                   =       6.36
  eval_loss               =     0.7647
  eval_runtime            = 0:05:01.89
  eval_samples_per_second =      3.312
  eval_steps_per_second   =      0.414
