Alpaca prompt format. samantha-v1.1-7b-phase2-1226
Namespace(model_name_or_path='ehartford/Samantha-1.11-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=1000, source_max_len=256, target_max_len=512, dataset='/cbica/home/xjia/qlora/data/phase2/gpt_online.csv', dataset_format='alpaca', output_dir='./output/samantha-v1.1-7b-phase2-1226', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=650, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/samantha-v1.1-7b-phase2-1226/runs/Dec26_10-06-44_211affn005', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=150, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=64, dataloader_num_workers=3, past_index=-1, run_name='./output/samantha-v1.1-7b-phase2-1226', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=False, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 256
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=True, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
loading base model ehartford/Samantha-1.11-7b...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
trainable params: 79953920.0 || all params: 3660320768 || trainable: 2.184341894267557
torch.bfloat16 422051840 0.11530460491051696
torch.uint8 3238002688 0.8846226582948481
torch.float32 266240 7.273679463493403e-05
{'loss': 1.0178, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.9011, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.9887, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.8037, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.8568, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.912, 'learning_rate': 0.0002, 'epoch': 0.59}
{'eval_loss': 0.8278530836105347, 'eval_runtime': 123.7016, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 1.01, 'epoch': 0.63}
{'mmlu_loss': 2.3098612713317075, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_nutrition': 0.5151515151515151, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_high_school_psychology': 0.5333333333333333, 'mmlu_eval_accuracy_professional_law': 0.28823529411764703, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.34782608695652173, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.3888888888888889, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_sociology': 0.5, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.125, 'mmlu_eval_accuracy_human_aging': 0.6521739130434783, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.72, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.42105263157894735, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_philosophy': 0.4117647058823529, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5813953488372093, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.1, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.19230769230769232, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy': 0.40639250861973497, 'epoch': 0.63}
{'loss': 0.8142, 'learning_rate': 0.0002, 'epoch': 0.68}
{'loss': 0.8862, 'learning_rate': 0.0002, 'epoch': 0.78}
{'loss': 0.755, 'learning_rate': 0.0002, 'epoch': 0.88}
{'loss': 0.7893, 'learning_rate': 0.0002, 'epoch': 0.98}
{'loss': 0.8565, 'learning_rate': 0.0002, 'epoch': 1.08}
{'loss': 0.7254, 'learning_rate': 0.0002, 'epoch': 1.17}
{'eval_loss': 0.8255715370178223, 'eval_runtime': 123.7301, 'eval_samples_per_second': 8.082, 'eval_steps_per_second': 1.01, 'epoch': 1.25}
{'mmlu_loss': 2.204169894879063, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_high_school_psychology': 0.5333333333333333, 'mmlu_eval_accuracy_professional_law': 0.28823529411764703, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.37681159420289856, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_medical_genetics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_sociology': 0.6818181818181818, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.1875, 'mmlu_eval_accuracy_human_aging': 0.6086956521739131, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.72, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.42105263157894735, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_philosophy': 0.4117647058823529, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5813953488372093, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.23076923076923078, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy': 0.4125047548854508, 'epoch': 1.25}
{'loss': 0.803, 'learning_rate': 0.0002, 'epoch': 1.27}
{'loss': 0.7502, 'learning_rate': 0.0002, 'epoch': 1.37}
{'loss': 0.7328, 'learning_rate': 0.0002, 'epoch': 1.47}
Saving PEFT checkpoint...
{'loss': 0.8167, 'learning_rate': 0.0002, 'epoch': 1.56}
{'loss': 0.7158, 'learning_rate': 0.0002, 'epoch': 1.66}
{'loss': 0.7969, 'learning_rate': 0.0002, 'epoch': 1.76}
{'loss': 0.7304, 'learning_rate': 0.0002, 'epoch': 1.86}
{'eval_loss': 0.7609136700630188, 'eval_runtime': 123.6681, 'eval_samples_per_second': 8.086, 'eval_steps_per_second': 1.011, 'epoch': 1.88}
{'mmlu_loss': 2.1127450422694287, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_high_school_psychology': 0.5166666666666667, 'mmlu_eval_accuracy_professional_law': 0.2823529411764706, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.34782608695652173, 'mmlu_eval_accuracy_college_physics': 0.2727272727272727, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_sociology': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_high_school_geography': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_biology': 0.21875, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.42105263157894735, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_philosophy': 0.38235294117647056, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5697674418604651, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.23076923076923078, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy': 0.4141925818853526, 'epoch': 1.88}
{'loss': 0.7256, 'learning_rate': 0.0002, 'epoch': 1.96}
{'loss': 0.797, 'learning_rate': 0.0002, 'epoch': 2.05}
{'loss': 0.6738, 'learning_rate': 0.0002, 'epoch': 2.15}
{'loss': 0.6952, 'learning_rate': 0.0002, 'epoch': 2.25}
{'loss': 0.759, 'learning_rate': 0.0002, 'epoch': 2.35}
{'loss': 0.6792, 'learning_rate': 0.0002, 'epoch': 2.44}
{'eval_loss': 0.7657064199447632, 'eval_runtime': 123.6438, 'eval_samples_per_second': 8.088, 'eval_steps_per_second': 1.011, 'epoch': 2.5}
{'mmlu_loss': 2.3320061092575393, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_high_school_psychology': 0.5666666666666667, 'mmlu_eval_accuracy_professional_law': 0.2823529411764706, 'mmlu_eval_accuracy_professional_medicine': 0.3225806451612903, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.6923076923076923, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.37681159420289856, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_sociology': 0.6363636363636364, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_prehistory': 0.2571428571428571, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_high_school_geography': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.28125, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.4473684210526316, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.2558139534883721, 'mmlu_eval_accuracy_philosophy': 0.4411764705882353, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5581395348837209, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.19230769230769232, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy': 0.41314724376480305, 'epoch': 2.5}
{'loss': 0.7476, 'learning_rate': 0.0002, 'epoch': 2.54}
{'loss': 0.6672, 'learning_rate': 0.0002, 'epoch': 2.64}
{'loss': 0.6751, 'learning_rate': 0.0002, 'epoch': 2.74}
{'loss': 0.7696, 'learning_rate': 0.0002, 'epoch': 2.84}
{'loss': 0.6737, 'learning_rate': 0.0002, 'epoch': 2.93}
Saving PEFT checkpoint...
{'loss': 0.7176, 'learning_rate': 0.0002, 'epoch': 3.03}
{'loss': 0.6603, 'learning_rate': 0.0002, 'epoch': 3.13}
{'eval_loss': 0.7227248549461365, 'eval_runtime': 123.6685, 'eval_samples_per_second': 8.086, 'eval_steps_per_second': 1.011, 'epoch': 3.13}
{'mmlu_loss': 2.285998815049728, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_high_school_psychology': 0.5666666666666667, 'mmlu_eval_accuracy_professional_law': 0.2823529411764706, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.391304347826087, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_prehistory': 0.3142857142857143, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_high_school_geography': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.28125, 'mmlu_eval_accuracy_human_aging': 0.6086956521739131, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.4473684210526316, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_philosophy': 0.4411764705882353, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5465116279069767, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy': 0.4173368340827875, 'epoch': 3.13}
{'loss': 0.6155, 'learning_rate': 0.0002, 'epoch': 3.23}
{'loss': 0.7258, 'learning_rate': 0.0002, 'epoch': 3.33}
{'loss': 0.6438, 'learning_rate': 0.0002, 'epoch': 3.42}
{'loss': 0.6753, 'learning_rate': 0.0002, 'epoch': 3.52}
{'loss': 0.672, 'learning_rate': 0.0002, 'epoch': 3.62}
{'loss': 0.6229, 'learning_rate': 0.0002, 'epoch': 3.72}
{'eval_loss': 0.7503958940505981, 'eval_runtime': 123.6485, 'eval_samples_per_second': 8.087, 'eval_steps_per_second': 1.011, 'epoch': 3.76}
{'mmlu_loss': 2.3622756730765104, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_high_school_computer_science': 0.4444444444444444, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_astronomy': 0.3125, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_high_school_psychology': 0.55, 'mmlu_eval_accuracy_professional_law': 0.29411764705882354, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.34782608695652173, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_clinical_knowledge': 0.27586206896551724, 'mmlu_eval_accuracy_high_school_geography': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.28125, 'mmlu_eval_accuracy_human_aging': 0.5217391304347826, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.6190476190476191, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_philosophy': 0.38235294117647056, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5697674418604651, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_college_medicine': 0.3181818181818182, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy': 0.4083584191559788, 'epoch': 3.76}
{'loss': 0.7355, 'learning_rate': 0.0002, 'epoch': 3.81}
{'loss': 0.6424, 'learning_rate': 0.0002, 'epoch': 3.91}
{'loss': 0.643, 'learning_rate': 0.0002, 'epoch': 4.01}
{'loss': 0.7024, 'learning_rate': 0.0002, 'epoch': 4.11}
{'loss': 0.5899, 'learning_rate': 0.0002, 'epoch': 4.21}
{'loss': 0.6564, 'learning_rate': 0.0002, 'epoch': 4.3}
{'eval_loss': 0.7204914689064026, 'eval_runtime': 123.6787, 'eval_samples_per_second': 8.085, 'eval_steps_per_second': 1.011, 'epoch': 4.38}
{'mmlu_loss': 2.3825683407485485, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.4090909090909091, 'mmlu_eval_accuracy_security_studies': 0.2962962962962963, 'mmlu_eval_accuracy_high_school_psychology': 0.5833333333333334, 'mmlu_eval_accuracy_professional_law': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.36231884057971014, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_business_ethics': 0.6363636363636364, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_prehistory': 0.34285714285714286, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_high_school_geography': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.25, 'mmlu_eval_accuracy_human_aging': 0.6086956521739131, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7142857142857143, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.3684210526315789, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_philosophy': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5581395348837209, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy': 0.42101017078223946, 'epoch': 4.38}
{'loss': 0.6112, 'learning_rate': 0.0002, 'epoch': 4.4}
Saving PEFT checkpoint...
{'loss': 0.5698, 'learning_rate': 0.0002, 'epoch': 4.5}
{'loss': 0.7098, 'learning_rate': 0.0002, 'epoch': 4.6}
{'loss': 0.6102, 'learning_rate': 0.0002, 'epoch': 4.69}
{'loss': 0.6626, 'learning_rate': 0.0002, 'epoch': 4.79}
{'loss': 0.6209, 'learning_rate': 0.0002, 'epoch': 4.89}
{'loss': 0.5681, 'learning_rate': 0.0002, 'epoch': 4.99}
{'eval_loss': 0.7294379472732544, 'eval_runtime': 123.6319, 'eval_samples_per_second': 8.089, 'eval_steps_per_second': 1.011, 'epoch': 5.01}
{'mmlu_loss': 2.361605021481713, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_machine_learning': 0.2727272727272727, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_security_studies': 0.37037037037037035, 'mmlu_eval_accuracy_high_school_psychology': 0.6166666666666667, 'mmlu_eval_accuracy_professional_law': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.3333333333333333, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.4444444444444444, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_physics': 0.35294117647058826, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_prehistory': 0.37142857142857144, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.3125, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7142857142857143, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.3157894736842105, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_philosophy': 0.38235294117647056, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5232558139534884, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_professional_accounting': 0.3870967741935484, 'mmlu_eval_accuracy': 0.42001632592721705, 'epoch': 5.01}
{'loss': 0.7077, 'learning_rate': 0.0002, 'epoch': 5.09}
{'loss': 0.5872, 'learning_rate': 0.0002, 'epoch': 5.18}
{'loss': 0.5672, 'learning_rate': 0.0002, 'epoch': 5.28}
{'loss': 0.6255, 'learning_rate': 0.0002, 'epoch': 5.38}
{'loss': 0.5069, 'learning_rate': 0.0002, 'epoch': 5.48}
{'loss': 0.6853, 'learning_rate': 0.0002, 'epoch': 5.57}
{'eval_loss': 0.7264975905418396, 'eval_runtime': 123.6381, 'eval_samples_per_second': 8.088, 'eval_steps_per_second': 1.011, 'epoch': 5.63}
{'mmlu_loss': 2.4302527867257595, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy_astronomy': 0.3125, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_security_studies': 0.3333333333333333, 'mmlu_eval_accuracy_high_school_psychology': 0.6166666666666667, 'mmlu_eval_accuracy_professional_law': 0.3058823529411765, 'mmlu_eval_accuracy_professional_medicine': 0.25806451612903225, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.36231884057971014, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.5, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_human_sexuality': 0.3333333333333333, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.7272727272727273, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_prehistory': 0.2857142857142857, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_high_school_geography': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_biology': 0.28125, 'mmlu_eval_accuracy_human_aging': 0.6086956521739131, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7142857142857143, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.76, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.34210526315789475, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.27906976744186046, 'mmlu_eval_accuracy_philosophy': 0.38235294117647056, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5581395348837209, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_college_medicine': 0.36363636363636365, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy': 0.4137527687402763, 'epoch': 5.63}
{'loss': 0.6035, 'learning_rate': 0.0002, 'epoch': 5.67}
{'loss': 0.5745, 'learning_rate': 0.0002, 'epoch': 5.77}
{'loss': 0.6331, 'learning_rate': 0.0002, 'epoch': 5.87}
Saving PEFT checkpoint...
{'loss': 0.5084, 'learning_rate': 0.0002, 'epoch': 5.97}
{'loss': 0.6633, 'learning_rate': 0.0002, 'epoch': 6.06}
{'loss': 0.5658, 'learning_rate': 0.0002, 'epoch': 6.16}
{'loss': 0.4517, 'learning_rate': 0.0002, 'epoch': 6.26}
{'eval_loss': 0.798636257648468, 'eval_runtime': 123.6584, 'eval_samples_per_second': 8.087, 'eval_steps_per_second': 1.011, 'epoch': 6.26}
{'mmlu_loss': 2.416290213043491, 'mmlu_eval_accuracy_high_school_mathematics': 0.2413793103448276, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_machine_learning': 0.18181818181818182, 'mmlu_eval_accuracy_nutrition': 0.6060606060606061, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy_astronomy': 0.375, 'mmlu_eval_accuracy_high_school_us_history': 0.45454545454545453, 'mmlu_eval_accuracy_security_studies': 0.37037037037037035, 'mmlu_eval_accuracy_high_school_psychology': 0.5833333333333334, 'mmlu_eval_accuracy_professional_law': 0.3058823529411765, 'mmlu_eval_accuracy_professional_medicine': 0.2903225806451613, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_econometrics': 0.08333333333333333, 'mmlu_eval_accuracy_public_relations': 0.25, 'mmlu_eval_accuracy_world_religions': 0.7368421052631579, 'mmlu_eval_accuracy_professional_psychology': 0.36231884057971014, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_virology': 0.3888888888888889, 'mmlu_eval_accuracy_high_school_european_history': 0.5555555555555556, 'mmlu_eval_accuracy_moral_scenarios': 0.24, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_medical_genetics': 0.7272727272727273, 'mmlu_eval_accuracy_high_school_physics': 0.29411764705882354, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_sociology': 0.7272727272727273, 'mmlu_eval_accuracy_college_biology': 0.25, 'mmlu_eval_accuracy_us_foreign_policy': 0.6363636363636364, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_prehistory': 0.34285714285714286, 'mmlu_eval_accuracy_clinical_knowledge': 0.3103448275862069, 'mmlu_eval_accuracy_high_school_geography': 0.5, 'mmlu_eval_accuracy_high_school_biology': 0.28125, 'mmlu_eval_accuracy_human_aging': 0.5652173913043478, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.7142857142857143, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_marketing': 0.72, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_moral_disputes': 0.34210526315789475, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.2558139534883721, 'mmlu_eval_accuracy_philosophy': 0.35294117647058826, 'mmlu_eval_accuracy_high_school_world_history': 0.34615384615384615, 'mmlu_eval_accuracy_miscellaneous': 0.5581395348837209, 'mmlu_eval_accuracy_logical_fallacies': 0.6111111111111112, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_college_medicine': 0.2727272727272727, 'mmlu_eval_accuracy_management': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.375, 'mmlu_eval_accuracy_high_school_microeconomics': 0.2692307692307692, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy': 0.41649233506610717, 'epoch': 6.26}
{'loss': 0.6636, 'learning_rate': 0.0002, 'epoch': 6.36}
{'train_runtime': 28549.2578, 'train_samples_per_second': 2.914, 'train_steps_per_second': 0.023, 'train_loss': 0.7002212524414062, 'epoch': 6.36}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =       6.36
  train_loss               =     0.7002
  train_runtime            = 7:55:49.25
  train_samples_per_second =      2.914
  train_steps_per_second   =      0.023
***** eval metrics *****
  epoch                   =       6.36
  eval_loss               =     0.7293
  eval_runtime            = 0:02:03.65
  eval_samples_per_second =      8.087
  eval_steps_per_second   =      1.011
