Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.12s/it]
/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:37<00:00, 17.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:37<00:00, 18.64s/it]
Token indices sequence length is longer than the specified maximum sequence length for this model (4718 > 4096). Running this sequence through the model will result in indexing errors
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 15.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 16.81s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:19<00:19, 19.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 13.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.34s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:48<00:00, 22.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:48<00:00, 24.29s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 19.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.94s/it]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:27,  3.88s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:08<00:25,  4.21s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:12<00:20,  4.17s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:16<00:17,  4.25s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:20<00:12,  4.20s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:25<00:08,  4.19s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:29<00:04,  4.20s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:31<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:31<00:00,  3.89s/it]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.49s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:09,  1.50s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:07,  1.51s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.51s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:07<00:04,  1.52s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:09<00:03,  1.51s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:10<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.40s/it]
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:08<02:25,  8.08s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:16<02:19,  8.22s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:24<02:09,  8.11s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:32<01:59,  7.99s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:40<01:52,  8.07s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:48<01:45,  8.10s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:56<01:35,  7.94s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [01:04<01:27,  7.99s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [01:12<01:20,  8.01s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [01:20<01:11,  7.94s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [01:28<01:04,  8.05s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [01:36<00:56,  8.07s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [01:44<00:48,  8.06s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [01:52<00:39,  8.00s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [02:00<00:32,  8.03s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [02:08<00:24,  8.06s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [02:16<00:15,  7.92s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [02:24<00:07,  7.98s/it]Loading checkpoint shards: 100%|██████████| 19/19 [02:31<00:00,  7.59s/it]Loading checkpoint shards: 100%|██████████| 19/19 [02:31<00:00,  7.95s/it]
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:03<01:05,  3.66s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:07<01:01,  3.64s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:10<00:58,  3.65s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:14<00:54,  3.61s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:18<00:51,  3.65s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:21<00:47,  3.67s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:25<00:44,  3.67s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:29<00:40,  3.69s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:33<00:37,  3.71s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:36<00:33,  3.70s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:40<00:29,  3.71s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:44<00:32,  4.00s/it]
Traceback (most recent call last):
  File "/gpfs/fs001/cbica/home/xjia/qlora/examples/qlora_models_generate.py", line 128, in <module>
    model = AutoModelForCausalLM.from_pretrained(
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
    return model_class.from_pretrained(
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3706, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4116, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/modeling_utils.py", line 786, in _load_state_dict_into_meta_model
    set_module_quantized_tensor_to_device(
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/integrations/bitsandbytes.py", line 98, in set_module_quantized_tensor_to_device
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(device)
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/bitsandbytes/nn/modules.py", line 179, in to
    return self.cuda(device)
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/bitsandbytes/nn/modules.py", line 157, in cuda
    w_4bit, quant_state = bnb.functional.quantize_4bit(w, blocksize=self.blocksize, compress_statistics=self.compress_statistics, quant_type=self.quant_type)
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/bitsandbytes/functional.py", line 847, in quantize_4bit
    qabsmax, state2 = quantize_blockwise(absmax, blocksize=256)
  File "/cbica/home/xjia/.conda/envs/textlearning/lib/python3.9/site-packages/bitsandbytes/functional.py", line 610, in quantize_blockwise
    out = torch.zeros_like(A, dtype=torch.uint8)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 44.56 GiB total capacity; 42.59 GiB already allocated; 2.31 MiB free; 43.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
