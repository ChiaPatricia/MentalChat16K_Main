/net/pr2/projects/plgrid/plggllm/anaconda3/envs/llm/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Namespace(__cached__setup_devices=device(type='cuda', index=0), _n_gpu=1, adafactor=False, adam8bit=False, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, auto_find_batch_size=False, bf16=True, bf16_full_eval=False, bits=4, cache_dir=None, data_seed=42, dataloader_drop_last=False, dataloader_num_workers=3, dataloader_pin_memory=True, dataset='/net/pr2/projects/plgrid/plggllm/MentalGPT/data/lab/self_instruct_gpt3.5_instruction.csv', dataset_format=None, ddp_backend=None, ddp_broadcast_buffers=None, ddp_bucket_cap_mb=None, ddp_find_unused_parameters=None, ddp_timeout=1800, debug=[], deepspeed=None, deepspeed_plugin=None, disable_tqdm=False, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, do_eval=True, do_mmlu_eval=True, do_predict=False, do_train=True, double_quant=True, eval_accumulation_steps=None, eval_dataset_size=1024, eval_delay=0, eval_steps=48, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, fp16=False, fp16_backend='auto', fp16_full_eval=False, fp16_opt_level='O1', fsdp=[], fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_min_num_params=0, fsdp_transformer_layer_cls_to_wrap=None, full_determinism=False, full_finetune=False, generation_config=GenerationConfig {
  "max_new_tokens": 256,
  "transformers_version": "4.31.0"
}
, generation_max_length=None, generation_num_beams=None, gradient_accumulation_steps=8, gradient_checkpointing=True, greater_is_better=None, group_by_length=True, half_precision_backend='auto', hub_model_id=None, hub_private_repo=False, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, ignore_data_skip=False, include_inputs_for_metrics=False, jit_mode_eval=False, label_names=None, label_smoothing_factor=0.0, learning_rate=0.0002, length_column_name='length', load_best_model_at_end=False, local_rank=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/llama2-7b-gpt-0113/runs/Jan13_16-11-54_t0039', logging_first_step=False, logging_nan_inf_filter=True, logging_steps=10, logging_strategy=<IntervalStrategy.STEPS: 'steps'>, lora_alpha=16.0, lora_dropout=0.1, lora_r=64, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, max_eval_samples=1000, max_grad_norm=0.3, max_memory_MB=80000, max_mmlu_samples=None, max_steps=480, max_train_samples=None, metric_for_best_model=None, mmlu_dataset='mmlu-fs', mmlu_source_max_len=2048, mmlu_split='eval', model_name_or_path='meta-llama/Llama-2-7b-hf', mp_parameters='', no_cuda=False, num_train_epochs=3.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, output_dir='./output/llama2-7b-gpt-0113', overwrite_output_dir=False, past_index=-1, per_device_eval_batch_size=8, per_device_train_batch_size=16, per_gpu_eval_batch_size=None, per_gpu_train_batch_size=None, predict_with_generate=False, prediction_loss_only=False, push_to_hub=False, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, quant_type='nf4', ray_scope='last', remove_unused_columns=False, report_to=[], resume_from_checkpoint=None, run_name='./output/llama2-7b-gpt-0113', save_on_each_node=False, save_safetensors=False, save_steps=120, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_total_limit=40, seed=0, sharded_ddp=[], skip_memory_metrics=True, sortish_sampler=False, source_max_len=256, target_max_len=512, tf32=None, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, torchdynamo=None, tpu_metrics_debug=False, tpu_num_cores=None, train_on_source=False, trust_remote_code=False, use_auth_token=True, use_ipex=False, use_legacy_prediction_loop=False, use_mps_device=False, warmup_ratio=0.03, warmup_steps=0, weight_decay=0.0, xpu_backend=None)
loading base model meta-llama/Llama-2-7b-hf...
Adding special tokens.
adding LoRA modules...
loaded model
Splitting train dataset in train and validation according to `eval_dataset_size`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:53<01:53, 113.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:35<00:00, 71.08s/it] Loading checkpoint shards: 100%|██████████| 2/2 [02:35<00:00, 77.51s/it]
/net/pr2/projects/plgrid/plggllm/anaconda3/envs/llm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map:  12%|█▏        | 121/1000 [00:00<00:00, 8461.05 examples/s]
Traceback (most recent call last):
  File "qlora.py", line 845, in <module>
    train()
  File "qlora.py", line 714, in train
    data_module = make_data_module(tokenizer=tokenizer, args=args)
  File "qlora.py", line 655, in make_data_module
    eval_dataset = eval_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})
  File "/net/pr2/projects/plgrid/plggllm/anaconda3/envs/llm/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 592, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/net/pr2/projects/plgrid/plggllm/anaconda3/envs/llm/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 557, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/net/pr2/projects/plgrid/plggllm/anaconda3/envs/llm/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3093, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/net/pr2/projects/plgrid/plggllm/anaconda3/envs/llm/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3446, in _map_single
    example = apply_function_on_filtered_inputs(example, i, offset=offset)
  File "/net/pr2/projects/plgrid/plggllm/anaconda3/envs/llm/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "qlora.py", line 655, in <lambda>
    eval_dataset = eval_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})
TypeError: object of type 'NoneType' has no len()
srun: error: t0039: task 0: Exited with exit code 1
