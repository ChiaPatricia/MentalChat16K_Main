{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists, join, isdir\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
    "from peft import PeftModel\n",
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_checkpoint(checkpoint_dir):\n",
    "    if isdir(checkpoint_dir):\n",
    "        is_completed = exists(join(checkpoint_dir, 'completed'))\n",
    "        if is_completed: return None, True # already finished\n",
    "        max_step = 0\n",
    "        for filename in os.listdir(checkpoint_dir):\n",
    "            if isdir(join(checkpoint_dir, filename)) and filename.startswith('checkpoint'):\n",
    "                max_step = max(max_step, int(filename.replace('checkpoint-', '')))\n",
    "        if max_step == 0: return None, is_completed # training started, but no checkpoint\n",
    "        checkpoint_dir = join(checkpoint_dir, f'checkpoint-{max_step}')\n",
    "        print(f\"Found a previous checkpoint at: {checkpoint_dir}\")\n",
    "        return checkpoint_dir, is_completed # checkpoint found!\n",
    "    return None, False # first training\n",
    "\n",
    "def generate(model, tokenizer, prompt, user_question, max_new_tokens=512, top_p=0.9, temperature=0.7):\n",
    "    inputs = tokenizer(prompt.format(user_question=user_question), return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        generation_config=GenerationConfig(\n",
    "            do_sample=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TORCH_USE_CUDA_DSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e25c02145b24313b34bce50a6b6a3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mbos_token_id \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Load the model (use bf16 for faster inference)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m base_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     model_name_or_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# device_map='cpu',\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mBitsAndBytesConfig(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         bnb_4bit_compute_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         bnb_4bit_use_double_quant\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m         bnb_4bit_quant_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mnf4\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     model_name_or_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2263756269632d6c6f67696e2e757068732e7570656e6e2e656475222c2275736572223a22786a6961227d/cbica/home/xjia/qlora/examples/vicuna_generate.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39mfrom_pretrained(model, adapter_path, device_map\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/modeling_utils.py:3309\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3299\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3300\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3302\u001b[0m     (\n\u001b[1;32m   3303\u001b[0m         model,\n\u001b[1;32m   3304\u001b[0m         missing_keys,\n\u001b[1;32m   3305\u001b[0m         unexpected_keys,\n\u001b[1;32m   3306\u001b[0m         mismatched_keys,\n\u001b[1;32m   3307\u001b[0m         offload_index,\n\u001b[1;32m   3308\u001b[0m         error_msgs,\n\u001b[0;32m-> 3309\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3310\u001b[0m         model,\n\u001b[1;32m   3311\u001b[0m         state_dict,\n\u001b[1;32m   3312\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3313\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3314\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3315\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3316\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3317\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3318\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3319\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3320\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3321\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3322\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3323\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3324\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3325\u001b[0m     )\n\u001b[1;32m   3327\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3328\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/modeling_utils.py:3699\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3697\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m   3698\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fsdp_enabled() \u001b[39mor\u001b[39;00m is_fsdp_enabled_and_dist_rank_0():\n\u001b[0;32m-> 3699\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3700\u001b[0m             model_to_load,\n\u001b[1;32m   3701\u001b[0m             state_dict,\n\u001b[1;32m   3702\u001b[0m             loaded_keys,\n\u001b[1;32m   3703\u001b[0m             start_prefix,\n\u001b[1;32m   3704\u001b[0m             expected_keys,\n\u001b[1;32m   3705\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3706\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3707\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3708\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3709\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3710\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3711\u001b[0m             is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   3712\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3713\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3714\u001b[0m         )\n\u001b[1;32m   3715\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3716\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/modeling_utils.py:751\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    748\u001b[0m             fp16_statistics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    750\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_name:\n\u001b[0;32m--> 751\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[1;32m    752\u001b[0m                 model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, fp16_statistics\u001b[39m=\u001b[39;49mfp16_statistics\n\u001b[1;32m    753\u001b[0m             )\n\u001b[1;32m    755\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/.conda/envs/textlearning/lib/python3.9/site-packages/transformers/integrations/bitsandbytes.py:108\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m    106\u001b[0m     new_value \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    107\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 108\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# # TODO: Update variables\n",
    "# max_new_tokens = 512\n",
    "# top_p = 0.9\n",
    "# temperature=0.7\n",
    "\n",
    "# Base model\n",
    "model_name_or_path = 'lmsys/vicuna-7b-v1.5'\n",
    "# Adapter name on HF hub or local checkpoint path.\n",
    "# adapter_path_replicate, _ = get_last_checkpoint('output/guanaco-7b-A40')\n",
    "adapter_path = '/cbica/home/xjia/qlora/output/MentalGPT-7b-newCombined-1016/checkpoint-1400/adapter_model' #'timdettmers/guanaco-7b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "# Fixing some of the early LLaMA HF conversion issues.\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "# Load the model (use bf16 for faster inference)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map='cpu',\n",
    "    device_map={\"\": 0},\n",
    "    load_in_4bit=True,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "    )\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map='cpu',\n",
    "    device_map={\"\": 0},\n",
    "    load_in_4bit=True,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "    )\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_path, device_map='cpu')\n",
    "\n",
    "base_model.eval()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = '''Intimacy has always been a complex area for me, and I'm seeking counseling to explore my discomfort and fears related to emotional closeness. \n",
    "                   Whenever a relationship becomes more intimate, fear takes over, and I struggle to express my feelings or trust others completely. \n",
    "                   There is a particular incident from my past that has had a lasting impact on my ability to be intimate with others. \n",
    "                   I was in a long-term relationship where my partner consistently violated my boundaries, leaving me feeling emotionally unsafe and scared of letting anyone else get close.'''\n",
    "\n",
    "prompt = (\n",
    "    \"You are a helpful mental health counselling assistant, please answer the mental health questions based on the patient's description.\n",
    "    \"The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. \"\n",
    "    \"### User: {user_question}\"\n",
    "    \"### Assistant: \"\n",
    ")\n",
    "\n",
    "# prompt = (\n",
    "#     \"A chat between a user with mental illness concern and a professional, helpful mental health counseling assitant. \"\n",
    "#     \"The assistant gives helpful, comprehensive, and appropriate answers to the user's questions. \"\n",
    "#     \"### User: {user_question}\"\n",
    "#     \"### Assistant: \"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*40+\"Base Vicuna\"+\"=\"*40)\n",
    "base_response = generate(base_model, user_question)\n",
    "print(\"\\n\")\n",
    "print(\"=\"*40+\"Vicuna qlora\"+\"=\"*40)\n",
    "response = generate(model, user_question)\n",
    "import pdb; pdb.set_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
